from sentence_transformers import SentenceTransformer
import pandas as pd

@transformer
def transform(data, *args, **kwargs):
    # 1. Initialize the model (already cached in your Docker build!)
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    chunked_data = []
    
    for doc in data:
        text = doc['text']
        # 2. Strategy: Split text into 500-character chunks
        # This is a 'Tiny Step' to keep logic simple but effective
        chunks = [text[i:i+500] for i in range(0, len(text), 500)]
        
        for i, chunk in enumerate(chunks):
            # 3. Create the Embedding (The math vector)
            embedding = model.encode(chunk).tolist()
            
            chunked_data.append({
                'document_id': doc['filename'],
                'chunk_id': f"{doc['filename']}_{i}",
                'content': chunk,
                'vector': embedding
            })
    
    return pd.DataFrame(chunked_data)
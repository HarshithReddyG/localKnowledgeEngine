{"filename": "DuckDB_In_Action_Final_MotherDuck.pdf", "text": "MANNINGMark Needham\nMichael HungerMichael Simons\nForeword by  Mark Raasveldt\n                 and  Hannes M\u00fchleisenCompliments of2 EPILOGUE   \n \n DuckDB as the hub of data ingestions and transformation\nData sources\nOptional persistenceData transformation\nProgramming\nlanguagesServerless SQL Analytics\nPowered by DuckDB\nCloud data warehouse for the rest of us\nBeautiful Web IDE and Notebook for DuckDBUse it for:\nIncludes:Serverless backend for Data Apps\ncentralized data storage and sharing for orgs\nsupport for many tools in the mds ecosystem\nHYBRID QUERYING: USE YOUR LAPTOP DUCKDB WITH THE CLOUD\nMO THERDUCK.COMData lake query engineDuckDB in ActioniiDuckDB in Action\nMARK NEEDHAM\nMICHAEL HUNGER\nMICHAEL SIMONS\nFOREWORD  BY MARK RAASVELDT\nAND HANNES  M\u00dcHLEISEN\nMANNING\nSHELTER  ISLANDFor online information and ordering of this and other Manning books, please visit\nwww.manning.com . The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761Shelter Island, NY 11964\nEmail: orders@manning.com\n\u00a92024 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning\u2019s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.\nThe author and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The author and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from negligence, accident, or any other cause, or from any \nusage of the information herein.\nManning Publications Co. Development editor: Rhonda Mason\n20 Baldwin Road Technical editor: Jordan Tigani\nPO Box 761 Review editor: Radmila ErcegovacShelter Island, NY 11964 Production editor: Kathy Rossland\nCopy editor: Christian Berk\nProofreader: Melody Dolab \nTechnical proofreader: Dirk Gomez\nTypesetter and cover designer: Marija Tudor\nISBN 9781633437258\nPrinted in the United States of AmericaTo Rainer and Stefan\n\u2014Michael Simons\nI dedicate this book to a ll the people who are suff ering in the world from \ninjustice, poverty, war, and disease. It is a shame that humanity wastes its \nfuture and the planet on greed for wealth and power in stead of working \ntogether to create a better world for all. I will donate most of the royalties from \nthis book to charities helping to  make the world a better place.\n\u2014Michael Hungervibrief contents\n1\u25a0An introduction to DuckDB 1\n2\u25a0Getting started with DuckDB 12\n3\u25a0Executing SQL queries 22\n4\u25a0Advanced aggregation and analysis of data 58\n5\u25a0Exploring data without persistence 98\n6\u25a0Integrating with the Python ecosystem 123\n7\u25a0DuckDB in the cloud with MotherDuck 144\n8\u25a0Building data pipelines with DuckDB 163\n9\u25a0Building and deploying data apps 193\n10 \u25a0Performance considerations for large datasets 223\n11 \u25a0Conclusion 262\n \n   \n viicontents\nforeword xii\npreface xiv\nacknowledgments xv\nabout this book xviiabout the authors xx\nabout the cover illustration xxii\n1 An introduction to DuckDB 1\n1.1 What is DuckDB? 2\n1.2 Why should you care about DuckDB? 3\n1.3 When should you use DuckDB? 41.4 When should you not use DuckDB? 5\n1.5 Use cases 5\n1.6 Where does DuckDB fit in? 61.7 Steps of the data processing flow 7\nData formats and sources 7\u25a0Data structures 8\u25a0Developing \nthe SQL 8\u25a0Using or processing the results 9\n2 Getting started with DuckDB 12\n2.1 Supported environments 12\n2.2 Installing the DuckDB CLI 13\nmacOS 13\u25a0Linux and Windows 13CONTENTS viii\n2.3 Using the DuckDB CLI 13\nSQL statements 14\u25a0Dot commands 14\u25a0CLI \narguments 15\n2.4 DuckDB\u2019s extension system 15\n2.5 Analyzing a CSV file with the DuckDB CLI 17\nResult modes 18\n3 Executing SQL queries 22\n3.1 A quick SQL recap 233.2 Analyzing energy production 23\nDownloading the dataset 24\u25a0The target schema 25\n3.3 Data definition language queries 26\nThe CREATE TABLE statement 26\u25a0The ALTER TABLE \nstatement 28\u25a0The CREATE VIEW statement 29\nThe DESCRIBE statement 30\n3.4 Data manipulation language queries 31\nThe INSERT statement 31\u25a0Merging data 35\u25a0The \nDELETE statement 36\u25a0The SELECT statement 36\n3.5 DuckDB-specific SQL extensions 50\nDealing with SELECT 50\u25a0Inserting by name 53\nAccessing aliases everywhere 54\u25a0Grouping and ordering \nby all relevant columns 54\u25a0Sampling data 55\nFunctions with optional parameters 56\n4 Advanced aggregation and analysis of data 58\n4.1 Pre-aggregating data while ingesting 59\n4.2 Summarizing data 61\n4.3 On subqueries 62\nSubqueries as expressions 64\n4.4 Grouping sets 66\n4.5 Window functions 69\nDefining partitions 71\u25a0Framing 74\u25a0Named \nwindows 75\u25a0Accessing preceding or following rows \nin a partition 77\n4.6 Conditions and filtering outside the WHERE clause 78\nUsing the HAVING clause 79\u25a0Using the QUALIFY \nclause 80\u25a0Using the FILTER clause 81\n4.7 The PIVOT statement 82CONTENTS ix\n4.8 Using the ASOF JOIN 86\n4.9 Using table functions 90\n4.10 Using LATERAL joins 93\n5 Exploring data without persistence 98\n5.1 Why use a database with out persisting any data? 99\n5.2 Inferring file type and schema 99\nA note on CSV parsing 101\n5.3 Shredding nested JSON 102\n5.4 Translating CSV to Parquet 108\n5.5 Analyzing and quer ying Parquet files 113\n5.6 Querying SQLite an d other databases 116\n5.7 Working with Excel files 120\n6 Integrating with the Python ecosystem 123\n6.1 Getting started 124\nInstalling the Python package 124\u25a0Opening up a \ndatabase connection 124\n6.2 Using the relational API 125\nIngesting CSV data with the Python API 126\u25a0Composing \nqueries 128\u25a0SQL querying 133\n6.3 Querying pandas DataFrames 134\n6.4 User-defined functions 1366.5 Interoperability with Apache Arrow and Polars 141\n7 DuckDB in the clou d with MotherDuck 144\n7.1 Introduction to MotherDuck 145\nHow it works 145\u25a0Why use MotherDuck? 146\n7.2 Getting started with MotherDuck 147\nUsing MotherDuck through the UI 148\u25a0Connecting to \nMotherDuck with DuckDB via token-based authentication 148\n7.3 Making the best possible use of MotherDuck 150\nUploading databases to MotherDuck 150\u25a0Creating \ndatabases in MotherDuck 152\u25a0Sharing databases 153\nManaging S3 secrets and loading Data from S3 buckets 156Optimizing data ingestion and MotherDuck usage 156Querying your data with AI 157\n\u25a0Integrations 161CONTENTS x\n8 Building data pipe lines with DuckDB 163\n8.1 Data pipelines and the role of DuckDB 164\n8.2 Data ingestion with dlt 165\nInstalling a supported source 166\u25a0Building a pipeline 167\nExploring pipeli ne metadata 170\n8.3 Data transformation and modeling with dbt 171\nSetting up a dbt project 172\u25a0Defining sources 173\nDescribing transforma tions with models 173\u25a0Testing \ntransformations and pipelines 176\u25a0Transforming all \nCSV files 178\n8.4 Orchestrating data pipelines with Dagster 182\nDefining assets 183\u25a0Running pipelines 185\u25a0Managing \ndependencies in a pipeline 186\u25a0Advanced computation in \nassets 189\u25a0Uploading to MotherDuck 191\n9 Building and depl oying data apps 193\n9.1 Building a custom data app with Streamlit 194\nWhat is Streamlit? 195\u25a0Building our app 195\nUsing Streamlit components 197\u25a0Visualizing data using \nplot.ly 201\u25a0Deploying our app on the Community \nCloud 204\n9.2 Building a BI dashboard with Apache Superset 207\nWhat is Apache Superset? 207\u25a0Superset\u2019s workflow 209\nCreating our first dashboard 210\u25a0Creating a dataset \nfrom an SQL query 216\u25a0Exporting and importing \ndashboards 220\n10 Performance considerations for large datasets 223\n10.1 Loading and querying the full Stack Overflow \ndatabase 224\nData dump and extraction 224\u25a0The data model 226\nExploring the CSV file data 228\u25a0Loading the data into \nDuckDB 230\u25a0Fast exploratory queries on large tables 233\nPosting on weekdays 236\u25a0Using enums for tags 238\n10.2 Query planning and execution 243\nPlanner and optimizer 243\u25a0Runtime and \nvectorization 243\u25a0Visualizing query plans with Explain \nand Explain Analyze 245\n10.3 Exporting the Stack Overflow data to Parquet 248CONTENTS xi\n10.4 Exploring the New York Taxi dataset from Parquet \nfiles 251\nConfiguring credentials for S3 access 252\u25a0Auto-inferring file \ntypes 252\u25a0Exploring Parquet schema 253\u25a0Creating \nviews 254\u25a0Analyzing the data 255\u25a0Making use of the \ntaxi dataset 259\n11 Conclusion 262\n11.1 What we have learned in this book 262\n11.2 Upcoming stable versions of DuckDB 26311.3 Aspects we did not cover 263\n11.4 Where can you learn more? 264\n11.5 What is the future of data engineering with \nDuckDB? 264\nappendix Client APIs for DuckDB 265\nindex 281xiiforeword\nWelcome, dear reader, to this book about Du ckDB. It feels somewhat surreal to write a\nforeword for this book about DuckDB beca use it seems like everything has happened\nso quickly. The world of data manageme nt systems moves slow ly\u2014software projects\nstarted in the 70s are still in strong positions on the market.\n It has only been a few sh ort years since we sat at the Joost bar in Amsterdam one\nevening in 2018 and decided we were going to build a new system. We had been toy-\ning with the idea previously but had been he sitant, as we knew it was a daft idea. The\ncommon wisdom is that it takes \u201cten mill ion dollars\u201d to make a new database system\nsuccessful. But we decided on an equally daft plan: we would create a new kind of data\nmanagement system\u2014one that had never b een built before\u2014an in-process analytical\nsystem. Maybe the usual rules did not apply to this new kind of  system. After some\nmore beers, we had pretty much decided on  the first rough draft of DuckDB\u2019s archi-\ntecture. The very next day, we started hacking.\n Only a year later, in 2019, we opened up our repository and started telling people\nabout it. We showed our first demo of Du ckDB at the 2019 SIGMOD conference, coin-\ncidentally in Amsterdam. Since we co-organ ized the conference, we snuck stickers in\nthe goodie bags in an early attempt at a ty pe of viral marketing. At the same time, we\nalso opened up the source code repository to the public. The \u201cduck was out of thebag,\u201d so to speak.\n But thousands of open source projects ar e started every day, and the vast majority\nwill\u2014regrettably or not\u2014neve r gain any traction. This wa s also our expe ctation\u2014that,\nmost likely, nobody was going to care a bout our \u201cDuckDB.\u201d But an amazing thing hap-\npened: little by little, the stars on the GitHub repository started to accumulate. We\nthink this came about because of another desi gn goal of DuckDB: ease of use. We had\nobserved that the prevailing sentiment in da ta systems seemed to have been that theFOREWORD xiii\nworld should be grateful to be allowed to use the hard-won results of database systems\nresearch and the systems we build. We ha d observed a worrying effect, however: the\nresults of decades of research were simply being ignored because they were hard to\nuse. In somewhat of a paradigm shift for da ta systems, one design  goal of DuckDB was\nto make it as easy to use as possible and to  fix some of the biggest gripes we had heard\nfrom practitioners.\n Somehow, people seem to have noticed. Big popularity gains came from activity on\nthe social network formerly known as Twi tter and most notably from regularly being\nfeatured on Hacker News. Now DuckDB is downloaded millions of times each month\nand used everywhere from the biggest co mpanies to the smallest embedded devices.\nMotherDuck offers practition ers a hosted version but in DuckDB style and with a\nstrong local component. Heck, people are even wr iting books about DuckDB.\n We\u2019re glad that Mark and the two Mich aels are the ones who bring this book to\nyou. It\u2019s an honor for us that such an excellent team is writing this book. They are\nexperts in explaining challenging data tec hnology to developers in a fun, engaging,\nbut still deeply competent way. We hope you enjoy this book and, of course, that youenjoy working with DuckDB.\n \n \u2014M\nARK RAASVELDT  AND HANNES  M\u00dcHLEISEN\nCREATORS  OF DUCKDB, 2023xivpreface\nThis book covers DuckDB\u2014a modern, fast, embedded analytical database. It runs on\nyour machine and can easily process many gi gabytes of data from a variety of sources,\nincluding JSON, CSV, Parquet, SQLite, and Po stgres. DuckDB integrates well into the\nPython and R ecosystems and allows you to query in-memory data frames without\ncopying the data. You don\u2019t need to spin up cloud data warehouses for your day-to-daydata processing anymore; you can just run DuckDB on your data, locally or in the\ncloud.\n With DuckDB, you can solve your relational  data analytics tasks without friction. It\nis really user friendly and easy to learn. Best of all, you can us e it embedded in your\nPython environments and applications, much  like SQLite. We stro ngly believe that we\nhit the sweet spot in teaching DuckDB, cove ring its CLI-embedded  mode, Python inte-\ngrations, and capabilities for building data  pipelines as well as processing data\u2014all\nwhile also guiding readers through a pa inless deep-dive into modern SQL with\nDuckDB.\n While we all are longtime data expert practitioners and educ ators, we come from\ndifferent corners of this sp ectrum\u2014graph, real-time colu mnar, and relational data-\nbases\u2014yet we all find someth ing of value in DuckDB that we think is worth speaking\nabout. We enjoy using DuckDB a lot, both outside our expertise but also as a useful\ntool in our respective areas of work.xvacknowledgments\nThanks go out to Jordan Tigani, cofoun der and CEO of MotherDuck, who was our\ntechnical editor. His diligent work and feed back made our examples and writing a lot\nbetter. Big thanks also to all the other technical reviewers wh o diligently worked\nthrough the chapters and left their feedback, especially to Marcos Ortiz, Georg Heiler,\nand Jacob Matson for reviewing the original  proposal, and to Dirk Gomez for check-\ning all the code.\n The writings and explanations by Mark  Raasveldt, Hannes M\u00fchleisen, and Alex\nMonahan helped us a lot, teaching us more  about the inner workings of DuckDB, the\nideas behind it, and some SQL gymnastics  we had no idea were possible before.\nThank you!\n Many thanks to Ryan Boyd and Mehdi Quazza. They not only gave valuable feed-\nback but helped us a lot in making  more people aware of the book.\n We also thank the editing team at Ma nning, especially Rhonda Mason and Jona-\nthan Gennick, for dealing so positively with us, and Chri stian Berk for his diligent\ncopy editing and fast feedback loop. Mich ael Hunger and Michael Simons are happy\nabout our colleagues at Neo4j, who may ha ve raised an eyebrow or two but have\nenough self-confidence to acknowledge that  both the graph and relational approach\ncan coexist. DuckDB has been influential for us when thinking about empathy for\nusers of our software.\n To all the reviewers\u2014Andrej Abramu\u0161i \u0107, Abhilash Babu Jyotheendra Babu, Anjan\nBacchu, Chris Bolyard, Thiago Britto Bo rges, Nadir Doctor, Didier Donsez, Dirk\nG\u00f3mez, Simon Hewitt, Andrew Judd, Madiha Khalid, Simeon Leyzerzon, Noel Llevares,\nSebastian Maier, Eli Mayost, Sumit Pal, Anup  K. Parikh, Sasha Sankova, William Jamir\nSilva, Ganesh Swaminathan, Mary Anne Thygesen, Rohini Uppuluri, Ankit Virmani,\nWondi Wolde, and Heng Zhang\u2014your sugges tions helped make this a better book.ACKNOWLEDGMENTS xvi\n And, of course, a book  such as this, written in addition and parallel to the duties at\nwork, takes a toll on spare ti me and private lives. We thank our families for their con-\ntinuous support of our crazy ideas.xviiabout this book\nWe didn\u2019t want to write a reference book (that\u2019s what the docs are for), but rather, to\nshare the excitement and joy we experienced when working with DuckDB so thatyou\u2019ll learn something new on every page , while having the same fun we had when\nwriting. The book is fast-paced, informat ion-rich, hands-on, and informative, with\neasy-to-understand and practical examples.\nWho should read this book\nThe ideal reader for this book is a data en gineer, data scientist, or developer who is\ninterested in analyzing existing structured  data efficiently with out having to set up\ninfrastructure. They should be familiar and comfortable with command-line tools and\npreferably some Python. We will cover a lot of SQL, starting with simple clauses and\nworking our way toward advanced, analytical  statements. DuckDB is available on all\nmajor operating systems and does not requir e any installation process; downloading\nand running the executable is enough. Ou r chapter on MotherDuck, the serverless\nanalytic platform, requires  creating an account if you want to try it out.\nHow this book is organized: A road map\nWe start with a gentle introduction to Duck DB in chapters 1 and 2, presenting its use\ncases and its place in modern data pipelines.  First, we will make sure you are able to\nuse the DuckDB CLI before we proceed with an introduction to SQL in chapter 3. We\nwill cover the basic clauses and statements before entering the world of advanced data\nanalysis with SQL in chapter 4, using adva nced aggregations, window functions, recur-\nsive SQL, and more. Of course, we will in clude the vendor-specific, developer-friendly\nextensions that DuckDB brings to the table.ABOUT  THIS BOOK xviii\n DuckDB has many facets to it, with one of them being the fact that it does not\nforce its persistence storage upon you. We  spend the whole of chapter 5 discussing\nhow you can actually use the SQL engine on top of many different file formats for\nyour purpose, without ingest ing the data into tables.\n Chapter 6 will dive deep into DuckDB\u2019s Python integration before we move to the\ncloud with MotherDuck in Chapter 7.\n After that, we will have all the tools ready to build effective data pipelines (chapter\n8) and deploy data applications (chapter 9).\n In chapter 10, we will take a step back and discuss some considerations for large\ndatasets and apply what we\u2019ve learned so far.\n DuckDB not only offers a CLI and a fantastic Python integration but also Java, C,\nC++, Julia, Rust, and many other language integrations. In the appendix, we will have\na look at these, especially how to use DuckDB from Java.\nAbout the code \nThis book contains many examples of source code both in numbered listings and inline with normal text. In both cases,  the source code is formatted in a \nfixed-width\nfont like this  to separate it from ordinary text. Sometimes, code is also in bold  to\nhighlight code that has changed from previo us steps in the chapter, such as when a\nnew feature is added to an existing line of code.\n In many cases, the original source co de has been reformatted; we\u2019ve added line\nbreaks and reworked indent ation to accommodate the available page space in the\nbook. In rare cases, even this was not en ough, and listings include line-continuation\nmarkers ( \u27a5). In those cases, you might need to  remove an extra space introduced by\nthat marker to make the code work or fix long URLs.\n Additionally, comments in th e source code have often been removed from the list-\nings when the code is described in the te xt. Code annotations accompany many of the\nlistings, highlighting important concepts. \n You can get executable snippets of code from the liveBook (online) version of this\nbook at https:/ /livebook.manning.c om/book/duckdb-in-action . The complete code\nfor the examples in the book is availabl e for download from the Manning website at\nhttps:/ /www.manning.com/books/duckdb-in-action , and from GitHub at https:/ /\ngithub.com/duckdb-in-action/examples .\nliveBook discussion forum\nPurchase of DuckDB in Action  includes free access to liveB ook, Manning\u2019s online read-\ning platform. Using liveBook\u2019s exclusive discussion features, you can attach comments\nto the book globally or to specific sections or paragraphs. It\u2019s a snap to make notes for\nyourself, ask and answer technical question s, and receive help from the author and\nother users. To acce ss the forum, go to https:/ /livebook.manning.com/book/duckdb\n-in-action/discussion . You can also learn more about Manning\u2019s forums and the rules\nof conduct at https:/ /livebook.manning.com/discussion .ABOUT  THIS BOOK xix\n Manning\u2019s commitment to our readers is  to provide a venue where a meaningful\ndialogue between individual  readers and betw een readers and the author can take\nplace. It is not a commitment  to any specific amount of participation on the part of\nthe authors, whose contribution to the forum remains voluntary (and unpaid). We\nsuggest you try asking the auth ors some challenging question s lest their interest stray!\nThe forum and the archives of previous di scussions will be accessible from the pub-\nlisher\u2019s website as long as the book is in print.xxabout the authors\nMARK NEEDHAM\nMark is a product marketing engineer at ClickHouse, where he\ncreates short-form videos and wr ites blog posts about real-time\ndata warehouses. He also works on developer experience, simpli-\nfying the getting-started experience by making product tweaksand improvements to the documentation.\n    Mark has worked in the data infrastructure field for the last\ndecade, first at Neo4j on graph da tabases and then at StarTree on\nreal-time analytics with Apache  Pinot. He has been blogging\nabout his experiences with writing so ftware for the last 15 years at markhneed\nham.com  and has created many short educational videos on data and AI topics at\nhttps:/ /www.youtube.com/@learndatawithmark .\n He tweets as @markhneedham.\n \nM\nICHAEL  HUNGER\nMichael Hunger has been passi onate about software develop-\nment for more than 35 years. Fo r the last 14 years, he has been\nworking on the open source Neo4j graph database, filling manyroles, most recently as head of product innovation and developer\nproduct strategy. Before joining Ne o4j, he consulted in large Java\nprojects and wrote his fair share of SQL code for Oracle, Infor-mix, and MySQL databases. He also created the Jequel SQL DSL\nin 2006, which was later merged into similar efforts.\n As a developer, Michael en joys many aspects of progra mming languages, tools, and\ntechnologies, learning new things every day,  participating in exciting and ambitious\nABOUT  THE AUTHORS xxi\nopen source projects, and contributing to and writing software-rel ated books and arti-\ncles. His interests span Java, Kotlin, Grap hQL, Graph Databases, Generative AI, and\nmodern data analytics. Michael has spok en at numerous conferences and helped\norganized several of them. His efforts got him accepted to the Java Champions pro-\ngram; he\u2019s been writing a bi-monthly colu mn on \u201cEffective Java \u201d for the Java SPEK-\nTRUM print magazine for more than 12 year s. Michael helps kids learn to program by\nrunning weekly girls-only codi ng classes at local schools.\n You can find more about Michaels\u2019s writing and projects on his blog at\nhttps:/ /www.jexp.de .\n \nMICHAEL  SIMONS\nMichael Simons is a Java champion and senior staff software engi-neer at Neo4j and has been workin g professionally as a developer\nfor more than 20 years. In his role  at Neo4j, he is a vital part of\nNeo4j\u2019s integration into the broader Java ecosystem.\n     Before entering the graph space, he worked in the German util-ity sector, using SQL to comput e and predict energy usage for\nlarge German transport grid oper ators and energy producers, way\nbefore analytical databases became  more mainstream. To this day,\nhe enjoys using the declarative nature of SQL (and, of course, Cypher) to ask\nmachines for answers instead of inst ructing them to produce a result.\n Michael is a known speaker at confer ences, bridging Java and databases\u2014\nrelational and graph alike\u2014fo r many years. Michael is th e author of the bestselling\nbook Spring Boot 2 and co-author of arc42 by Example , a book about software architec-\nture documentation. He also writes a blog at info.michael-simons.eu .\n In the spare time that is left, Michael still dreams about beco ming an amateur ath-\nlete, and when he isn\u2019t training for the ne xt marathon, he uses DuckDB to document\nhis progress at biking.michael-simons.eu/history .\n \nxxiiabout the cover illustration\nThe figure on the cover of DuckDB in Action  is \u201cPaisanne Dequito,\u201d or \u201cA peasant\nwoman from Quito,\u201d taken from a collection by Jacques Grasset de Saint-Sauveur, pub-\nlished in 1788. Each illustration is  finely drawn and colored by hand. \n In those days, it was easy to identify wh ere people lived and wh at their trade or sta-\ntion in life was just by their dress. Manni ng celebrates the inventiveness and initiative\nof the computer business with book covers ba sed on the rich diversity of regional cul-\nture centuries ago, brought back  to life by pictures from collections such as this one.\n  \n \n   \n \n  \n \n  \n \n  \n \n 1An introduction\nto DuckDB\nWe\u2019re excited that you\u2019ve picked up this  book and are ready to learn about a tech-\nnology that seems to go against the grain of everything that we\u2019ve learned about big\ndata systems over the last decade. We\u2019ve had a lot of fun using DuckDB, and wehope you will be as enthused as we ar e after reading this book. This book\u2019s\napproach to teaching is hands-on, concise,  and fast paced and will include lots of\ncode examples.\n After reading the book, you should be able to use DuckDB to analyze tabular\ndata in a variety of formats.  You will also have a handy new tool in your toolbox for\ndata transformation, cleanup, and conv ersion. You can integrate it into your\nPython notebooks and processes to replace pandas DataFrames in  situations whereThis chapter covers \n\uf0a1Why DuckDB, a single node in-memory database, \nemerged in the era of big data\n\uf0a1DuckDB\u2019s capabilities\n\uf0a1How DuckDB works and fits into your data pipeline2 CHAPTER  1An introduction to DuckDB\nthey are not performing. You will be able to  build quick applications for data analysis\nusing Streamlit with DuckDB. Let\u2019s get started!\n1.1 What is DuckDB?\nDuckDB is a modern embedded  analytics database that ru ns on your machine and lets\nyou efficiently process and query gigaby tes of data from different sources. Embedded\ndatabases  run within another process, like your  application or notebook, and are not\naccessed over a network. DuckDB was crea ted in 2018 by Mark Raasveldt and Hannes\nM\u00fchleisen, who, at the time, were rese archers in database systems at Centrum\nWiskunde & Informatica (CWI)\u2014the national  research institute for mathematics and\ncomputer science in the Netherlands. \n The founders and the CWI spun DuckDB Labs off as a startup to further develop\nDuckDB. Its engineer ing team focuses on making DuckDB more efficient, user\nfriendly, and better integrated.\n The nonprofit DuckDB Foundation gove rns the DuckDB Project by safeguarding\nthe intellectual property and ensuring the continuity of the open source project\nunder the MIT license. The foundation\u2019s operations and DuckDB\u2019s development aresupported by commercial memb ers, while association memb ers can inform the devel-\nopment road map.\n While DuckDB focuses on th e local processing of data, another startup, Mother-\nDuck, aims to extend DuckDB to  a distributed, self-serve an alytics system that can pro-\ncess data in the cloud and on the edge. It adds collaboration and sharing capabilities\nto DuckDB and supports processing da ta from all kinds of cloud storage.\n The DuckDB ecosystem is qu ite broad, allowing many people and organizations to\ncreate integrations and generally useable ap plications as well as get excited about its\npossibilities. Fortunately, the DuckDB community is very helpful and friendly\u2014you\ncan find them on Discord ( https:/ /discord.duckdb.org/ ) and GitHub ( https:/ /\ngithub.com/duckdb/duckdb ). The documentation is comprehensive and detailed\nenough to answer most questions.\n DuckDB lets you process and join local or  remote files (e.g., from cloud buckets or\nURLs) in different formats, including CSV, JSON, Parquet, and Ap ache Arrow, as well\nas several databases, like MySQL, SQLite, and Postgres. You can even query pandas or\nPolars DataFrames from your Python script s or Jupyter notebooks. A diagram showing\nconceptually how DuckDB is typically used is shown in figure 1.1.\n Unlike the pandas and Polars DataFrame libr aries, DuckDB is a real analytics data-\nbase, implementing more efficient data-pro cessing mechanisms that can handle large\nvolumes of data in seconds. With its SQL di alect, even complex queries can be expressed\nmore succinctly. Its ex pressiveness allows you to handle more operations inside a single\ndatabase query, avoiding multiple executions, which would be more costly.\n The architecture of the core database en gine is the basis for efficient processing\nand memory management. You can see a diag ram showing the way that a query is pro-\ncessed in figure 1.2.3 1.2 Why should you care about DuckDB?\nFigure 1.1 DuckDB and other tools in the ecosystem\nFigure 1.2 A high-level overview of DuckDB\u2019s query-processing pipeline\nWe can see that DuckDB proc esses queries the same way as  other databases, with an\nSQL parser, query execution planner, an d query runtime. The query engine is vector-\nized, which means it processes chunks of data in parallel and benefits from modern\nmulticore CPU architectures. DuckDB supports several extensions that add new capa-\nbilities to the system, as well as user-defined  functions, and has a variety of user inter-\nfaces, including a CLI, API, and lower-leve l integration into other systems, like data\nprocessing libraries. \n1.2 Why should you care about DuckDB?\nDuckDB makes data analytics fast and fun again, without the need to set up large\nApache Spark clusters or ru n a cloud data warehouse just to process a few hundred\ngigabytes of data. Accessing data from ma ny different sources directly and running\nthe processing where the data resides with out copying it over the wire makes your\nwork faster, si mpler, and cheaper. This not only saves time, but also  a lot of money,\nand reduces frustration. \n For example, we recently had to process AWS access log files residing in S3. Usu-\nally, we would run AWS Athena SQL querie s against the compresse d JSON files. This\ntends to get expensive, with a large part of  the cost being based on the amount of data\nscanned by the analytics service. Now we can instead deploy DuckDB to an EC2 VMOutput data formatsSELECT year, avg(a.value)\nFROM read_csv(...) as aJOIN sales as b  ON a.region = b.regionGROUP BY year\nDuckDB\nData sources\nParser\nSQL StatementUnoptimized\nlogical planOptimized\nlogical planPhysical\nplanPlanner Optimizer Physical planner4 CHAPTER  1An introduction to DuckDB\nand query the files close to the data for a fr action of the cost, as we only pay for the\nVM, not for the processed data volume.\n With DuckDB, you can run lots of experi ments and validate your ideas and hypoth-\neses quickly and locally, all simply by usin g SQL. In addition to supporting the ANSI\nSQL standard, DuckDB\u2019s SQL di alect extends the standard with innovations like the\nfollowing:\n\uf0a1Simplifying SELECT  * queries with SELECT  * EXCLUDE()  and SELECT  * REPLACE()\n\uf0a1Ordering by and grouping results by ALL columns (e.g., GROUP BY ALL  saves the\nuser from typing out all field names)\n\uf0a1Using PIVOT  and UNPIVOT  to transpose rows and columns\n\uf0a1The STRUCT  data type and associated functions,  which make it easy to work with\ncomplex ne sted data\nWe are excited about DuckDB because it helps to simplify data pipelines and data prepa-\nration, allowing more time for the actual analysis, exploration,  and experimentation.\n In this book, we hope to convince  you of the following about DuckDB:\n\uf0a1It is faster than SQLite  for analytical workloads.\n\uf0a1It is easier to set up than a Spark cluster.\n\uf0a1It has lower resource re quirements than pandas.\n\uf0a1It doesn\u2019t throw weird Rust errors like Polars.\n\uf0a1It is easier to set up and use than PostgreSQL, Redshift, and other relational\ndatabases.\n\uf0a1It is faster and more powerful fo r data transformations than Talend. \n1.3 When should you use DuckDB?\nYou can use DuckDB for all analytics tasks that can be expressed in SQL and work on\nstructured data (i.e., tables or documents) as long as yo ur data is already available\n(not streaming) and data volumes don\u2019t ex ceed a few hundred gigabytes. Its columnar\nengine can deal well with both wide tables with many columns as well as large tables\nwith many rows. DuckDB can process a variety of data formats, as previously outlined,\nand can be extended to inte grate with other systems. \n As the data doesn\u2019t leave your system (loc al or privacy-compliant hosting), it\u2019s also\ngreat for analyzing private data, like health information, home automation data, patient\ndata, personal identifying information, financial statements, and similar datasets.\n Here are some examples of some commo n analysis tasks that DuckDB is well\nplaced to solve:\n\uf0a1Analyzing log files where they are stored , without needing to copy them to new\nlocations\n\uf0a1Quantifying personal medical data abou t one\u2019s self, such as a runner might do\nwhen monitoring heart rates\n\uf0a1Reporting on the power generation an d consumption using data from smart\nmeters5 1.5 Use cases\n\uf0a1Optimizing ride data from modern tr ansport operations for bikes and cars\n\uf0a1Preprocessing and pre-cleaning of user -generated data for machine learning\ntraining\nA great use of DuckDB is for more efficientl y processing data that is already available\nin pandas or Polars DataFrames because it can access the data directly without having\nto copy the data from the DataFrame repres entation. The same is true for outputs and\ntables generated by DuckDB. These can be  used as DataFrames  without additional\nmemory usage or transfer. \n1.4 When should you not use DuckDB?\nAs DuckDB is an analytics database, it ha s minimal support for tr ansactions and paral-\nlel write access. Therefore, you couldn\u2019t us e it in applications and APIs that process\nand store input data arriving arbitrarily. \n The data volumes you can process with DuckDB are mostly limited by the main\nmemory of your computer. While it supports spilling ov er memory (out-of-memory\nprocessing) to disk, that fe ature is aimed more at exceptional situations, where the\nfinal portion of processing won\u2019t fit into memory. In most cases, that means you\u2019ll\nhave a limit of a few hundred gigabytes for pr ocessing, with not all of it needing to be\nin memory at the same time, as DuckDB  optimizes loading only what\u2019s needed.\n DuckDB focuses on the long tail of data an alytics use cases, so if you\u2019re in an enter-\nprise environment with a complex setup of da ta sources, tools, and applications pro-\ncessing many terabytes of data, DuckDB might not be the right choice for you.\nDuckDB does not support processing live da ta streams that update continuously. Data\nupdates should happen in bulk by loading new tables or large chunks of new data at\nonce. DuckDB is not a streaming, real-tim e database; you would have to implement a\nbatching approach yourself by setting up a process to create mini-batches of data from\nthe stream and store those mini-batches somewhere that could then be queried by\nDuckDB. \n1.5 Use cases\nThere are many use cases for a tool like Du ckDB. Of course, the mo st exciting is when\nit can be integrated with existing cloud,  mobile, desktop, and command-line applica-\ntions and do its job behind the scenes. In these cases, it would be the equivalent of the\nbroad usage of SQLite today, only for anal ytical processing inst ead of transactional\ndata storage. When analyzing data that shouldn\u2019t leave the us er\u2019s device, such as\nhealth, training, financial or home automati on data, an efficient local infrastructure\ncomes in handy. The local analytics and prep rocessing also reduce the volume of data\nthat has to be transported from edge devices, like smart meters or sensors.\n DuckDB is also useful for fast analysis of larger datasets, such as log files, where\ncomputation and reduction can be done where the data is stored, saving high data\ntransfer time and costs. Cu rrently, cloud vend ors offer expensive analytics services,\nlike BigQuery, Amazon Redshift, and AWS Athena, which charge by processed data6 CHAPTER  1An introduction to DuckDB\nvolume to process this kind of data. You can replace many of those uses with sched-\nuled cloud functions processing the data with DuckDB. You can also chain those pro-\ncessing functions by writing out intermediate  results to cloud storage, which can then\nalso be used for auditing.\n For data scientists, using DuckDB\u2019s stat e-of-the-art query engine can make data\npreparation, analysis, filtering, and aggreg ation more efficient than using pandas or\nother DataFrame libraries\u2014and all of this  without leaving the comfortable environ-\nment of a notebook with Python or R APIs. This will put more advanced data analytics\ncapabilities in the hands of data science users so that they can make better use of\nlarger data volumes while being faster and more efficient. We will show several of\nthese later in the book. Also, the complexi ty of the setup can be greatly reduced,\nremoving the need to involv e a data operations group.\n A final exciting use case will be the distri buted analysis of data between cloud storage,\nthe edge network, and the local device. This  is, for instance, currently being worked on\nby MotherDuck, which allows you to run DuckDB both in the cloud and locally. \n1.6 Where does DuckDB fit in?\nThis book assumes you have some existing da ta that you want to analyze or transform.\nThat data can reside in flat files like CSV,  Parquet, or JSON, or another database sys-\ntem, like PostgreSQL or SQLite. For the book, we provide example data in the book\u2019s\nGitHub repository: https:/ /github.com/duckdb-in-action/examples . \n Depending on your use case, you can use DuckDB transiently to transform, filter,\nand pass the data through to another format (figure 1.3). In most cases, though, you\nwill create tables for your data to persist it for subsequent high-performance analysis.\nWhen doing that, you can also transform an d correct column names, data types, and\nvalues. If your input data is nested docume nts, you can unnest and flatten the data to\nmake relational data analysis easier and more efficient.\nData sources\nOptional persistenceData transformation\nProgramming\nlanguages\nFigure 1.3 Using DuckDB \nin a data pipeline7 1.7 Steps of the data processing flow\n In the next step, you need to determin e which SQL capabilities or DuckDB fea-\ntures can help you perform that analysis or transformation. You can also perform\nexploratory data analysis  (EDA) to quickly get an overvi ew of the distribution, ranges,\nand relationships in your data. \n After getting acquainted with the data, you can proceed to the actual analytics\ntasks. Here, you will build th e relevant SQL statements incrementally, verifying at each\nstep that the sample of the results produced matches your expectations. At this stage,\nyou might create additional tables or view s before using advanced SQL features, like\nwindow functions, common table expressions, and pivots. Finally, you need to decidewhich way the results are consumed: by turn ing them into files or databases again,\nserving them to users through an application or API, or vi sualizing them in a Jupyter\nnotebook or dashboard. \n1.7 Steps of the data processing flow\nIn the following sections, we will describe some specific aspects of DuckDB\u2019s architec-\nture and feature set at a high  level to give you an overal l understanding and apprecia-\ntion. We have ordered the sections in th e sequence of how you would use DuckDB,\nfrom loading data to populating tables an d writing SQL for analysis to visualizing\nthose results, as shown in figure 1.4. \nFigure 1.4 The data processing flow\n1.7.1 Data formats and sources\nDuckDB supports a large numb er of data formats and data sources, and it lets you\ninspect and analyze their data with little ceremony. Unlike other data systems, such as\nSQL Server, you don\u2019t need to first specif y schema details up front. When reading\ndata, the database uses sensible defaults and inherent schema information from the\ndata, which you can override when needed. \nNOTE With DuckDB, you can focus more on  the data processing and analysis\nyou need to do and less on upfront data engineering. Because it is an opensource project built by prac titioners, there is a lot of  emphasis on usability\u2014if\nsomething is too hard to use, some one in the community will propose and\nsubmit a fix. And if the built-in fu nctionality does not reach far enough,\nthere\u2019s probably an extension that addr esses your needs (e.g., geospatial data\nor full-text search).Load\ndataPopulate\ntablesAnalyze\nwith\nSQLUse\nresults8 CHAPTER  1An introduction to DuckDB\nDuckDB supports a vari ety of data formats:\n\uf0a1CSV files can be loaded in bulk and pa rallel, and their co lumns are automati-\ncally mapped.\n\uf0a1DataFrames\u2019 memory can be handled di rectly by DuckDB  inside the same\nPython process without the need to copy data.\n\uf0a1JSON formats can be destructured, flat tened, and transformed into relational\ntables. DuckDB also has a JSON type  for storing this type of data.\n\uf0a1Parquet files, along with their schema  metadata, can be queried. Predicates\nused in queries are pushed down and ev aluated at the Parquet storage layer to\nreduce the amount of data loaded. This  is the ideal columnar format to read\nand write for data lakes.\n\uf0a1Apache Arrow columnar-shaped data ca n be read via Arrow Database Connec-\ntivity (ADBC) without data copying and transformations. \n\uf0a1Accessing data in cloud buckets, like S3 or GCP, reduces transfer and copyinfrastructure and allows for cheap processing of large data volumes.\n1.7.2 Data structures\nDuckDB handles a variety of tables, views, and data types. For table columns, process-\ning, and results, there are more data type s available than just the traditional data\ntypes, like string (varchar),  numeric (integer, float, and decimal), dates, timestamps,\nintervals, Boolean, and binary large objects (BLOBs). \n DuckDB also supports structured data type s like enums, lists, maps (dictionaries),\nand structs:\n\uf0a1Enums \u2014Indexed, named elements of a set that can be stored and processed\nefficiently.\n\uf0a1Lists or arrays \u2014These hold multiple elements of the same type, and there are a\nvariety of functions for operating on these lists.\n\uf0a1Maps \u2014Efficient key\u2013value pairs that can be  used for keeping keyed data points.\nThey are used during JSON processing and can be constructed and accessed inseveral ways.\n\uf0a1Structs \u2014Consistent key\u2013value structures, where the same key always has values\nof the same data type. That allows for more efficient storage, reasoning, and\nprocessing of structs.\nDuckDB also allows you to create your own types and database extensions, which can\nprovide additional data types. DuckDB can also create virtual or derived columns that\nare created from other data via expressions. \n1.7.3 Developing the SQL\nWhen analyzing data, you usually start by ga ining an understanding of the shape of the\ndata. Then, you work from simple queries to creating more an d more complex ones\nfrom the basic building blocks. You can use DESCRIBE  to learn about the columns and9 1.7 Steps of the data processing flow\ndata types of your data sources, tables, and views. Armed with that  information, you can\nget basic statistics and distributions of  a dataset by running count queries, count(*) ,\nglobally or grouped by inte resting dimensions like time, location, or item type. That\ngives you some good insigh ts into what to expect from the data available. \n DuckDB even has a SUMMARIZE  clause ( https:/ /duckdb.org/docs/guides/meta/\nsummarize.html ) that gives you statistics per column:\n\uf0a1count\n\uf0a1min, max, avg, and std (deviation)\n\uf0a1approx_unique  (estimated count of distinct values)\n\uf0a1percentiles  (q25, q50, q75)\n\uf0a1null_percentage  (part of the data being null)\nTo write your analytic s query, you can start working on a subset of the data by using\nLIMIT  or by only looking at a single input f ile. Start by outlining the result columns\nthat you need (these ma y sometimes be converte d\u2014e.g., for dates using strptime ).\nThose are the columns you would group by. Then, apply aggregat ions and filters to\nyour data as needed. There are many di fferent aggregation functions available in\nDuckDB ( https:/ /duckdb.org/docs/sql/aggregates.html ), from traditional ones, like\nmin, avg, and sum, to more advanced ones like histogram , bitstring_agg , list , or\napproximations like approx_count_distinct . There are also advanced aggregations,\nincluding percentiles, entrop y or regression computatio n, and skewness. For running\ntotals and comparisons with previous and next rows, you would use window functions\naggregation  OVER  (PARTITION  BY column  ORDER  BY column2  [RANGE \u2026]) . Repeatedly\nused parts of your analytics statement ca n be extracted into named common table\nexpressions (CTEs) or views. Often, it also helps for readability to move parts of the\ncomputation into subqueries and use their re sults to check for ex istence or do some\nnested data preparation. \n While you\u2019re building up your analytical  statement, you can check the results at\nany time to make sure they are still correc t and you\u2019ve not taken an incorrect detour.\nThis takes us to our next and last sectio n on using the results of your queries. \n1.7.4 Using or processing the results\nYou\u2019ve written your statement and gotten the analytics results quickly from DuckDB.\nNow what?\n It would be useful to keep your results ar ound (e.g., by storing them in a file or a\ntable). Creating a table from your  results is straightforward with CREATE  TABLE  <name>\nAS SELECT  \u2026. DuckDB can write a variety of form ats, including CSV, JSON, Parquet,\nExcel, and Apache Arro w. It also supports other data base formats, like SQLite, Post-\ngres, and others, via custom extensions. For smaller results sets, you can also use the\nDuckDB CLI to output th e data as CSV or JSON.\n But because a picture tells more than 1,000 rows, often the preferred choice is data\nvisualization. With the built-in bar function, you can render inline bar charts of your10 CHAPTER  1An introduction to DuckDB\ndata. You can also use command-line plotting tools, like youplot , for some quick\nresults in your terminal. \n In most cases, though, you would use th e large Python and JavaScript ecosystem to\nvisualize your results. For those purposes, you can turn your results into DataFrames,\nwhich then can be rendered into a variety of charts with matplotlib ; ggplot  in\nPython; ggplot2  in R; or d3, nivo , or observable  in JavaScript. A visual representa-\ntion showing this is provided in figure 1.5.\nFigure 1.5 Visualizing data in a dashboard or Jupyter Notebook\nAs DuckDB is so fast, you can serve the resu lts directly from your queries on the data\nvia an API that web, command-line, or mo bile clients can consume. You only really\nneed a traditional client\u2013datab ase server setup if your sour ce data is too big to move\naround and your results are comparativel y small (much less than 1% of the volume).Line\nPie\nCalendar heatmap11 Summary\nOtherwise, you can embed DuckDB into your  application (e.g., built with Streamlit)\nor dashboarding tool and have it run on lo cal raw data or a local DuckDB database. \nSummary\n\uf0a1DuckDB is a newly developed analytical database that excels at in-memory pro-\ncessing.\n\uf0a1The database supports an extended dialect of SQL and gains new capabilitieswith extensions.\n\uf0a1DuckDB can read a variety of formats na tively from local and remote sources.\n\uf0a1The integration in Python, R, and othe r languages is seamless and efficient.\n\uf0a1As an in-process database, it can proc ess data efficiently without copying.\n\uf0a1In addition to the traditional data types, DuckDB also supports lists, maps,\nstructs, and enums.\n\uf0a1DuckDB provides a lot of functions on data types and values, making data pro-cessing and shaping much easier.\n\uf0a1Building up your SQL queries step by step after learning about the shape of\nyour data helps you stay in control.\n\uf0a1You can use the results of your query in a variety of ways, from generating\nreports and visualizing in charts to outputting in new formats. 12Getting started\nwith DuckDB\nNow that we have an understanding of what DuckDB is and why it came into prom-\ninence in the early 2020s, it\u2019s time to ge t familiar with it. This chapter will be cen-\ntered on the DuckDB command-line interface  (CLI). We\u2019ll learn how to install it on\nvarious environments, before learning a bout the its built-in commands. We\u2019ll con-\nclude by querying a remote CSV file.\n2.1 Supported environments\nDuckDB is available for a range of diff erent programming languages and operating\nsystems (Linux, Windows, and macOS) both for Intel/AMD and ARM architec-\ntures. At the time of writing, there is  support for the command line, Python, R,\nJava, JavaScript, Go, Rust, Node.js, Julia,  C/C++, ODBC, JDBC, WASM, and Swift. In\nthis chapter, we will focus on the DuckDB command line exclusively, as we thinkThis chapter covers \n\uf0a1Installing and learning how to use the DuckDB \ncommand-line interface \n\uf0a1Executing commands in the DuckDB CLI\n\uf0a1Querying remote files13 2.3 Using the DuckDB CLI\nthat is the easiest way to get you up to sp eed. The DuckDB CLI does not require a sep-\narate server installation, as DuckDB is an embedded database, and in the case of the\nCLI, it is embedded in the CLI executable. \n The command-line tool is published to GitHub releases, and there are a variety of\npackages for different operating systems and architectures. You ca n find the full list\non the installation page: https:/ /duckdb.org/doc s/installation/index . \n2.2 Installing the DuckDB CLI\nThe installation is a \u201ccopy to\u201d installation , meaning no installe rs or libraries are\nneeded. The CLI consists of  a single binary named duckdb . Let\u2019s learn how to go\nabout installing DuckDB. \n2.2.1 macOS\nOn macOS, the official recommendation is to use the Homebrew ( https:/ /brew.sh )\npackage installer, as shown in the following listing. \n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/\\\nHomebrew/install/HEAD/install.sh)\"\nbrew install duckdb\n2.2.2 Linux and Windows\nThere are several different packages ava ilable for Linux and Windows, depending on\nthe particular architecture and version that you\u2019re using. You can find a full listing on\nthe GitHub releases page ( https:/ /github.com/duckdb/duckdb/releases ). In the fol-\nlowing listing, we learn how to get the Du ckDB CLI running on Linux with an AMD64\narchitecture. \nwget https://github.com/duckdb/duckdb/releases/download/v0.10.0/\\\nduckdb_cli-linux-amd64.zip\nunzip duckdb_cli-linux-amd64.zip./duckdb -version\n2.3 Using the DuckDB CLI\nThe simplest way to launch the CLI is as foll ows\u2014and yes, it\u2019s that short, and it\u2019s quick:\nduckdb\nThis will launch DuckDB and the CLI. You should see something like the following\noutput:Listing 2.1 Installing DuckDB on macOS via Homebrew\nListing 2.2 Getting DuckDB running on LinuxThis is only necessary to install the \nHomebrew package manager itself\u2014\ndon\u2019t run it if you already have it.\nDon\u2019t forget to update th is link to the latest \nversion from the GitHub releases page \n(https://github.com/du ckdb/duckdb/releases).14 CHAPTER  2Getting started with DuckDB\nv0.10.0 20b1486d11\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.Use \".open FILENAME\" to reopen on a persistent database.\nThe database will be transient, with all da ta held in memory. It will disappear when\nyou quit the CLI, which you can do by typing .quit  or .exit .\n2.3.1 SQL statements\nYou can enter or paste SQL statements di rectly in the command line and end them\nwith a semicolon and a newline. While there is no semicolon, you can enter newlines.They will be executed directly and output the results in compact table format. You can\nchange the output formats as  explained in section 2.5.1.  For longer running opera-\ntions, a progress bar will be shown. The fo llowing listing provides a simple example\nselecting a few constant values.\nselect v.* from values (1),(3),(3),(7) as v;\nBy default, it will be printed in a tabular format:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0 \u2502\n\u2502 int32 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021 \u2502\n\u25023 \u2502\u25023 \u2502\n\u25027 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n2.3.2 Dot commands\nIn addition to SQL statements and comm ands, the CLI has several special commands\nthat are only available in the CLI: the sp ecial dot commands. To use one of these com-\nmands, begin the line with a period (.) im mediately followed by the name of the com-\nmand you wish to execute. Additional arguments to the command are entered, space\nseparated, after the command. Dot commands  must be entered on a single line, and\nno whitespace may occur before the period . No semicolon is required at the end of\nthe line in contrast to a normal SQL statement or command. \n Some of the most popular dot commands are described as follows:\n\uf0a1.open  closes the current database file and opens a new one.\n\uf0a1.read  allows reading SQL files to execute from within the CLI.\n\uf0a1.tables  lists the currently avai lable tables and views.\n\uf0a1.timer on/off  toggles SQL timing output.\n\uf0a1.mode  controls output formats.Listing 2.3 A simple select statement15 2.4 DuckDB\u2019s extension system\n\uf0a1.maxrows  controls the number of rows to show by default (for duckbox  format).\n\uf0a1.excel  shows the output of next command in spreadsheet.\n\uf0a1.exit , .quit  or ctrl-d  exit the CLI.\nA full overview ca n be retrieved via .help . \n2.3.3 CLI arguments\nThe CLI takes in arguments that can be used  to adjust the databa se mode, control the\noutput format, or decide whether the CLI is going to enter interactive mode. The\nusage is duckdb  [OPTIONS]  FILENAME  [COMMANDS] . \n Some of the most popular CLI ar guments are described as follows:\n\uf0a1-readonly  opens the database in read-only mode.\n\uf0a1-json  sets the output mode to json .\n\uf0a1-line  sets the output mode to line .\n\uf0a1-unsigned  allows for the loading of unsigned extensions.\n\uf0a1-s COMMAND  or -c COMMAND  runs the provided command and then exits. This is\nespecially helpful when combined with the .read  dot command, which reads\ninput from the given filename.\nThe following is an example that demonstr ates how the CLI can be parameterized to\noutput the results of a query as JSON:\nduckdb --json -c 'select v.* from values (1),(3),(3),(7) as v;'\n[{\"col0\":1},\n{\"col0\":3},{\"col0\":3},{\"col0\":7}]\nTo get a list of the avai lable CLI arguments, call duckdb  -help . \n2.4 DuckDB\u2019s extension system\nDuckDB has an extension system used to hous e functionality that isn\u2019t part of the core\nof the database. You can think of extensio ns as packages that you can install with\nDuckDB. \n DuckDB comes preloaded with several extensions, which vary depending on the\ndistribution that you\u2019re using. You can get a list of all the available extensions, whether\ninstalled or not, by calling the duckdb_extensions  function. Let\u2019s start by checking\nthe fields returned by this function. \nDESCRIBE\nSELECT *\nFROM duckdb_extensions();Listing 2.4 The format of duckdb_extensions  output16 CHAPTER  2Getting started with DuckDB\nThe duckdb_extensions  function returns, among other information, the name of the\nextension and whether it is installed and actually loaded:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502\u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500|\u2502 extension_name \u2502 VARCHAR \u2502\u2502 loaded \u2502 BOOLEAN \u2502\u2502 installed \u2502 BOOLEAN \u2502\u2502 install_path \u2502 VARCHAR \u2502\u2502 description \u2502 VARCHAR \u2502\u2502 aliases \u2502 VARCHAR[] \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\nLet\u2019s check which exte nsions we have installed on our machine:\nSELECT extension_name, loaded, installed\nfrom duckdb_extensions()ORDER BY installed DESC, loaded DESC;\nThe results of running the query are as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 extension_name \u2502 loaded \u2502 installed \u2502\u2502 varchar \u2502 boolean \u2502 boolean \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 autocomplete \u2502 true \u2502 true \u2502\u2502 fts \u2502 true \u2502 true \u2502\u2502 icu \u2502 true \u2502 true \u2502\u2502 json \u2502 true \u2502 true \u2502\u2502 parquet \u2502 true \u2502 true \u2502\u2502 tpch \u2502 true \u2502 true \u2502\u2502 httpfs \u2502 false \u2502 false \u2502\u2502 inet \u2502 false \u2502 false \u2502\u2502 jemalloc \u2502 false \u2502 false \u2502\u2502 motherduck \u2502 false \u2502 false \u2502\u2502 postgres_scanner \u2502 false \u2502 false \u2502\u2502 spatial \u2502 false \u2502 false \u2502\u2502 sqlite_scanner \u2502 false \u2502 false \u2502\u2502 tpcds \u2502 false \u2502 false \u2502\u2502 excel \u2502 true \u2502 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 15 rows 3 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nYou can install any extension by typing the INSTALL  command followed by the exten-\nsion\u2019s name. The extension will then be in stalled in your database but not loaded. To\nload an extension, type LOAD  followed by the same name. The extension mechanism is\nidempotent, meaning you ca n issue both commands seve ral times without running\ninto errors. 17 2.5 Analyzing a CSV file with the DuckDB CLI\nNOTE Since version 0.8 of DuckDB, the da tabase autoloads installed exten-\nsions if it can determine they are needed, so you might not need the LOAD\ncommand. \nBy default, DuckDB cannot query files that  live elsewhere on the internet, but that\ncapability is availa ble via the official httpfs  extension. If it is no t already in your distri-\nbution, you can install and load the httpfs  extension:\nINSTALL httpfs;\nLOAD httpfs;\nThis extension lets us directly query files hosted on an HTTP(S) server without having\nto download the files locally, and it suppor ts S3 as well as a few other cloud storage\nproviders. We can then chec k where that\u2019s been installed by entering the following:\nFROM duckdb_extensions()\nSELECT loaded, installed, install_pathWHERE extension_name = 'httpfs';\nYou should see this output:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500----\u2510\u2502 loaded \u2502 installed \u2502 install_path \u2502\u2502 boolean \u2502 boolean \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 true \u2502 true \u2502 /path/to/httpfs.duckdb_extension \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe can see that this extension has now been  loaded and installed and also view the\nlocation where it\u2019s been installed. \n2.5 Analyzing a CSV file  with the DuckDB CLI\nWe\u2019re going to start with a demonstration of the CLI for a common task for any data\nengineer\u2014making sense of the data in a CSV file! It doesn\u2019t matter where our data is\nstored, be it on a remote HT TP server or cloud storage (S3, GCP, or HDFS), DuckDB\ncan now process it directly without having to do a manu al download and import pro-\ncess. As the ingestion of many  supported file formats, such as CSV and Parquet, is par-\nallelized by default, it should be lightni ng quick to get your  data into DuckDB. \n We went looking for CSV files on GitHub  and came across a dataset containing the\ntotal population figures for several countries ( https:/ /mng.bz/KZKZ ). We can write\nthe following query to count the number of records:\nSELECT count(*)\nFROM 'https://github.com/bnokoro/Data-Science/raw/master/'\n'countries%20of%20the%20world.csv';\nIf we run this query, we should see the fo llowing output indicating we\u2019ve got popula-\ntion data for over 200 countries:18 CHAPTER  2Getting started with DuckDB\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 227 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIf, as is the case here, our URL or filename  ends in a specific extension (e.g., .csv),\nDuckDB will automatically pr ocess it. But what if we tr y to automatically process a\nshort link of that  same CSV file?\nSELECT count(*)\nFROM 'https://bit.ly/3KoiZR0';\nRunning this query result s in the following error:\nError: Catalog Error: Table with name https://bit.ly/3KoiZR0 does not exist!\nDid you mean \"Player\"?LINE 1: select count(*) from 'https://bit.ly/3KoiZR0';\nAlthough it\u2019s a CSV file, DuckDB doesn\u2019t know that because it doesn\u2019t have a .csv  suf-\nfix. We can solve this problem by using the read_csv_auto  function, which processes\nthe provided URI as if it was a CSV file, despite its lack of .csv  suffix. The updated\nquery is shown in the following listing. \nSELECT count(*)\nFROM read_csv_auto(\"https://bit.ly/3KoiZR0\");\nThis query will return the same result as  the query that used the canonical link from\nwhich the format could be deduced.\n2.5.1 Result modes\nFor displaying the results, you can choose between different modes using .mode\n<name> . You can see a list of available modes by typing .help mode . \n Throughout this chapter, we\u2019ve been using duckbox  mode, which returns a flexible\ntable structure. DuckDB comes with a series  of different modes, which broadly fit into\ntwo categories:\n\uf0a1Table based \u2014These types of modes work well  with few columns and include\nduckbox , box, csv, ascii , table , list , and column .\n\uf0a1Line based \u2014These types of modes work well  with many columns and include\njson , jsonline , and line .\nThere are some others that don\u2019t fit into those categories, including html , insert ,\nand trash  (no output).\n Our first query counted the number of reco rds in the CSV file, but it\u2019d be interest-\ning to know what columns it has. Many colu mns would get truncated if we were to use\nthe default mode, so we\u2019re going to change to line  mode before running the query:Listing 2.5 Specifying the format of a remote file19 2.5 Analyzing a CSV file with the DuckDB CLI\n.mode line\nSELECT *\nFROM read_csv_auto(\"https://bit.ly/3KoiZR0\")LIMIT 1;\nThe results of running this query are shown in the following listing.\nCountry = Afghanistan\nRegion = ASIA (EX. NEAR EAST)\nPopulation = 31056997\nArea (sq. mi.) = 647500\nPop. Density (per sq. mi.) = 48,0\nCoastline (coast/area ratio) = 0,00\nNet migration = 23,06\nInfant mortality (per 1000 births) = 163,07\nGDP ($ per capita) = 700\nLiteracy (%) = 36,0\nPhones (per 1000) = 3,2\nArable (%) = 12,13\nCrops (%) = 0,22Other (%) = 87,65\nClimate = 1\nBirthrate = 46,6Deathrate = 20,34\nAgriculture = 0,38\nIndustry = 0,24\nService = 0,38\nAs you can see from the output, line  mode takes up a lot more space than duckbox ,\nbut we\u2019ve found it to be the best mode fo r doing initial exploration of datasets that\nhave plenty of columns. You can always change back to another mode once you\u2019vedecided on a subset of co lumns you\u2019d like to use.\n The dataset has lots of interesting inform ation about various countries. Let\u2019s write\na query to count the number of countries and find the maximum population average\narea across all countries. This query only re turns a few columns, so we\u2019ll switch back to\nduckbox  mode before running the query:\n.mode duckbox\nSELECT count(*) AS countries,\nmax(Population) AS max_population,round(avg(cast(\"Area (sq. mi.)\" AS decimal))) AS avgArea\nFROM read_csv_auto(\"https://bit.ly/3KoiZR0\");\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 countries \u2502 max_population \u2502 avgArea \u2502\u2502 int64 \u2502 int64 \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 227 \u2502 1313973713 \u2502 598227.0 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Listing 2.6 A result in line  modeChanging to \nline mode20 CHAPTER  2Getting started with DuckDB\nSo far, no tables have been created in th e process, and we\u2019ve just touched the tip of\nthe iceberg of demonstrating what DuckDB actually can do. While the previous exam-\nples have all been run in interactive mode , the DuckDB CLI can also run in a nonin-\nteractive fashion. It can read from standa rd input and write to standard output. This\nmakes it possible to build all sorts of pipelines.\n Let\u2019s conclude with a script  that extracts the population, birth rate, and death rate\nin countries in Western Europe and creates a new local CSV file containing that data.We can either \n.exit  from the DuckDB CLI or open another tab before running the\nfollowing command:\nduckdb -csv \\\n-s \"SELECT Country, Population, Birthrate, Deathrate\nFROM read_csv_auto('https://bit.ly/3KoiZR0')WHERE trim(region) = 'WESTERN EUROPE'\" \\\n> western_europe.csv\nThe first few lines of western_europe.csv can be viewed with a command-line tool or\ntext editor. If we use the head  tool, we can find the first six lines\u2014the header and five\nrows of data\u2014like this:\nhead -n6 western_europe.csv\nThe output would then look like table 2.1.\nWe can also create Parquet files, but for that , we can\u2019t pipe the output straight into a\nfile with a Parquet extension. Instead, we can use the COPY \u2026 TO  clause with the file-\nname as the destination, as shown in the following listing.\nduckdb \\\n-s \"COPY (\nSELECT Country, Population, Birthrate, DeathrateFROM read_csv_auto('https://bit.ly/3KoiZR0')\nWHERE trim(region) = 'WESTERN EUROPE'\n) TO 'western_europe.parquet' (FORMAT PARQUET)\"Table 2.1 The first six lines of western_europe.csv, showing population, birth rate, and death rate of\nsome countries in Western Europe\nCountry Population Birthrate Deathrate\nAndorra 71,201 8,71 6,25\nAustria 8,192,880 8,74 9,76\nBelgium 10,379,067 10,38 10,27Denmark 5,450,661 11,13 10,36\nFaroe Islands 47,246 14,05 8,7\nListing 2.7 Writing explicitly to a Parquet file21 Summary\nYou could then view the contents of the Pa rquet file using any Parquet reader, perhaps\neven DuckDB itself!\nduckdb -s \"FROM 'western_europe.parquet' LIMIT 5\"\nThe results will be the same as those in table 2.1.\nSummary\n\uf0a1DuckDB is available as a library for Py thon, R, Java, JavaScript, Julia, C/C++,\nODBC, WASM, and Swift.\n\uf0a1The CLI supports addition al dot commands for contro lling outputs, reading\nfiles, built-in help, and more.\n\uf0a1With .mode , you can use several display modes, including duckbox , line , and\nascii .\n\uf0a1You can query CSV files directly from  an HTTP server by installing the httpfs\nextension.\n\uf0a1You can use the CLI as a step in any data pipeline, without creating tables, by\nquerying external datasets and writing re sults to standard ou t or other files. Config file for repeated configuration and use\nRepeated configuration and usage can be stored in a config file that lives at $HOME/\n.duckdbrc. This file is read during startup, and all commands in it\u2014both dot com-\nmands and SQL commands\u2014are executed via one .read  command. This allows you\nto store both the configuration state of the CLI and anything you might want to initial-\nize with SQL commands. \nAn example of something that might go in the duckdbrc file is a custom prompt and\nwelcome message when you launch DuckDB, like this:\n-- Duck head prompt\n.prompt 'O> '-- Example SQL statement\nselect 'Begin quacking now '||cast(now() as string) as \"Ready, Set, ...\";22Executing SQL queries\nNow that you\u2019ve learned abou t the DuckDB CLI, it\u2019s time to tickle your SQL brain.\nWe will be using the CLI version of DuckDB  throughout this chapter. However, all\nthe examples here can be fully applied from within any of the supported environ-ments, such as the Python client, the Ja va JDBC driver, or any of the other sup-\nported language interfaces.\n In this chapter, we will quickly go ov er some basic and necessary SQL statements\na n d  t h e n  m o v e  o n  t o  m o r e  a d v a n c e d  q u e r y i n g .  I n  a d d i t i o n  t o  e x p l a i n i n g  S Q L\nbasics, we\u2019ll also be covering more complex topics, including common table\nexpressions and window functions. Duck DB supports both of these, and thisThis chapter covers \n\uf0a1The different categories of SQL statements and \ntheir fundamental structure\n\uf0a1Creating tables and structures for ingesting a real-world dataset\n\uf0a1Laying the fundamentals for analyzing a huge dataset in detail\n\uf0a1Exploring DuckDB-specific extensions to SQL23 3.2 Analyzing energy production\nchapter will teach you how to build queries for doing the best possible in-memory\nonline analytical processi ng (OLAP) with DuckDB. \n To get the sample up and running, you should have an idea about data ingestion\nwith DuckDB, as discussed in chapter 2, es pecially how to ingest CSV files and deal\nwith implicit (automatic) or explicit colu mn detection. Knowledge of the data types\npresented in chapter 1 will also be helpful. If  you want to go straight to querying data,\nplease jump to section 3.4.3,  in which we discuss SQLs SELECT  statement in detail. We\nthink it\u2019s better to start by defining tables  and structures first, populating them with\ndata, and then querying them, rather than making up queries on generated or nonex-\nistent data.\n3.1 A quick SQL recap\nSQL queries  are composed of several statemen ts, which are in turn composed of\nclauses. A command  is a query submitted to the CLI or any other of the supported cli-\nents. Commands in the DuckDB CLI are terminated with a semicolon. Whitespaces\ncan be used freely in SQL commands. You ha ve the option to either align your com-\nmands beautifully or type them all in one line\u2014it doesn\u2019t matter which you choose.\nSQL is case-insensitive fo r keywords and identifiers.\n Most statements support several clauses that change their behavior, most promi-\nnently the WHERE , GROUP  BY, and ORDER  BY clauses. WHERE  adds conditions on which\nrows are included in the final result, GROUP  BY aggregates many values into buckets\ndefined by one or more keys, and ORDER  BY specifies the order of the results returned. \n Next, we will demonstrate how to use each of the statements and clauses relevant\nto your analytical workloads using a real-world example: energy production from pho-\ntovoltaics. The aim of this example is to pr ovide you with concrete details on each of\nthe concepts so that you leave with an un derstanding of how they are applicable in\nyour own workloads. \n3.2 Analyzing energy production\nEnergy consumption and production have be en the subjects of OLAP-related analysis\nfor a while. Smart meters measuring cons umption in 15-minute intervals have been\navailable to many industries\u2014such as meta l processing and large production plants\u2014\nfor some time now and have become quite standard. These measurements are used toprice the consumed energy, forecast consumption, and more. \n With the rise of smart monitoring system s, detailed energy readings are now avail-\nable in private households as well, becomi ng more mainstream each year. Imagine you\nhave a photovoltaic grid and smart meter inst alled at your house. You want to be able\nto plan your electricity usage a bit or forecast  an amortization of your grid, the same way\nlarge industries can. To do so, you don\u2019t have  to go into a full time-series database and\na live dashboard. Hopefully, DuckDB and the examples we use throughout this chapterprovide a good starting point for creating your own useful reports.24 CHAPTER  3Executing SQL queries\n The dataset we are going to use in the following examples is available from the\nU.S. Department of Energy under the name  Photovoltaic Data Acquisition (PVDAQ;\nhttps:/ /mng.bz/9d6o ). The dataset is documented on GitHub ( https:/ /github.com/\nopenEDI/documentation/blob/main/pvdaq.md ). The National Renewable Energy\nLaboratory from the Department of Energy also offers a nice, simple API for getting\nthe partitioned CSV/Parquet files via PVDAQ ( https:/ /developer.nrel.gov/docs/\nsolar/pvdaq-v3/ ). Access is free and requires littl e personal information. The dataset\nis published under the Creative  Commons Attribution license ( http:/ /opendefinition\n.org/licenses/cc-by/ ). Parts of the dataset are redi stributed unchan ged for ease of\naccess in this chapter with the sources of this book. \nNOTE Why are we storing measurements in quarter-hour intervals when\nmodern sensors produce much finer me asurements? A period of 15 minutes\nturned out to be fit enough for the af orementioned purposes , such as pricing\nand buying smart intervals,  while simultaneously small enough to be handled\nwith ease in most modern relational systems. The power output or consump-\ntion is measured in units of watts (W) or kilowatts (kW) and is typically soldusing kilowatt-hours (kWh). These 15-minute intervals can be easily con-\nverted from W to kWh, while remaining accurate enough for good produc-\ntion charts. In most cases, you want to  smooth the values on at least an hourly\nbasis\u2014peaks and dips due to clouds ar e often irrelevant. Reviewing a weather\nforecasting chart and using daily measurements generally provides a good\nbase interval, as this accounts for weekends and bank holidays and smooths\nout small irregularities.\n3.2.1 Downloading the dataset\nWe will use DuckDB\u2019s httpfs  extension to load the data without going through CSV\nfiles. To install it, run install httpfs;  load  httpfs;  in your DuckDB CLI. We\u2019ll be\nworking with the fo llowing data files:\n\uf0a1https://oedi-data-lake.s3.amaz onaws.com/pvdaq/csv/systems.csv \u2014This file contains\nthe list of all PV systems the PVDAQ measures.\n\uf0a1Readings for systems 10, 34, and 1,200 from 2019 and 2020 \u2014The URLs all follow\nthe schema discussed in the forthcoming text (please change the system_id\nand year  URL parameters accordingly). You\u2019ll need an API key to access\nthem\u2014we are using DEMO_KEY .\nThe URLs used to get the data are described as follows, with the API key, system ID,\nand year all supp lied via query string parameters:\nhttps://developer.nrel.gov/api/pvdaq/v3/data_file?api_key=DEMO_KEY\n&system_id=34&year=2019\nIf you can\u2019t access those URLs for any reason, the source code of this book contains a\ndatabase export under the name ch03_db , which has the comple te dataset. You can\nimport it into a fresh database  by using the following commands:25 3.2 Analyzing energy production\nduckdb my_ch03.db\nimport database 'ch03_db';\nTIP Another option is to use a remote database on motherduck.com  via\nATTACH  'md:_share/duckdb_in_action_ch3_4/d0c08584-1d33-491c-8db7-\ncf9c6910eceb'  in your DuckDB CLI. While th e shared example is read-only,\nit contains all data we used, and you can follow all examples that don\u2019t deal\nwith insertion and the like. Chapter 12  will cover the services offered by\nMotherDuck in detail.\nWe picked this dataset for specific reasons:  its domain is easy to grasp yet complex\nenough to introduce many analytical concepts  backed with actual real-world needs. As\nwith any analytical process, you will eventua lly run into inconsistent data. This is the\ncase in some series in this dataset too.\n If you don\u2019t use the ready-made database , don\u2019t worry yet about the necessary que-\nries for ingesting the raw data\u2014we will get th ere in a bit. In the upcoming sections, we\nwill first discuss and create the database schema and then download the readings for\nseveral PV systems. \n3.2.2 The target schema\nDuckDB is a relational database management system  (RDBMS). That means it is a system\nfor managing data stored in relations. A relation  is essentially a mathematical term for\na table. \n Each table is a named collection of rows. Each row of a given table has the same set\nof named columns, and each co lumn is of a specific data type. Tables themselves are\nstored inside schemas, and a collection of schemas constitu tes the entire database you\ncan access.\nNOTE What is a surrogate key ? To address rows in a t a b l e ,  a  c o l u m n  w i t h  a\nunique value or a combination of colu mns that is unique over all rows is\nrequired. Such a column is usually referred to as the primary key. Not all data\nyou can possibly store in a database has unique attributes. For example, using\na person\u2019s name as their unique or pr imary key would be an awful choice. In\nsuch scenarios, database schema de signers often introd uce numerical col-\numns based on a monotonous, increasing  sequence or columns containing a\nuniversally unique identifier  (UUID) as surrogate keys . \nThe schema for our data set consists of a handful of tables (figure 3.1). These tables\nare normalized so that the supported join s can easily be demonstrated. The three\ntables we\u2019ll be working with are as follows:\n\uf0a1systems \u2014This contains the systems for wh ich production values are read.\n\uf0a1readings \u2014This contains the actual re adings taken for the systems.\n\uf0a1prices \u2014This contains the prices for selling energy. Prices in our examples are\nmeasured in European cents per kilowatt-hour (cents/kWh), but using any unitper kilowatt-hour will work.26 CHAPTER  3Executing SQL queries\nFigure 3.1 Energy consumption schema\nThe systems  table uses the ID defined in the CSV se t. We treat it as an externally gen-\nerated surrogate key. Th prices  table uses a SEQUENCE , and the readings  table uses a\nconcatenated natural key  (the ID of the system they have been read from plus the time-\nstamp at which they have been read). \n3.3 Data definition language queries\nWe have already seen that you can query many sources with DuckDB without creating\na schema containing tables first. DuckDB , however, is a full-fledged RDBMS, and we\nwill use data definition language (DDL) qu eries to create our target schema prior to\ningesting our dataset. New tables are created with the CREATE TABLE  statement, and\nexisting tables can be altered with the ALTER TABLE  statement. If you don\u2019t need a\ntable anymore, you will want to use DROP TABLE . \nNOTE DuckDB supports the entire collection of data definition language\nclauses, but we only use a subset of th em in this chapter for brevity\u2019s sake. Be\nsure to consult the statements documentation ( https:/ /duckdb.org/docs/\nsql/statements/overview ) to see all the supported clauses.\n3.3.1 The CREATE TABLE statement\nLet\u2019s create the table for the systems we are going to monitor with the CREATE  TABLE\nstatement. You must specify the name of th e table to create and the list of columns.\nOther options, such as modifiers to the whole statement, are optional. The column\nlist is defined by the name of the column  followed by a type  and optional column\nconstraints. systems\nid\nidINTEGER\nname VARCHAR\nINTEGERreadings\npricessystem_id  INTEGER\nread_on  TIMESTAMP\npower123\nABC\n123\n123INTEGER\nTIMESTAMP\nDECIMAL(8,3)\nvalue DECIMAL(5,2) DECIMAL(5,2)\nvalid_from DATE\nvalid_until DATEDATE\nDATE123\n12327 3.3 Data definition language queries\n \nCREATE TABLE IF NOT EXISTS systems (\nid INTEGER PRIMARY KEY,\nname VARCHAR(128) NOT NULL\n);\nIn the previous examples, as well as upcomi ng examples that build upon the table cre-\nated here, we have made use of various cons traints: primary and un ique keys as well as\nforeign keys. We do this not only to demonstrate the available options of the CREATE\nSTATEMENT  but also because we care about data integrity. \n In DuckDB\u2014as with almost any other da tabase\u2014constraints usually have a nega-\ntive effect on performance when loading a huge amount of data, as the indexes that\nback those constraints must be recreated,  respectively updated, and rules must be\nchecked. If you don\u2019t need integr ity checks, omit those constraints.\nNOTE DuckDB also offers a CREATE  OR REPLACE  TABLE  s t a t e m e n t .  T h i s  w i l l\ndrop an existing table and replace it with the new definition. We prefer the IF\nNOT EXISTS  clause, though, as we consider it safer than unconditionally drop-\nping a table, since any potential data will be gone afterward. \nThe definition of the readings  table looks slightly different. The table uses a compos-\nite primary key. This is a key composed of the reference column system_id , which\npoints back to the systems  table and the timestamp colu mn containing the date and\ntime the value was read. Such a primary key constraint cannot be directly defined with\none of the columns but goes outside the column list. \nCREATE TABLE IF NOT EXISTS readings (\nsystem_id INTEGER NOT NULL,\nread_on TIMESTAMP NOT NULL,power DECIMAL(10,3) NOT NULL\nDEFAULT 0 CHECK(power >= 0),\nPRIMARY KEY (system_id, read_on),FOREIGN KEY (system_id)\nREFERENCES systems(id)\n);\nFinally, we cover the prices  table. The script for it actu ally contains two commands, as\nwe are going to use an incrementing nume ric value as the surrogate primary key. We\ndo this by using a DEFAULT  declaration with a function call to nextval() . This functionListing 3.1 A basic CREATE TABLE  statement\nListing 3.2 Creating the readings  table with an idempotent statementIF NOT EXISTS is an optional clause that \nmakes the whole command idempotent, so it does not fail if the table already exists.\nPRIMARY KEY makes this a mandatory column \nthat serves as a primary, and therefore \nunique, key. An index will also be added.This modifier makes the column a\nmandatory column (literal NULL\nvalues cannot be inserted).\nHere, several clauses are used to ensure \ndata quality: a default value of 0 is assumed for the power readings, and since \nan additional column check constraint is \nused, no negative values are inserted.\nThis is how a composite primary key \nis defined after the list of columns.\nForeign key constraints are also \ntable constraints and go after \nthe column definitions.28 CHAPTER  3Executing SQL queries\ntakes the name of a sequence as input. Se quences are numeric values stored in the\ndatabase outside table definition s. A sequence is created via CREATE  SEQUENCE . \nCREATE SEQUENCE IF NOT EXISTS prices_id\nINCREMENT BY 1 MINVALUE 10;\nCREATE TABLE IF NOT EXISTS prices (\nid INTEGER PRIMARY KEY\nDEFAULT(nextval('prices_id')),\nvalue DECIMAL(5,2) NOT NULL,valid_from DATE NOT NULL,\nCONSTRAINT prices_uk UNIQUE (valid_from)\n);\nWhy don\u2019t we use valid_from  as the primary key? In the initial applicat ion, we might\nbe only dealing with selling prices, but in the future, we might be dealing with buying\nprices too. There are several ways to model that, such as using an additional table or\nintroducing a type  column to the prices  table that specifies wh ether a certain value is\na selling or buying price. Using valid_from  as a primary key would prevent two prices\nwith different types from being valid from  the same date. Therefore, you would need\nto change a simple primary key to a compos ite one. While other databases might allow\ndropping and recreating primary and unique keys, DuckDB does not, so in this case,you would need to go through a bigger migration. \n Additionally, updating the values of primary keys can be costly on its own, not only\nfrom an index perspective but also from an organizational one (e.g., if the column has\nalready been used as a refere nce column for a foreign key) . Every constraint is backed\nby an index, and changing va lues often requires a reorgani zation of that index, which\ncan be slow and costly. Updating several ta bles in one transaction is a common source\nof errors, which often lead to inconsistenc ies. That danger is not present in the\nreadings  table, where we used the timestamp  column as the primary key because the\nreadings are essentially immutable. \nTIP Review the existing sequences in your database using SELECT sequence_\nname  FRIN  duckdb_sequences(); .\n3.3.2 The ALTER TABLE statement\nDefining a schema is a complex task, and or ganizations usually put a lot of effort into\nit. However, you will rarely encounter a case  in which a schema covers all eventualities\nand is completely correct from the start. Requirements change al l the time. Having a\nrequirement to capture the validity of a pric e, for example, makes an additional col-\numn necessary. In that case, use the ALTER TABLE  statement:\nALTER TABLE prices\nADD COLUMN IF NOT EXISTS valid_until DATE;Listing 3.3 Creating the prices  table with a primary key based on a sequence\nThis creates a monoto nous incrementing \nsequence, starting with 10.\nThis uses the nextval() function as \na default value for the id column.\nThis adds a unique table constraint \nfor the valid_from column.\nMany DDL-related statements  support an IF NOT EXISTS\nclause, which makes them less error-prone when\nworking with existing schemas.29 3.3 Data definition language queries\nAdditionally, with ALTER TABLE , we can DROP  and RENAME  the column as well as RENAME\nthe table. Some column options, such as default values, can be changed; however, add-\ning, dropping, or changing constraints is no t supported at the time of writing. If you\nwant to do that, you\u2019ll need to recreate the table.\n There are other ways to create tables, including Create table as select  (CTAS). This is\na shortcut that duplicates th e shape of a table and its content in one go. For example,\nwe could create a duplicate of the prices  table like this:\nCREATE TABLE prices_duplicate AS\nSELECT * FROM prices;\nWe could also add a LIMIT 0  clause to copy the schema of a table without data or a\nWHERE  clause with conditions to copy the shape together with some data. \n3.3.3 The CREATE VIEW statement\nThe CREATE VIEW  statement defines a view  of a query. It essent ially stores the state-\nment that represents the query, includin g all conditions and transformations. The\nview will behave as any ot her table or relation when being queried, and additional\nconditions and transformations can be ap plied. Some databases materialize views,\nwhile others don\u2019t. DuckDB will run the unde rlying statements of a view if you query\nthat view. If you find your self running into performance bugs, you might want to\nmaterialize the data of a view in a tempor ary table using a CTAS statement. Any addi-\ntional predicates that  you use when querying a view inside the WHERE  clause are often-\ntimes used as pushdown predicates . That means they will be added to the underlying\nquery defining the view and will not be used  as filters after the data has been loaded. \n A view that is helpful in our scenario is one that gives us the amount of energy pro-\nduced per system and per day in kWh. This view will encapsulate the logic to compute\nthat value with the necessary grouping statements for us. Views are a great way to cre-\nate an API inside your database. That AP I can serve ad hoc queries and applications\nalike. When the underlying computation chan ges, the view can be recreated with the\nsame structure with out affecting any outside application.\n The GROUP BY  clause is one of those clauses you hardly can go without in the rela-\ntional world; we will explore exactly why it\u2019s  so important later in the chapter. For this\nexample, it is enough to understand that the GROUP  BY clause computes the total\npower produced by system and day. The sum function used in the select list is a\nso-called aggregate functi on, aggregating the values belonging to a group. \nCREATE OR REPLACE VIEW v_power_per_day AS\nSELECT system_id,\ndate_trunc('day', read_on) AS day,round(sum(power) /4/1 0 0 0 ,2 ) A S kWh,\nFROM readings\nGROUP BY system_id, day;Listing 3.4 Creating a view for power production by system and day30 CHAPTER  3Executing SQL queries\nIt does not matter whether the underlying ta bles are empty for a view to be created, as\nlong as they exist. While we did create the readings  table, we haven\u2019t inserted any\ndata yet, so querying the view with SELECT  * FROM  v_power_per_day  will return an\nempty result for now. We will return to this view in section 3.4.1 and use it in several\nsubsequent examples in this chapter and chapter 4. \n3.3.4 The DESCRIBE statement\nPerhaps universally, relati onal databases support the DESCRIBE  statement to query the\ndatabase schema. In its most basic implemen tation, it usually works with tables and\nviews. \nTIP Relational databases are based on the relational mo del and eventually\nrelational algebra. The relational model was first described by Edgar F. Coddin 1970. In essence, all data  is stored as sets of tupl es grouped together in rela-\ntions. A tuple is an ordered list of attributes\u2014thi nk of it as the column list of a\ntable. A table, then, is the relation of a set of tuples. A view is also a relation of\ntuples, and so is the result of a query.  Graph databases, in contrast to rela-\ntional databases, store ac tual relations between enti ties. In this book, how-\never, we use the term as defined in the relational model. \nThe \nDESCRIBE  statement in DuckDB works not only  with tables but also with every-\nthing else being a relation: views, queries,  sets, and more. You might want to describe\nthe readings  table with DESCRIBE  readings; . Your result should be similar to the\nfollowing:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 int32 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 system_id \u2502 INTEGER \u2502 NO \u2502 PRI \u2502 \u2502 \u2502\n\u2502 read_on \u2502 TIMESTAMP \u2502 NO \u2502 PRI \u2502 \u2502 \u2502\n\u2502 power \u2502 DECIMAL(8,3) \u2502 NO \u2502 \u2502 0 \u2502 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nDescribing a specific subset of columns (a new tuple) selected from any table, such as\nDESCRIBE SELECT read_on, power FROM readings;\nyields the following:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 read_on \u2502 TIMESTAMP \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 power \u2502 DECIMAL(8,3) \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nLast but not least, describing any constructed tuple such as DESCRIBE VALUES (4711,\n'2023-05-28 11:00'::timestamp, 42);  works the same:31 3.4 Data manipulation language queries\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 col0 \u2502 INTEGER \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 col1 \u2502 TIMESTAMP \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 col2 \u2502 INTEGER \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTIP Use the DESCRIBE  statement in all scenarios in which you are unsure\nabout the shape of the data. It works for all kinds of relations, local and\nremote files. The type of file being used affects how efficiently DuckDB canoptimize the \nDESCRIBE  statement. For example, remote files (e.g., files in Par-\nquet format) can even be described ve ry quickly, while files in CSV format\noften take longer to describe, as they don\u2019t carry a schema with them, and the\nengine needs to sample their content. \n3.4 Data manipulation language queries\nIn the context of databases, all statements that insert, delete, modify, and read data are\nreferred to as data manipulation language  (DML). This section will first cover the INSERT\nand DELETE  statements before going into querying  data. We won\u2019t go into much detail\non the UPDATE  statement here. The beauty of SQL qu eries is that they compose very nat-\nurally, so everything you\u2019ll learn, for exam ple, about the WHERE  clause, also applies to\nthe clause being used in INSERT , DELETE , UPDATE , and SELECT  statements.\n3.4.1 The INSERT statement\nWhen creating data, the INSERT  statement is used. Inserting data is a task ranging\nfrom simple \u201cfire-and-forget\u201d statements to  complex statements mitigating conflicts\nand ensuring high-data quality. We start simple and naive by populating the price\ntable we created in listing 3.3. An INSERT  statement first specifies where you want to\ninsert and then what you want to insert. The where  is a table name\u2014here, it is the\nprices  table. The what can be a list of column values, but they must match the column\ntypes and order of the table. In our case, we \u2019re inserting one row with four values, two\nnumeric and two strings, with the la tter automatically being cast to a DATE :\nINSERT INTO prices\nVALUES (1, 11.59, '2018-12-01', '2019-01-01');\nThe preceding query is fragile in a couple of  ways. First, relying on the order of col-\numns will break your statement as soon as the target table changes. Also, we explicitly\nuse the 1 as a unique key. If you were to ex ecute the query a second time, it would\nrightfully fail, as the table contains alread y a row with the given key. The second row\nviolates the constraint that a primary key must be unique:\nD INSERT INTO prices\n> VALUES (1, 11.59, '2018-12-01', '2019-01-01');\nError: Constraint Error: Duplicate key \"id: 1\" violates primary key\n\u27a5constraint. If this is an unexpected constraint violation please32 CHAPTER  3Executing SQL queries\n\u27a5double check with the known index limitations section in our\n\u27a5documentation (https://duckdb.org/docs/sql/indexes).\nWhile the conflict could not have been prev ented given the schema, we can mitigate it\nby using the nonstandard ON CONFLICT  clause and just do nothing. The DO NOTHING\nclause targets the primary index by default (the id column, in this case). While still\nbeing fragile, this statement is at least now idempotent:\nINSERT INTO prices\nVALUES (1, 11.59, '2018-12-01', '2019-01-01')\nON CONFLICT DO NOTHING;\nIn this case, idempotency might be less usef ul than you think: you don\u2019t get an error,\nbut you most likely won\u2019t get the expected re sult, either. In our ex ample, a better solu-\ntion would be to specify all columns we want to insert and avoid using an explicit value\nfor the ID. Overall, we already defined a sequence and a default value for the column\nthat generates IDs for us:\nINSERT INTO prices(value, valid_from, valid_until)\nVALUES (11.47, '2019-01-01', '2019-02-01'),\n(11.35, '2019-02-01', '2019-03-01'),\n(11.23, '2019-03-01', '2019-04-01'),\n(11.11, '2019-04-01', '2019-05-01'),(10.95, '2019-05-01', '2019-06-01');\nThere\u2019s another possible cause of failure: we defined a unique key for the validity\ndate. On that error, we can actually reac t in a way that makes sense from a business\nperspective. We can insert or replace the va lue when a conflict on that key arises. In\nthe following example, we use the new price to update the old one:\nINSERT INTO prices(value, valid_from, valid_until)\nVALUES (11.47, '2019-01-01', '2019-02-01')\nON CONFLICT (valid_from)\nDO UPDATE SET value = excluded.value;\nWe will revisit that topic in section 3.4.2.\n Of course, it is possible to use the outcome of a SELECT  statement as input for the\nINSERT  statement. We will have a look at the anatomy of a SELECT  statement shortly,\nbut to complete the example, please use it  as follows. Think of this statement as a\npipeline to the INSERT  clause. As shown in listing 3.5, it selects all the data from the\nfile named prices.csv and inserts them in or der of appearance (you can find that file\ninside the ch03 folder in this book\u2019s GitHub repository: https:/ /github.com/duckdb\n-in-action/examples ).\nINSERT INTO prices(value, valid_from, valid_until)\nSELECT * FROM 'prices.csv' src;Listing 3.5 Inserting data from other relationsAs the table has multiple constraints (primary \nand unique keys), we must specify on which \nkey the conflict miti gation shall happen.33 3.4 Data manipulation language queries\nLet\u2019s also fill the systems  table and load the first bunch of readings before we go over\nthe SELECT  statement in detail. To be able to write the INSERT  statement properly, we\nmust understand what the CSV data looks like. We will make use of the fact that we\ncan use DESCRIBE  with any relation\u2014in this case, a re lation that is defined by reading\nthe CSV file:\nINSTALL 'httpfs';\nLOAD 'httpfs';\nDESCRIBE SELECT * FROM\n'https://oedi-data-lake.s3.amazonaws.com/pvdaq/csv/systems.csv';\nWithout specifying any type hints, sy stems.csv looks like this for DuckDB:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 system_id \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 system_public_name \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 site_id \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 site_public_name \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 site_location \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 site_latitude \u2502 DOUBLE \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 site_longitude \u2502 DOUBLE \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 site_elevation \u2502 DOUBLE \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nUsing the system_id  and system_public_name  will do nicely for us. However, it turns\nout that there are duplicates in the file, whic h will cause our insertion to fail. The eas-\niest way to filter out duplicates is by applying the DISTINCT  keyword in the columns\nclause of the SELECT  statement, as shown in the fo llowing listing. This ensures a\nunique set over all the columns we select.\nINSTALL 'httpfs';\nLOAD 'httpfs';\nINSERT INTO systems(id, name)\nSELECT DISTINCT system_id, system_public_name\nFROM 'https://oedi-data-lake.s3.amazonaws.com/pvdaq/csv/systems.csv'\nORDER BY system_id ASC;\nThe systems in section 3.2.1 have been select ed for specific reasons. We start with the\ndataset for system 34, as it suits our requ irements to begin with (having readings in\n15-minute intervals). It does have some in consistencies to deal with: the power output\nis sometimes NULL  (not present) or negative. We will use a CASE  expression to default\nmissing values to 0.\n The URL does not clearly identify which type  of file or structur e is behind it for\nDuckDB (e.g., by using a familiar extension su ch as .csv or .parquet). To address this,Listing 3.6 Inserting a distinct set of rows from another tableInstalls the httpfs extension and loads \nit so that we can access the URL34 CHAPTER  3Executing SQL queries\nwe must use the read_csv_auto  function (see the following listing), as the database\nwon\u2019t be able to infer the correct file type.\nINSERT INTO readings(system_id, read_on, power)\nSELECT SiteId, \"Date-Time\",\nCASE\nWHEN ac_powe r<0O Ra c_power IS NULL THEN 0\nELSE ac_power END\nFROM read_csv_auto(\n'https://developer.nrel.gov/api/pvdaq/v3/data_file?' ||'api_key=DEMO_KEY&system_id=34&year=2019'\n);\nA sample of the data for system 34 in 20 19 we just ingested can be achieved with\nSELECT * FROM readings WHERE date_trunc('day', read_on) = '2019-08-\n26' AND power <> 0;\nThe sample looks as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 system_id \u2502 read_on \u2502 power \u2502\u2502 int32 \u2502 timestamp \u2502 decimal(10,3) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 34 \u2502 2019-08-26 05:30:00 \u2502 1700.000 \u2502\u2502 34 \u2502 2019-08-26 05:45:00 \u2502 3900.000 \u2502\n\u2502 34 \u2502 2019-08-26 06:00:00 \u2502 8300.000 \u2502\n\u2502 \u00b7 \u2502\u00b7\u2502\u00b7 \u2502\u2502 \u00b7 \u2502\u00b7\u2502\u00b7 \u2502\n\u2502 \u00b7 \u2502\u00b7\u2502\u00b7 \u2502\n\u2502 34 \u2502 2019-08-26 17:30:00 \u2502 5200.000 \u2502\u2502 34 \u2502 2019-08-26 17:45:00 \u2502 2200.000 \u2502\n\u2502 34 \u2502 2019-08-26 18:00:00 \u2502 600.000 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 51 rows (6 shown) 3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNow that we\u2019ve finally ingested some da ta into the readings table, the view v_power_\nper_day  created in listing 3.4 al so returns data. Remember, v_power_per_day  creates\ndaily groups and sums up their power values, as shown with the output of\n`SELECT * FROM v_power_per_day WHERE day = '2019-08-26'`\nThe output is as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 day \u2502 kWh \u2502\n\u2502 int32 \u2502 date \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 34 \u2502 2019-08-26 \u2502 716.9 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Listing 3.7 Downloading and ingesting the first set of readings35 3.4 Data manipulation language queries\nIf you don\u2019t remember the definition of the vi ew, be sure to check it again. A view is a\ngreat way to encapsulate logic, such as by truncating the date to a day and aggregating\nthe total value of readings on that day, such as in our example.\n The query is essentially the same for 2020, apart from the URL parameter. Why\ndon\u2019t we generate a list of filenames using the range  function that acts as an inline\ntable like this?\nSELECT *\nFROM (\nSELECT 'https://' || years.range || '.csv' AS vFROM range(2019,2021) years\n) urls, read_csv_auto(urls.v);\nWhile this query is theoretically correct, it does not (yet) work due to restrictions in\nhow so-called table functions (see section 4.9) are implemented in DuckDB. At the\ntime of writing, they only accept constant parameters. Furthermore, read_csv  or\nread_parquet  learn about their schema by lookin g at the input parameters and read-\ning the given files, so there\u2019s a ch icken-and-egg problem to be solved.\n3.4.2 Merging data\nOftentimes, you find yourself with a datase t that contains duplicates or entries that\nalready exist within your database. While yo u can certainly ignore conflicts, as shown\nin section 3.4.1, when your only task is to refine and clean new data, you sometimes\nwant to merge new data into existing da ta. For this purpose, DuckDB offers the ON\nCONFLICT  DO UPDATE  clause, known as MERGE  INTO  in some other databases. In our\nexample, we might have multiple readings from different meters for the same system\nand want to compute the average reading. Instead of doing nothing on conflict, we\nuse a DO UPDATE  now.\n In listing 3.8, a random reading is insert ed first, and then an attempt is made to\ninsert a reading on the same time for the same device. The seco nd attempt will cause\na conflict, not on a primary key,  but on the composed key of system_id  and read_on .\nWith the DO UPDATE  clause, we specify the action to take when a conflict arises. The\nupdate clause can update as many column s as necessary, essentially doing a merge/\nupsert; complex expressions, such as a CASE  statement, ar e allowed too.\nINSERT INTO readings(system_id, read_on, power)\nVALUES (10, '2023-06-05 13:00:00', 4000);\nINSERT INTO readings(system_id, read_on, power)\nVALUES (10, '2023-06-05 13:00:00', 3000)\nON CONFLICT(system_id, read_on) DO UPDATESET power = CASE\nWHEN powe r = 0 THEN excluded.power\nELSE (power + excluded.power) / 2 END;Listing 3.8 Computing new values on conflict\nHere, the action \nis specified.\nColumns from the or iginal dataset can \nbe referred to by the alias .excluded.36 CHAPTER  3Executing SQL queries\nNOTE DuckDB also offers INSERT  OR REPLACE  and INSERT  OR IGNORE  as short-\nhand alternatives for ON CONFLICT  DO UPDATE  and ON CONFLICT  DO NOTHING ,\nrespectively. INSERT  OR REPLACE , however, does not have the ability to com-\nbine existing values, as in the preced ing example, nor does it allow you to\ndefine the conflict target.\n3.4.3 The DELETE statement\nThere are some outliers in the data sources we\u2019re using. We imported a bunch of read-\nings that are measured on different minute s of the hour, and we don\u2019t want them in\nour dataset. The easiest way to deal with them is to apply the DELETE  statement and get\nrid of them. The following DELETE  statement filters the rows to be deleted through a\ncondition based on a negated IN operator. That operator checks the containment of\nthe left expression inside the set of  expressions on the right-hand side. date_part  is\njust one of the many built-in functions of  DuckDB dealing with dates and timestamps.\nThis one (see the following listing) extracts a part from a timestamp\u2014in this case, the\nminutes from the read_on  column.\nDELETE FROM readings\nWHERE date_part('minute', read_on) NOT IN (0,15,30,45);\nSometimes, you will know a bout quirks and inconsistencies like these upfront, and\nyou won\u2019t have to deal with them after you ingest the data. With time-based data, as inour example, you could have written the ingesting statement utilizing the \ntime_\nbucket  function. We noticed that inconsiste ncy only after importing and think it\u2019s\nworthwhile to point this out.\n3.4.4 The SELECT statement\nThis section focuses on the SELECT  statement and querying the ingested data. This\nstatement retrieves data as rows from the data base or, if used in a nested fashion, cre-\nates ephemeral relations. Those relations can be queried again or used to insert data,\nas we have already seen.\n The essential clauses of a SELECT  statement and their canonical order are shown in\nthe following listing.\nSELECT select_list\nFROM tablesWHERE condition\nGROUP BY groups\nHAVING group_filterORDER BY order_expr\nLIMIT nListing 3.9 Cleaning the ingested data\nListing 3.10 The structure of a SELECT  statement37 3.4 Data manipulation language queries\nThere are more clauses, in both the stan dard and the DuckDB-specific SQL dialect,\nand we will discuss a couple of them in the next chapter as well. The official DuckDB\ndocumentation has a dedicated page to the SELECT  statement ( https://duckdb.org/\ndocs/sql/statements/select ), which we recommend using as a reference on how each\nclause of the SELECT  statement is supposed to be constructed.\n We think the following clauses are the most important to understand:\n\uf0a1FROM  in conjunction with JOIN\n\uf0a1WHERE\n\uf0a1GROUP BY\nThey define the sources of your queries, fi lter both reading queries as well as writing\nqueries, and, eventually, resh ape them. They are used in many contexts in addition to\nquerying data. Many other clauses ar e easier to understand, such as ORDER , which\u2014as\nyou\u2019d expect\u2014puts things in order.\nTHE SELECT AND FROM CLAUSES\nEvery standard SQL statement that  reads data starts with the SELECT  clause. The SELECT\nclause defines the columns or expressions that will eventually be returned as rows. If\nyou want to get everything from the source tables of your statement, you can use the *.\nNOTE Sometimes, the SELECT  clause is called a projection, choosing which\ncolumns to be returned. Ironically, the selection of rows happens in the\nWHERE  clause.\nThe SELECT  and FROM  clauses complement one another, and we could pick either to\nexplain first, or we could explain them together: the FROM  clause specif ies the source\nof the data on which the remainder of th e query should operate, and for most que-\nries, that will be one or more tables. If  there is more than one table list in the FROM\nclause or the additional JOIN  clause is used, we speak about joining tables together.\n The following statement will return two rows from the prices  table. The LIMIT\nclause we are introducing here limits the nu mber of returned rows . It\u2019s often wise to\nlimit the amount of data you get back in case you don\u2019t know the underlying dataset\nso that you don\u2019t cause a large amount of ne twork traffic or end up  with an unrespon-\nsive client:\nSELECT *\nFROM prices\nLIMIT 2;\nIt will return the first two rows. Without an ORDER  clause, the order is actually unde-\nfined and might differ in your instance:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 value \u2502 valid_from \u2502 valid_until \u2502\u2502 int32 \u2502 decimal(5,2) \u2502 date \u2502 date \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 \u2502 11.59 \u2502 2018-12-01 \u2502 2019-01-01 \u2502\u2502 10 \u2502 11.47 \u2502 2019-01-01 \u2502 2019-02-01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u251838 CHAPTER  3Executing SQL queries\nThe SQL dialect of DuckDB allows us to  cut the previous code down to just FROM\nprices; . (Without the limit, it will return all rows, but that\u2019s OK, since we know the\ncontent of that table from section 3.4.1.)\nTHE WHERE CLAUSE\nThe WHERE  clause allows you to filter your data by adding conditions to a query. Those\nconditions are built out of one or more  expressions. Data selected using a SELECT ,\nDELETE , or UPDATE  statement must match those predicat es to be included in the oper-\nations. This allows you to select only a subs et of the data in which you are interested.\nLogically, the WHERE  clause is applied immediately after the FROM  clause or the preced-\ning DELETE  or UPDATE  statement.\n In our example, we ca n replace that arbitrary LIMIT  with a proper condition that\nwill include only the prices for a specif ic year (2000) by adding the following WHERE\nclause. Take note that we are using a DuckDB  extension to SQL; in the case of a star-\nselect, you can omit the SELECT *  and just start with the FROM  clause:\nFROM prices\nWHERE valid_from BETWEEN\n'2020-01-01' AND '2020-12-31';\nBased on our example data, the query will return 11 rows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 value \u2502 valid_from \u2502 valid_until \u2502\n\u2502 int32 \u2502 decimal(5,2) \u2502 date \u2502 date \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 15 \u2502 8.60 \u2502 2020-11-01 \u2502 2023-01-01 \u2502\n\u2502 17 \u2502 8.64 \u2502 2020-10-01 \u2502 2020-11-01 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\n\u2502 25 \u2502 9.72 \u2502 2020-02-01 \u2502 2020-03-01 \u2502\u2502 26 \u2502 9.87 \u2502 2020-01-01 \u2502 2020-02-01 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 11 rows (4 shown) 4 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTHE GROUP BY CLAUSE\nGrouping by one or more columns generates one row of output per unique value of\nthose columns; it lets you group all rows that match those fields together. Then, the\ngrouped values get aggregated via an aggregation fu nction, such as count , sum, avg,\nmin, or max, so that one single value for that group is produced. This can be useful\nwhen you want to do things like compute the average number of readings per day or\nthe sum of the customers in each state. If the GROUP BY  clause is specified, the query is\nalways an aggregate query, even if no aggregations are present in the select list.\nDuckDB has a handy extension that lets you group your query by all columns that are\nnot part of an aggregate function: GROUP BY ALL . Figure 3.2 demonstrates how a selec-\ntion of rows is grouped by the column year  as well as the results of applying the aggre-\ngates count , avg, min, and max to it.The BETWEEN keyword is shorthand \nfor x <= v AND v <= y.39 3.4 Data manipulation language queries\nFigure 3.2 Grouping a dataset by year\nThere are many aggregate func tions to choose from. In addition to those previously\ncovered, which are relatively standard, here  are some that we think are often helpful:\n\uf0a1list \u2014Aggregates all values of each  group into a list structure\n\uf0a1any_value \u2014Picks any value from a nongrouping column\n\uf0a1first  or last \u2014Picks the first or the last va lue from a nongrouping column if\nthe result is ordered\n\uf0a1arg_max  and arg_min \u2014Solves the common task of finding the value of an\nexpression in the row having  a maximum or minimum valueid year value\n10 2019 11.47\n11 2019 11.3512 2019 11.2313 2019 11.11\n14 2019 10.95\n27 2019 9.9728 2019 10.0829 2019 10.18\n30 2019 10.3331 2019 10.48\n32 2019 10.64\n33 2019 10.79\n15 2020 8.60\n17 2020 8.64\n18 2020 8.77\n19 2020 8.90\n20 2020 9.03\n21 2020 9.17\n22 2020 9.30\n23 2020 9.44\n24 2020 9.58\n25 2020 9.7226 2020 9.87Group 1 (2019)\nGroup 2 (2020)year minimum_ price maximum_ price\n2019 9.97 11.47\n2020 8.60 9.87\nThe minimum \nprice in group 2The minimum \nprice in group 1min() and max() applied per group\nyearnum_ prices avg_price\n2019 12 10.715\n2020 11 9.183636363636364count() and avg() applied per group40 CHAPTER  3Executing SQL queries\n\uf0a1bit_and , bit_or , bit_xor , and others \u2014Bit operations that work on sets\n\uf0a1median , quantile computation, computing covariance, and general regressions \u2014An\nexhaustive set of statistical aggregates\nThe full list is availabl e on the DuckDB website: https:/ /duckdb .org/docs/sql/\naggregates . With that knowledge, let\u2019s see what we can do with our dataset.\n Let\u2019s pick up the prices example we started to use in section 3.4.4. First, we added\nthe WHERE  clause to find prices in a year. Whil e that was interesting, how about finding\nout the minimum and maximum prices per year?\n We will use the min and max aggregates grouped by th e year in which the prices\nhave been valid to find the highest and lowe st prices in the years from 2019 to 2020.\nThe valid_from  column is a date; we are only  interested in the year. The date_part\nfunction can extract that. If used without an  alias, the resulting column will be named\ndate_part('year',  valid_from) . This does not read nicely, and it is also cumber-\nsome to refer to. Therefore, the AS keyword is used to introduce the alias year .\nDuckDB allows us to refer to such an alias in the GROUP BY  clause, which is different\nfrom the SQL standard and is very helpful. The year  becomes the grouping key by\nspecifying it in the GROUP  BY clause, and its distinct values  will define the buckets for\nwhich the minimum and maximum values of the column we chose should be\ncomputed.\nSELECT date_part('year', valid_from) AS year,\nmin(value) AS minimum_price,\nmax(value) AS maximum_price\nFROM prices\nWHERE year BETWEEN 2019 AND 2020\nGROUP BY yearORDER BY year;\nThe result of this query is as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 year \u2502 minimum_price \u2502 maximum_price \u2502\n\u2502 int64 \u2502 decimal(5,2) \u2502 decimal(5,2) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 2019 \u2502 9.97 \u2502 11.47 \u2502\n\u2502 2020 \u2502 8.60 \u2502 9.87 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTIP DuckDB offers choices when dealing with date parts. You can use the\ngeneric date_part  function like we did and specify the part as a parameter.\nThere are identifiers for all relevant parts, such as 'day' , 'hour' ,'minute' ,\nand many others. All of them exist also as dedicated functions, so in listing3.11, we could have used \nyear(valid_from)  too. The generic function is\nhelpful when the part is derived from other expressions in the statement orListing 3.11 Grouped aggregates\nYou can have as many aggregate functions \nin the SELECT clause as you want.\nNote how we can reuse the alias we gave in \nthe SELECT clause in the GROUP BY clause.41 3.4 Data manipulation language queries\nwhen you try to write po rtable SQL. The dedicated functions are easier to\nread.\nTHE VALUES CLAUSE\nThe VALUES  clause is used to specify a fixed number of rows. We have seen it already\nwhile inserting data, which is a quite common use case. It is, however, much more ver-\nsatile in DuckDB than in so me other databases, as it ca n be used both as a standalone\nstatement and as part of the FROM clause , with any number of rows and columns.\nThere are a couple of scenarios in which th is is handy: for example, providing seed\ndata for conditions.\n Here\u2019s how to define a single row wi th two columns\u2014for example, a simple VALUES\n(1,2); :\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0 \u2502 col1 \u2502\u2502 int32 \u2502 int32 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021 \u25022 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTake note that multiple rows can be generated by simply enumerating multiple tuples:\nVALUES (1,2), (3,4); . You don\u2019t need to wrap them  in additional parentheses:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0 \u2502 col1 \u2502\n\u2502 int32 \u2502 int32 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25021 \u25022 \u2502\n\u25023 \u25024 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIf you do, however, as in VALUES ((1,2),  (3,4)); , you will create a single row with\ntwo columns, each containing a structured type:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col0 \u2502 col1 \u2502\n\u2502 struct(v1 integer, v2 integer) \u2502 struct(v1 integer, v2 integer) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 {'v1': 1, 'v2': 2} \u2502 {'v1': 3, 'v2': 4} \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWhen used in a FROM  clause, the resulting types can be named, together with their col-\numns. We will make use of that in the next  section while discussi ng joining logic. The\nfollowing snippet defines two rows with three columns within the VALUES  clause and\ncreates an inline named table that holds the column names. The name of that table is\narbitrary; we just picked t:\nSELECT *\nFROM (VALUES\n(1, 'Row 1', now()),42 CHAPTER  3Executing SQL queries\n(2, 'Row 2', now())\n) t(id, name, arbitrary_column_name);\nThe resulting virtual table looks like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 id \u2502 name \u2502 arbitrary_column_name \u2502\n\u2502 int32 \u2502 varchar \u2502 timestamp with time zone \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 \u2502 Row 1 \u2502 2023-06-02 13:44:30.309+02 \u2502\n\u2502 2 \u2502 Row 2 \u2502 2023-06-02 13:44:30.309+02 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTHE JOIN CLAUSE\nWhile you can get away with not using a JOIN  clause when analyzing single Parquet or\nCSV files, you should no t skip this section: joins are a fundamental relational opera-\ntion used to connect two tables or relati ons. The relations are referred to as the left\nand right sides  of the join, with the left side of the join being the table listed first. This\nconnection represents a new relation co mbining previously unconnected informa-\ntion, thus providing new insights.\n In essence, a join creates matching pair s of rows from both sides of the join. The\nmatching is usually based on a key column in  the left table being equal to a column in\nthe right table. Foreign key constraints are not required for join ing tables together.\nWe prefer the SQL standard definition of joins based on the JOIN .. USING  over JOIN\n.. ON  clauses, as you\u2019ll see in the following examples and throughout the rest of the\nbook. Nevertheless, joins can be expressed by simply enumerating the tables in the\nFROM  clause and comparing the key columns in the WHERE  clause.\nNOTE We are not using Venn diagrams for explaining joins because join\noperations are not pure set operatio ns, for which Venn diagrams would be a\ngreat choice. SQL does know  set operations, such as UNION , INTERSECT , and\nEXCEPT \u2014and DuckDB supports all of them. Joins, on the other hand, are all\nbased on a Cartesian product in relation al algebra, or in simple terms, they\nare all based on joining everything with  everything else and then filtering\nthings out. Essentially, all different joins can be derived from the CROSS JOIN .\nThe inner join then filters on some co ndition, and a left or right outer join\nadds a union to it, but that\u2019s all ther e is to set-based operations in joins.\nIn the following examples, we will use the VALUES  clause to define virtual tables with a\nfixed number of rows with a given set of va lues. These sets are helpful to understand-\ning the joining logic, as you will see both  the sources and the resulting rows in the\nexample. Usually, you will find yourself join ing different tables together, such as the\npower readings and the prices in our example.\n The simplest way of joining is via an INNER JOIN  (figure 3.3), which also happens\nto be the default. An inner join matches a ll rows from the left-hand side to rows from\nthe right-hand side that have a column with the same value.43 3.4 Data manipulation language queries\nFigure 3.3 The inner join only matching pairs with equal keys\nIf both relations have a colu mn with the same name, the USING  clause can be used to\nspecify that. The USING  clause will look up the specif ied columns in both relations and\nwork the same way as specif ying them yourself via the ON clause ( ON tab1.col  =\ntab2.col ).\nSELECT *\nFROM\n(VALUES (1, 'a1'),\n(2, 'a2'),\n(3, 'a3')) l(id, nameA)\nJOIN\n(VALUES (1, 'b1'),\n(2, 'b2'),(4, 'b4')) r(id, nameB)\nUSING (id);\nThe result will look like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 nameA \u2502 nameB \u2502\n\u2502 int32 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25021 \u2502 a 1 \u2502 b 1 \u2502\n\u25022 \u2502 a 2 \u2502 b 2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAn outer join, on the other hand, supplements NULL  values for rows on the specified\nside of the relation that have no matching  entry on the other. Think of several power-\nproducing systems in your database: for so me, you might have stored additional ven-\ndor information in another table, and for some, you don\u2019t. You would use an outer\njoin when tasked to give a list of all syst ems with the optional vendor or an empty col-\numn if there\u2019s no such vendor. The following listing uses a LEFT OUTER JOIN  so that all\nrows of the left relations are included and supplemented with NULL  values for rows\nthat don\u2019t have a match.Listing 3.12 Using an inner join1, a1\n2, a21, b1\n2, b2\n3, a3 4, b4INNER JOIN =a1 b1\na2 b21\n2\nThis is equivalent \nto ON r1.id = r2.id.44 CHAPTER  3Executing SQL queries\n \nSELECT *\nFROM\n(VALUES (1, 'a1'),\n(2, 'a2'),\n(3, 'a3')) l(id, nameA)\nLEFT OUTER JOIN\n(VALUES (1, 'b1'),\n(2, 'b2'),\n(4, 'b4')) r(id, nameB)\nUSING (id)\nORDER BY id;\nJoining the virtual tables from listing 3.13 with a LEFT  OUTER  JOIN  results in the following:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 nameA \u2502 nameB \u2502\u2502 int32 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021 \u2502 a 1 \u2502 b 1 \u2502\u25022 \u2502 a 2 \u2502 b 2 \u2502\n\u25023 \u2502 a 3 \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAll rows from the left-hand side have been included, and for a3, a NULL  value has been\njoined. Try changing the outer join from LEFT  to RIGHT , and observe which values are\nnow included. Both the LEFT  and RIGHT  outer join will return three rows in total. To\nget back four rows, you must use a full oute r join, as shown in the following listing.\nSELECT *\nFROM\n(VALUES (1, 'a1'),\n(2, 'a2'),(3, 'a3')) l(id, nameA)\nFULL OUTER JOIN\n(VALUES (1, 'b1'),\n(2, 'b2'),\n(4, 'b4')) r(id, nameB)\nUSING (id)ORDER BY id;\nFour rows will be returned, with two NULL  values, one for nameA  and one for nameB :\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 id \u2502 nameA \u2502 nameB \u2502\n\u2502 int32 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25021 \u2502 a 1 \u2502 b 1 \u2502\n\u25022 \u2502 a 2 \u2502 b 2 \u2502\n\u25023 \u2502 a 3 \u2502 \u2502\u25024 \u2502 \u2502 b 4 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Listing 3.13 Using a left outer join\nListing 3.14 Using a full outer join45 3.4 Data manipulation language queries\nFigure 3.4 depicts the left outer join; the fu ll outer join, we had in code previously,\nand the right outer join for comparison. While an outer join always gives you the rows\nan inner join would give, it would be wron g to suggest always using outer joins. An\ninner join will filter out rows that have no  matching data in the other table, which is\noften a requirement. An outer join will usually be appropriate when you want to\nenrich required data with optional data.\nThe preceding example applies the USING  clause for the join conditions, as both\ntables have an id column. In our example, we defined an id column in the systems\ntable and the foreign key column in readings  as system_id . We, therefore, must use\nthe ON clause. When joined on that column, the join will always produce a matching\nrow, as the join column ( id) is the column referenced by the foreign key we defined\non system_id . That means there can\u2019t be any row in the readings  table without a\nmatching entry in the systems  table:\nSELECT name, count(*) as number_of_readings\nFROM readings JOIN systems ON id = system_id\nGROUP BY name;\nNOTE A Cartesian product  is a mathematical term de scribing the list of all\nordered pairs that you can produce from  two sets of elements by combining\neach element from the first set with ea ch element of the second set. The size\nof a Cartesian product is equal to the product of the sizes of each set.\nThere are more join types, such as the CROSS JOIN , which creates a Cartesian product\nof all tuples, and the ASOF  (as of ), which will come in handy when dealing with the\nprices with a restricted validity. For example, the ASOF  join allows you to match rows\nfrom one table with rows from another tabl e based on temporal validity (or, as a321 1, a1\n2, a21, b1\n2, b2\n3, a3\n4, b4LEFT OUTER\n JOIN a1 b1\na2 b2\na3\n1, a1\n2, a21, b1\n2, b2FULL OUTER\n JOIN a1 b1\na2 b2\n3, a3\n4, b4a3\nb41\n2\n3\n41, a1\n2, a21, b1\n2, b2 RIGHT OUTER\n JOIN a1 b1\na2 b2\n3, a3\n4, b4b41\n2\n4\nFigure 3.4 Types of outer joins46 CHAPTER  3Executing SQL queries\nmatter of fact, with anything th at has an inequality condition\u2014 <=, <, > or >=). You will\nread about the ASOF  join in detail in section 4.8.\nWe\u2019d like to end this section with somethin g akin to a warning. In our examples, for\ninner and outer joins, we only discussed wh at happens when a value of a key column is\nnot found in one of the other tables. But wh at happens when one of the join columns\ncontains the same value multip le times, either in one of the join tables or in both?\nLet\u2019s find out in the next listing. The value 2 for the id column appears twice in the\nleft table, and the value 3 appe ars twice in the right table.\nSELECT *\nFROM\n(VALUES (1, 'a1'),\n(2, 'a2'),\n(2, 'a2'),\n(3, 'a3')) l(id, nameA)\nJOIN\n(VALUES (1, 'b1'),\n(2, 'b2'),(3, 'b3'),\n(3, 'b3')) r(id, nameB)\nUSING (id)ORDER BY id;\nThe result of this statement won\u2019t be four rows, as before, but six:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 nameA \u2502 nameB \u2502\n\u2502 int32 \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021 \u2502 a 1 \u2502 b 1 \u2502\n\u25022 \u2502 a 2 \u2502 b 2 \u2502\u25022 \u2502 a 2 \u2502 b 2 \u2502\n\u25023 \u2502 a 3 \u2502 b 3 \u2502\n\u25023 \u2502 a 3 \u2502 b 3 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The COPY TO command\nYou are building data pipelines around CSV files and often have data split across sev-\neral files with one common column per file. What if you wanted to reduce these filesto exactly one file without duplicating th e common column? That\u2019s easy to achieve\nwith an inner join and the \nCOPY TO  command. The latter takes any relation and copies\nit to a file using the specified format:\nduckdb -c \"COPY (SELECT * FROM 'production.csv' JOIN 'consumption.csv'\nUSING (ts) JOIN 'export.csv' USING (ts) JOIN 'import.csv' USING (ts) )\nTO '/dev/stdout' (HEADER)\"\nThis command will join four CSV  files on a shared column ts, keep only one copy of\nthe shared column in the SELECT *  statement, and copy the result to standard out.\nListing 3.15 An inner join between tables with duplicate key columns47 3.4 Data manipulation language queries\nThis is something you can prepare for when defining your schema. Typically, joins will\nhappen on columns that are known up fron t. In our example, that would be the id\npair of the systems  table referred to as system_id  in the readings  table. In the\nsystems  table, that column is defined as the pr imary key, and as such, it will always be\na unique value; hence, it can only appear once in that table. On the readings  table, it\nis defined as the foreign key, meaning it must exist in the other one. The foreign key\nusually creates a so-called index in the database, which allows quick lookups, withoutgoing through all rows, making the join pe rform well. The foreign key is not unique\nand does not need to be in most models. In our example, the system appearing multi-\nple times in the \nreadings  table (the right-hand side) is expected, unless you want your\nsystem to produce power only once.\nTHE WITH CLAUSE\nThe WITH  clause is also known as a common table expression  (CTE). CTEs are essentially\nviews that are limited in scope to a particular  query. Like a view, you might want to use\nthem to encapsulate parts of the logic of yo ur query into a standalone statement, or at\nleast into an isolated part of a bigger query. While it would be perfectly fine to create a\nview, you might not want that because you wo uld only need its result in the specific\ncontext of the bigger query. In addition, CT Es have one special trait that views don\u2019t:\nviews can reference other views, but they  cannot be nested. A CTE can reference\nother CTEs defined in the same WITH  clause. With that, you ca n build your query logic\nin an incremental fashion.\n WITH  clauses prevent the anti-pattern of  having subqueries  defined in the FROM\nclause. A subquery as a source relation in a FROM  clause is syntactically and semanti-\ncally valid, as its result is a relation on its ow n, but it is often hard to read. In addition,\nnested subqueries are not allowed to reference themselves.\n Finding the row containing the maximum value of a specific column within that\nrow is often computed using a subquery in the FROM  clause like this:\nSELECT max_power.v, read_on\nFROM (\nSELECT max(power) AS v FROM readings\n) max_power\nJOIN readings ON power = max_power.v;\nAs shown in listing 3.16, the subquery is re latively simple, and rewriting it as a CTE\ndoesn\u2019t seem to make a big difference at first glance. We take the same query, move it\nout of the FROM  clause, and add a name within the WITH  clause. The JOIN  statement\nstays the same.\nWITH max_power AS (\nSELECT max(power) AS v FROM readings\n)\nSELECT max_power.v, read_on\nFROM max_powerJOIN readings ON power = max_power.v;Listing 3.16 Replacing a subquery with a CTE48 CHAPTER  3Executing SQL queries\nFor single and rather basic qu eries like this, it does not make much of a difference\nwhether we use a subquery or a CTE. But what  if we ask for something like the maxi-\nmum average production of power per syst em and hour? Aggregate functions, like\nmax and avg, cannot be nested (i.e., you cannot do avg(max(v)) ), so you need to use\nindividual aggregates.\n The question of which row contains th e minimum or maximum value of a column\nis such a common task that  DuckDB has two built-in functions to perform it: arg_max\nand arg_min . These functions compute an expression defined by their first parameter\non the columns in the row for which the minimum or maximum value of the second\nparameter occurs the first time. The followi ng query will produce one row from the\ndataset at which the highest amount of power was generated (not the five times that\nthe query in listing 3.16 will return). This is because arg_max  stops at the first value it\nfinds that matches the maximum value, while the join will include all rows:\nSELECT max(power), arg_max(read_on, power) AS read_on\nFROM readings;\nThe next query, shown in listing 3.17, makes use of the arg_max  aggregate. It first\nencapsulates the complex logic of grouping  the readings into average production by\nsystem and hour\u2014creating the first aggregate\u2014in a CTE that we name per_hour , and\nthen it takes that CTE and comput es a second aggregate over it.\nWITH per_hour AS (\nSELECT system_id,\ndate_trunc('hour', read_on) AS read_on,\navg(power) / 1000 AS kWh\nFROM readingsGROUP BY ALL\n)\nSELECT name,\nmax(kWh),\narg_max(read_on, kWh) AS 'Read on'\nFROM per_hour\nJOIN systems s ON s.id = per_hour.system_id\nWHERE system_id = 34\nGROUP by s.name;\nThe result shows the that the Andre Agassi  Preparatory Academy has the system with\nthe highest production in our dataset:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name \u2502 max(kWh) \u2502 Read on \u2502\n\u2502 varchar \u2502 double \u2502 timestamp \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 [34] Andre Agassi Preparatory Academy \u2502 123.75 \u2502 2020-04-09 11:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Listing 3.17 Creating multiple groups\nUsing a proper \nname for the CTE\nThe average value per hour and day is the \nfirst aggregate we need; GROUP BY ALL is \na DuckDB extension cr eating a group from \nall columns not part  of an aggregate.The nested aggregate \nwe look for\nUsing the CTE as the driving \ntable in the FROM clause49 3.4 Data manipulation language queries\nNOTE We looked this building up, and the readings and values add up.\nAccording to a fact sheet covering the installation, \u201cBetwe en April 2010 and\nJuly 2011, Bombard installed 2,249 Shar p 240-watt solar modules on the roofs\nof five buildings and three solar support structures at the Agassi Academy inLas Vegas\u201d ( https:/ /mng.bz/jXKp ).\nCTEs can do one more cool thin g views and subqueries cannot. The \nWITH  clause fea-\ntures the additional keyword RECURSIVE , which makes it possible to reference a CTE\nnot only from other succeeding CTEs and the FROM  clause but from within itself. Such\na recursive CTE essentially will follow this pa ttern (shown in listing 3.18). To make this\nwork, we need to have some ki nd of initial seed for the recursion. This is easy for a\ntree structure: we take the row that has no parent row and use this as one leaf of a\nUNION  clause.\nCREATE TABLE IF NOT EXISTS src (\nid INT PRIMARY KEY,parent_id INT, name VARCHAR(8)\n);\nINSERT INTO src (VALUES\n(1, null, 'root1'),(2, 1, 'ch1a'),(3, 1, 'ch2a'),(4, 3, 'ch3a'),(5, null, 'root2'),(6, 5, 'ch1b')\n);\nWITH RECURSIVE tree AS (\nSELECT id,\nid AS root_id,[name] AS path\nFROM src WHERE parent_id IS NULLUNION ALLSELECT src.id,\nroot_id,list_append(tree.path, src.name) AS path\nFROM src\nJOIN tree ON (src.parent_id = tree.id)\n)SELECT path FROM tree;\nThe results are several paths, all starting at the root, maki ng up their way to the corre-\nsponding leaves:\n  \n Listing 3.18 Selecting a graph-shaped structure with recursive SQL\nInitialize a new list \nwith a list literal.\nThis is the recursive \ninitial seed.\nRecursive join until there are \nno more entries from the src \ntable with the given parent id.50 CHAPTER  3Executing SQL queries\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 path \u2502\n\u2502 varchar[] \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [root1] \u2502\n\u2502 [root2] \u2502\u2502 [root1, ch1a] \u2502\n\u2502 [root1, ch2a] \u2502\n\u2502 [root2, ch1b] \u2502\u2502 [root1, ch2a, ch3a] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe example aggregates names into a path from  the root of a tree to the leaf by using\nlist_append . You could use list_prepend  and inverse the parameter to build up\npaths from the leaves to the root nodes.\n As an exercise, you can try to compute th e longest path in the tree. The recursive\nCTE will stay the same, but you will want to apply the arg_max  function you learned\nabout in the SELECT  statement together with the length  aggregate on a list.\n3.5 DuckDB-specific SQL extensions\nOne of the goals of the authors of DuckDB is to make SQL more  accessible and user-\nfriendly. One way they\u2019ve done this is by adding additions to their implementation of\nS Q L  t h a t  m a k e  i t  e a s y  t o  do common tasks. In this se ction, we\u2019ll introduce those\nadditions. \n3.5.1 Dealing with SELECT \nSELECT *  is a two-edged sword: it is easy to write down, and the resulting tuples most\nlikely will contain what you actually need. Some problems associated with selecting all\ncolumns of a relation include the following:\n\uf0a1Instability of the resulting tuples, as a table definition might change (adding or\nremoving columns).\n\uf0a1Putting more memory pressure on the database server or process.\n\uf0a1While DuckDB is an embedded databa se and won\u2019t involve network traffic,\nselect-stars will cause more tr affic on nonembedded databases.\n\uf0a1A select-star might prevent an index-only scan. An index-only scan will occur\nw h e n  y o u r  q u e r y  c a n  u s e  a n  i n d e x ,  a n d  y o u  o n l y  r e t u r n  c o l u m n s  f r o m  t h a tindex so that any other IO can be avoided. An index-only scan is a desiredbehaviour in most cases.\nWhile it\u2019s best to avoid doing too many \nSELECT *  queries, sometimes they are neces-\nsary, and DuckDB actually makes them safer to use with the addition of two keywords:\nEXCLUDE  and REPLACE . If you are certain you really wa nt all columns, DuckDB offers a\nsimplified version of the SELECT  statement, omitting the SELECT  clause altogether.\nInstead, it starts with the FROM  clause so that, for example, a FROM prices  is possible.\nWe will present another example in listing 3.22. 51 3.5 DuckDB-specific SQL extensions\nEXCLUDING  SOME COLUMNS  WITH EXCLUDE\nEXCLUDE  excludes one or more columns from a st ar query. This is helpful when you\nhave a table or relation with many columns,  of which nearly all are necessary, exclud-\ning the few that are irrelevant to your use case. Normally, you would have to enumer-ate all the columns you are interested in and exclude the ones you don\u2019t care about.\nFor example, you want only the relevant data from prices:\nSELECT value, valid_from, valid_until FROM prices;\nThis becomes tedious and error-prone very quickly, especially with more than a hand-\nful of columns. With the EXCLUDE  clause, you only have to enumerate the columns you\nare not interested in:\nSELECT * EXCLUDE (id)\nFROM prices;\nYou can exclude as many columns as you want . You will achieve most of the flexibility\nof a pure SELECT * , keep the readability of the star , and ensure you don\u2019t access some-\nthing you don\u2019t need.\nRESHAPING  RESULTS  WITH REPLACE\nThink about the view v_power_per_day . It computes the kWh in fractions. Some users\nmay only want to return the integer values . Instead of rewriting the whole view, you\ncan just replace the single column kWh with its rounded value while selecting the\nremaining columns:\nSELECT * REPLACE (round(kWh)::int AS kWh)\nFROM v_power_per_day;\nThe REPLACE  clause takes in one or more pairs of x AS y  constructs, with x being an\nexpression that can refer to columns of the original select list, applying functions and\nother transformations to them, and y being a name that has been used in the original\nselect list. \n The structure of the result is the same , but the kWh column is now an integer\ncolumn:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 day \u2502 kWh \u2502\n\u2502 int32 \u2502 date \u2502 int32 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1200 \u2502 2019-08-29 \u2502 289 \u2502\n\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\n\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\n\u2502 10 \u2502 2020-03-19 \u2502 0 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1587 rows (2 shown) 3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u251852 CHAPTER  3Executing SQL queries\nDYNAMICALLY  PROJECTING  AND FILTERING  ON COLUMNS\nLet\u2019s recap the prices  table. It has two columns co ntaining information about the\nvalidity of a price:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 \u2026 \u2502 default \u2502 extra \u2502\u2502 varchar \u2502 varchar \u2502 varchar \u2502 \u2502 varchar \u2502 int32 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id \u2502 INTEGER \u2502 NO \u2502 \u2026 \u2502 nextval('prices_id') \u2502 \u2502\n\u2502 value \u2502 DECIMAL(5,2) \u2502 NO \u2502 \u2026 \u2502 \u2502 \u2502\n\u2502 valid_from \u2502 DATE \u2502 NO \u2502 \u2026 \u2502 \u2502 \u2502\n\u2502 valid_until \u2502 DATE \u2502 YES \u2502 \u2026 \u2502 \u2502 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4 rows 6 columns (5 shown) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe COLUMNS  expression can be used to project, filter, and aggregat e one or more col-\numns based on a regular expr ession. To select only colu mns that contain information\nabout validity, you can query the table like this:\nSELECT COLUMNS('valid.*') FROM prices LIMIT 3;\nThis returns all the relevant columns:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 valid_from \u2502 valid_until \u2502\n\u2502 date \u2502 date \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 2018-12-01 \u2502 2019-01-01 \u2502\n\u2502 2019-01-01 \u2502 2019-02-01 \u2502\n\u2502 2019-02-01 \u2502 2019-03-01 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nYou will benefit from using this technique if you have a table with lots of columns with\nsimilar names. Such a table could, for exam ple, take the form of a readings or mea-\nsurement table. More specifically, think of an IoT sensor that produces many different\nreadings per measurement. In that use case , there is another interesting feature: you\ncan apply any function over a dynamic select ion of columns that will produce as many\ncomputed columns.  Here, we compute several maximu m values at once for all col-\numns in the price  table that contain the word valid :\nSELECT max(COLUMNS('valid.*')) FROM prices;\nThis results in the maximum values for valid_from  and valid_until :\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 max(prices.valid_from) \u2502 max(prices.valid_until) \u2502\u2502 date \u2502 date \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023-01-01 \u2502 2024-02-01 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u251853 3.5 DuckDB-specific SQL extensions\nIf you find yourself writin g long conditions in the WHERE  clause, combining many\npredicates with AND, you can simplify that with the COLUMNS  expression as  well. To find\nall the prices that have been valid in 20 20 alone, you would want every row for which\nboth the valid_from  and valid_until  columns are between January 1, 2020 and\n2021, which is just what th e following query expresses:\nFROM prices WHERE COLUMNS('valid.*') BETWEEN '2020-01-01' AND '2021-01-01';\nYou might have noticed that  the regular expression .* sticks out a bit. Many people\nare more familiar with the % and _ wildcards used with the LIKE  operator. The % char-\nacter represents zero, one, or multiple characters, while the underscore sign rep-\nresents only one character. Luckily, COLUMNS  supports lambda functions. \nTIP A lambda function  is a self-contained block of functionality that can be\npassed around and used in your code. Lambda functions have differentnames in different programming langua ges, such as lambda expressions in\nJava, Kotlin, and Python; closures in Swift; and blocks in C. \nThe preceding query selecting a range of prices can also be written like this:\nFROM prices\nWHERE COLUMNS(col -> col LIKE 'valid%')BETWEEN '2020-01-01' AND '2021-01-01';\nLast but not least, you can combine the COLUMNS  expression with the REPLACE  or\nEXCLUDE  conditions too. Let\u2019s say you want to  compute the maximum value over all the\ncolumns in the prices  table, except the generated id value. You can get them like this:\nSELECT max(COLUMNS(* EXCLUDE id)) FROM prices;\n3.5.2 Inserting by name\nDo you remember listing 3.6? In that list ing, we used a statement in the form of\nINSERT INTO target(col1, col2) SELECT a, b FROM src  to populate our systems\ntable. This works but can be fragile to maintain, as the INSERT  statement requires\neither the selected columns to be in the sa me order as they are defined by the target\ntable or to repeat the column names\u2014once in the INTO  clause, once in the select list. \n DuckDB offers a BY NAME  clause to solve that probl em, and listing 3.19 can be\nrewritten as follows, keeping the mapping from the column names in the source to\nthe column names for the target together in one place. The BY NAME  keyword in the\nlisting indicates that the columns in the sele ct clause that follows shall be matched by\nname onto columns of the target table. \nINSERT INTO systems BY NAME\nSELECT DISTINCTListing 3.19 Insertion by nameThe expression inside the COLUMNS expression \nis a Lambda function ev aluating to true when \nthe column name is like the given text.54 CHAPTER  3Executing SQL queries\nsystem_id AS id,\nsystem_public_name AS NAME\nFROM 'https://oedi-data-lake.s3.amazonaws.com/pvdaq/csv/systems.csv'ON CONFLICT DO NOTHING;\nWhether you add new columns or remove co lumns from the insertion, you now only\nneed to change the query in one place. Any constraints, however, such as non-null col-\numns, must still be fulfilled. \n3.5.3 Accessing aliases everywhere\nYou probably haven\u2019t noticed it , but several of our examples benefit from something\nthat should be the standard\u2014but isn\u2019t. Th e moment you introduce an alias to a col-\numn, you can access them in succeeding clau ses. In listing 3.20 , we access the nonag-\ngregate alias is_not_system10 \u2014as defined in the select list\u2014in the WHERE  and GROUP\nBY clauses without repeating the column defi nition. The latter is not possible in many\nother relational databases. Th e same applies to the alias power_per_month  we gave to\nthe sum aggregate; we ca n access it in the HAVING  clause too. \nSELECT system_id > 10 AS is_not_system10,\ndate_trunc('month', read_on) AS month,\nsum(power) / 1000 / 1000 AS power_per_month\nFROM readings\nWHERE is_not_system10 = TRUE\nGROUP BY is_not_system10, monthHAVING power_per_month > 100;\n3.5.4 Grouping and ordering by all relevant columns\nAs discussed in the section covering the GROUP BY  clause, all nonaggregate columns\nneed to be enumerated in a GROUP BY  clause. If you have many nonaggregate col-\numns, this can be a painful experience an d one that DuckDB al leviates by allowing\nyou to use GROUP BY ALL . We can rewrite v_power_per_day  as shown in the following\nlisting. \nCREATE OR REPLACE VIEW v_power_per_day AS\nSELECT system_id,\ndate_trunc('day', read_on) AS day,\nround(sum(power) /4/1 0 0 0 ,2 ) A S kWh,\nFROM readingsGROUP BY ALL;\nA similar concept exists for ordering. An ORDER BY ALL  w i l l  s o r t  t h e  r e s u l t  b y  t h e\nincluded columns, from le ft to right. Querying the freshly created view withListing 3.20 Accessing aliases in WHERE , GROUP BY , and HAVING  clauses\nListing 3.21 Creating grouping sets by grouping by all nonaggregate valuesAccessing an alias that \nrefers to a nonaggregate\nAccessing an alias that \nrefers to an aggregate55 3.5 DuckDB-specific SQL extensions\nSELECT system_id, day FROM v_power_per_day ORDER BY ALL\nwill sort the result first by system_id  and then by day. In the case of a select-star, the\norder of the columns is defined by the table or view definition, of  course. The follow-\ning statement is valid SQL in DuckDB, and it returns the power produced in kWH per\nday, sorted first by systems, by days, and then by kWH. \nFROM v_power_per_day ORDER BY ALL;\n3.5.5 Sampling data\nWhen working with large datasets, we often want to get a sample of the data ratherthan look through everything . Assuming you have imported the readings for at least\nsystem 34, you will have more than 50,000 records in your database. This can be con-\nfirmed with a \nSELECT  count(*)  FROM  readings . We can get an overview of the nonzero\npower readings by asking for a sample of n percent or n number of rows, as shown in\nthe following listing. \nSELECT power\nFROM readings\nWHERE power <> 0\nUSING SAMPLE 10%\n(bernoulli);\nThis is much easier and more flexible than dealing with ar bitrary limits, as it provides\na better and more reliable overview. The sa mpling itself uses probabilistic sampling\nmethods, however, unless a seed is  specified with the additional REPEATABLE  clause.\nThe sampling rate in percent is  not meant to be an exact hi t. In our example, it varies\nby about 2,000 rows from the approximately 20,000 with a power  column not equal to\nzero. \n If you instruct DuckDB to use a specific rate for sampling, it applies system  sam-\npling, including each vector by an equal chance. Sampling on vectors instead of work-\ning on tuples (which is done by the alternative bernoulli  method) is very effective\nand has no extra overhead. As one vector is  roughly about 2,048 tuples in size (see\nhttps:/ /duckdb.org/docs/api/c/data_chunk ), it is not suited for smaller datasets, as\nall data will be included or filtered out. Even for the ~100,000 readings that have apower value greater than zero, we recommend \nbernoulli  for a more evenly distrib-\nuted sampling. \n For a fixed sampling size, a method called reservoir  is used. The reservoir is filled\nup first with as many elements as requ ested and then streams the rest, randomlyListing 3.22 Omitting the SELECT  clause and simplifying ordering\nListing 3.23 Sampling a relation\nRetrieves a sample of roughly \n10% the size of the data\nSpecifies the sampling \nmethod to use56 CHAPTER  3Executing SQL queries\nswapping elements in the reservoir. You ca n learn more about this interesting tech-\nnique in the samples documentation ( https:/ /duckdb.org /docs/sql/samples ). \n3.5.6 Functions with optional parameters\nA couple of functions in DuckDB (e.g., read_json_auto ) have required parameters\nand one or more parameters with sensible defaults that are op tional. The aforemen-\ntioned example has 17 parameters; you can get a list of them with the following code:\nSELECT DISTINCT unnest(parameters)\nFROM duckdb_functions()\nWHERE function_name = 'read_json_auto';\nWe are using a distinct here because there are a couple of overloads with different\ntypes. Luckily, DuckDB supports named op tional arguments. Assume you want to\nspecify the dateformat  only; in that case, you would use the name=value  syntax, as\nshown in the following listing. \necho '{\"foo\": \"21.9.1979\"}' > 'my.json'\nduckdb -s \\\n\"SELECT * FROM read_json_auto(\n'my.json',\ndateformat='%d.%M.%Y'\n)\"\nDuckDB is able to parse the non-iso-formatted string into a proper date since we used\nthe dateformat  parameter:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2502\n\u2502 date \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 1979-01-21 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nSummary\n\uf0a1SQL queries are composed of several st atements, which are, in turn, composed\nof clauses. Queries are ca tegorized as data definition language (DDL) or data\nmanipulation language (DML).\n\uf0a1DML queries cover creating, reading, updating, and deleting rows.\n\uf0a1Manipulation of data is not only about changing persistent state but also about\ntransforming existing relations into new ones; therefore, reading data also falls\nunder DML.\n\uf0a1DDL queries, such as CREATE TABLE  and CREATE VIEW , are used in DuckDB to\ncreate a persistent schema. This is inline with any other relational database andis independent, regardless of whether DuckDB is started with the database\nstored on disk or in memory.Listing 3.24 Using named parameters\nThis is using the named \nparameter dateformat.57 Summary\n\uf0a1A rigid schema makes data inconsistenc ies more visible\u2014bli ndly ingesting data\nwith inconsistencies will fail, due to constraint errors.\n\uf0a1Constraint errors can be mitigate d with appropriate actions defined\nON CONFLICT  when creating or updating rows.\n\uf0a1DuckDB makes SQL even easier to  write, with innovations like SELECT  *\nEXCLUDE()  and SELECT  * REPLACE()  and more intuitive alias usage.58Advanced aggregation\nand analysis of data\nThe goal of this chapter is to give you some ideas on how an analytical database,\nsuch as DuckDB, can be used to provid e reports that would take a considerably\nlarger amount of code written in an imperative programming language. While wewill build upon the foundation laid in chapter 3, we will leave a simple \nSELECT xzy\nFROM  abc behind quickly. Invest ing your time in learning modern SQL won\u2019t be\nwasted. The constructs presented here can be used everywhere DuckDB can be run\nor embedded and, therefore,  enrich your application.This chapter covers\n\uf0a1Preparing, cleaning and aggregating data while \ningesting\n\uf0a1Using window functions to create new aggregates over different partitions of any dataset\n\uf0a1Understanding the different types of subqueries\n\uf0a1Using common table expressions\n\uf0a1Applying filters to any aggregate59 4.1 Pre-aggregating data while ingesting\n4.1 Pre-aggregating da ta while ingesting\nLet\u2019s move forward with our example scenario . In section 3.4.1, we worked with the\ndata for a photovoltaic grid that\u2014while having some consistency problems\u2014was a\ngood fit for our schema and idea. Remember, the goal is to store measurements in\nintervals of 15 minutes. If you look at the other datasets you downloaded throughout\nsection 3.2.1, you will notice that some co me in intervals of other than 15 minutes.\nOne quick way to peek into the files is the tail  command, return ing the last n lines of\na file (head  would work as well). Using it on 2020_10.csv  shows that this file contains\nmeasurements in 1-minute intervals:\n> duckdb -s \".maxwidth 40\" -s \"FROM read_csv_auto('2020_10.csv') LIMIT 3\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 SiteID \u2502 Date-Time \u2502 \u2026 \u2502 module_temp_3 \u2502 poa_irradiance \u2502\n\u2502 int64 \u2502 timestamp \u2502 \u2502 double \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 \u2502 2020-01-23 11:20:00 \u2502 \u2026 \u2502 14.971 \u2502 748.36 \u2502\n\u2502 10 \u2502 2020-01-23 11:21:00 \u2502 \u2026 \u2502 14.921 \u2502 638.23 \u2502\n\u2502 10 \u2502 2020-01-23 11:22:00 \u2502 \u2026 \u2502 14.895 \u2502 467.67 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 3 rows 16 columns (4 shown) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAnd, of course, 2020_1200.csv  has another interval\u2014this time, 5 minutes\u2014but the\noverall structure al so looks different:\n> duckdb -s \".maxwidth 40\" -s \"FROM read_csv_auto('2020_1200.csv') LIMIT 3\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 SiteID \u2502 Date-Time \u2502 \u2026 \u2502 ac_power_metered \u2502 power_factor \u2502\n\u2502 int64 \u2502 timestamp \u2502 \u2502 int64 \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 1200 \u2502 2020-01-01 00:00:00 \u2502 \u2026 \u2502 20 \u2502 0.029 \u2502\n\u2502 1200 \u2502 2020-01-01 00:05:00 \u2502 \u2026 \u2502 20 \u2502 0.029 \u2502\n\u2502 1200 \u2502 2020-01-01 00:10:00 \u2502 \u2026 \u2502 20 \u2502 0.029 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 3 rows 6 columns (4 shown) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRemember, those are data files from the same pool. Even those are inconsistent\nbetween different sources. Data  analytics is quite often a bout dealing with those exact\nsame problems. Let\u2019s use one of the many fu nctions DuckDB offers to deal with dates,\ntimes, and timestamps\u2014in this case, time_bucket() . time_bucket()  truncates time-\nstamps to a given interval and aligns th em to an optional offset, creating a time bucket .\nTime buckets are a powerful mechanism for aggregating sensor readings and friends.\nTogether with GROUP BY  and avg as aggregate functions, we can prepare and eventu-\nally ingest the data according to our re quirements. We create time buckets in a\n15-minute interval and compute the average power produced of all readings that fall\ninto a specific bucket. 60 CHAPTER  4Advanced aggregation and analysis of data\nNOTE The following examples will only work if you followed along with our\nexamples in chapter 3, populating the systems  table. \nWhen you look at the query, you\u2019ll notice a CASE  WHEN  THEN  ELSE  END construct, a CASE\nstatement, which works like an if/else  construct. What it does here is turn readings\nwith a value lower than zero, or with no va lue at all, into zero before computing the\naverage. That\u2019s one of the oddities of this  dataset: maybe the sensor, or perhaps the\nnetwork, malfunctioned. You\u2019ll  never know, but you have to deal with the data. Here,\nwe decided it is OK to treat NULL  values like negative values and cap them to zero. In\ncases that throw off your calculation, you might consider a FILTER  for the aggregate.\nWe will discuss this in section 4.6.3. \nINSERT INTO readings(system_id, read_on, power)\nSELECT any_value(SiteId),\ntime_bucket(\nINTERVAL '15 Minutes',\nCAST(\"Date-Time\" AS timestamp)\n) AS read_on,\navg(\nCASE\nWHEN ac_powe r<0O R ac_power IS NULL THEN 0\nELSE ac_power END)\nFROM\nread_csv_auto(\n'https://developer.nrel.gov/api/pvdaq/v3/' ||\n'data_file?api_key=DEMO_KEY&system_id=10&year=2019'\n)\nGROUP BY read_on\nORDER BY read_on;\nThe imports for the remaining dataset are iden tical. You will want to change the file-\nname in the FROM  clause accordingly. \nNOTE There are many more date- and ti me-based functions in DuckDB.\nWhen in doubt, have a look at  the reference documentation: https:/ /\nduckdb.org/docs/sql/ functions/timestamp . You will be able to parse nearly\nany string into a prop er date or timestamp.\nYour decision on whether to avoid ingestin g altogether and do all kinds of analytics\nin-memory based on external files; aggregate,  to some extent, durin g ingestion; or only\naggregate during analysis usually comes do wn to a tradeoff between, among other\nthings, the size of your dataset, your goal s for long-term storage,  and your further pro-\ncessing needs. Trying to make a generally a pplicable solution here is, therefore, boundListing 4.1 Cleaning and transforming data during ingestion\nThis picks any value of the column  SiteId from the CSV file. The\nfiles are per system, which means th at this column is the same in\neach row, so picking any one of them is correct. Applying\nany_value() is necessary, as we compute an aggregate (avg).\nThis truncates the timestamp to a quarter-hour. \nNotice how we explicitly cast the column to a \ntimestamp with standard SQL syntax; in addition, \nthe transformed value gets an alias (read_on).\nHere, avg computes \nthe average of all \nreadings in the bucket \ncreated previously because we group the \nresult by this bucket.61 4.2 Summarizing data\nto fail. In the scenario here, we decided to  both ingest and aggr egate for educational\npurposes and keep the dataset sma ll enough to be shareable. \n4.2 Summarizing data\nYou usually want to know some characterist ics of a new dataset before going into an\nin-depth analysis of it, such as the number of values (in our example, how many read-\nings), the distribution and magnitude of numerical values (without knowing if we aredealing in watts or kilowatt s, our reports would be blatantly wrong), and the interval\nsize of time series. DuckDB has the unique \nSUMMARIZE  command, which quickly gives\nyou this information about any dataset. Run SUMMARIZE  readings;  in your database.\nYour results sh ould be similar to this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 max \u2502 \u2026 \u2502 q75 \u2502 count \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 \u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 system_id \u2502 INTEGER \u2502 1200 \u2502 \u2026 \u2502 1200 \u2502 151879\u2502\n\u2502 read_on \u2502 TIMESTAMP \u2502 2020-06-26 11:00:0 0\u2502\u2026\u2502 \u2502 151879\u2502\n\u2502 power \u2502 DECIMAL(10,3) \u2502 133900.000 \u2502 \u2026 \u2502 5125 \u2502 151879\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThere are many more columns, but we abbrev iated the list for readability. Switch your\nCLI to line mode by running .mode line  and then summarizing a subset of the read-\nings with\nSUMMARIZE SELECT read_on, power FROM readings WHERE system_id = 1200;\nas follows:\ncolumn_name = read_on\ncolumn_type = TIMESTAMP\nmin = 2019-01-01 00:00:00\nmax = 2020-06-26 11:00:00\napprox_unique = 50833\navg =\nstd =\nq25 =q50 =\nq75 =\ncount = 52072\nnull_percentage = 0.0%\ncolumn_name = power\ncolumn_type = DECIMAL(10,3)\nmin = 0.000\nmax = 47873.333\napprox_unique = 6438\navg = 7122.5597121293595\nstd = 11760.089219586542q 2 5=2 0\nq 5 0=2 7\nq75 = 953262 CHAPTER  4Advanced aggregation and analysis of data\ncount = 52072\nnull_percentage = 0.0%\nSUMMARIZE  works directly on tables, but as sh own in the preceding code snippet, it\nworks on query results too. Yo u don\u2019t even have to ingest data at all before applying\nSUMMARIZE ; it can be run against a CSV or Parquet file as well.\n4.3 On subqueries\nImagine you want to compute the average of  the total power produced by the systems\nyou manage. For that, you would need to apply two aggregate functions: avg and sum.\nIt turns out that you cannot nest them. A naive approach, like \nSELECT avg(sum(kWh)) FROM v_power_per_day GROUP BY system_id\nfails with\nError: Binder Error: aggregate function calls cannot be nested\nYou need to stage that computation, and a subquery is one way to achieve this, as\nshown in the following listing.\nSELECT avg(sum_per_system)\nFROM (\nSELECT sum(kWh) AS sum_per_system\nFROM v_power_per_day\nGROUP BY system_id\n);\nThis statement now dutifully returns avg(sum_per_system) = 133908.087 . The inner\nquery in this statement has two characteristics:\n\uf0a1It returns several rows.\n\uf0a1It does not depend on values from the outer query.\nThis query is called an uncorrelated subquery . An uncorrelated subquery is just a query\nnested inside another one, and it operates  as if the outer query executed on the\nresults of the inner query. \n Now on to the next question you might be faced with: On which day and for which\nsystem was the highest amount of power pr oduced? One way to solve this problem is\nby using a subquery as the right-hand side of a comparison in the WHERE  clause, as\nshown in the following listing.\nSELECT read_on, power\nFROM readings\nWHERE power = (SELECT max(power) FROM readings);Listing 4.2 A subquery being used to compute nested aggregates\nListing 4.3 A subquery being used as a right-hand side in a comparison63 4.3 On subqueries\nThis subquery is different from the first on e in that it only returns a single, scalar\nvalue. This is called a scalar, uncorrelated subquery . \nNOTEarg_min  and arg_max  are aggregate functions that compute an expres-\nsion of the row in which the minimum or maximum value appears. If you are\ninterested in only one expression, usin g these functions as your solution is\npreferable to any subquery for tasks like the ones we\u2019ve covered in the preced-ing text. If you are interested in mo re than one expression or evaluating\nother values than minimum or maximu m values, you won\u2019t get around sub-\nqueries in conditions.\nThe result essentially reads as follows: the maximum output of 133,900 W has been pro-\nduced at five different times . The following snippet shows the full result:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 read_on \u2502 power \u2502\u2502 timestamp \u2502 decimal(10,3) \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 2019-05-08 12:15:00 \u2502 133900.000 \u2502\u2502 2019-05-23 10:00:00 \u2502 133900.000 \u2502\u2502 2019-05-23 11:30:00 \u2502 133900.000 \u2502\u2502 2019-05-28 11:45:00 \u2502 133900.000 \u2502\u2502 2020-04-02 11:30:00 \u2502 133900.000 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWhat if you wanted to determine the ma ximum power and read ing time on a per-\nsystem basis? This would be tricky to do with the original subquery because that only\nshows us the values for the overall max-po wer production. Therefore, we would need\nthe subquery to return different  values for different rows; to  achieve this, we can use a\ncorrelated subquery, which uses the fields from the outer query inside the inner one,\nas shown in the following listing.\nSELECT system_id, read_on, power\nFROM readings r1WHERE power = (\nSELECT max(power)FROM readings r2WHERE r2.system_id = r1.system_id\n)ORDER BY ALL;\nThis subquery is a scalar, correlated subquery . The inner query is related to the outer\nquery in the way the database must evaluate it for every row of the outer query. In the\nresult, we again see the five days for the hi ghest overall value, and now we also see the\nhighest values produced for systems 10 and 1,200:\n Listing 4.4 Using correlated, scalar subqueries\nThis is the condition that correlates \nthe subquery to the outer query, not the comparison of the power value.64 CHAPTER  4Advanced aggregation and analysis of data\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 read_on \u2502 power \u2502\u2502 int32 \u2502 timestamp \u2502 decimal(10,3) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 \u2502 2019-02-23 12:45:00 \u2502 1109.293 \u2502\u2502 34 \u2502 2019-05-08 12:15:00 \u2502 133900.000 \u2502\n\u2502 34 \u2502 2019-05-23 10:00:00 \u2502 133900.000 \u2502\n\u2502 34 \u2502 2019-05-23 11:30:00 \u2502 133900.000 \u2502\u2502 34 \u2502 2019-05-28 11:45:00 \u2502 133900.000 \u2502\n\u2502 34 \u2502 2020-04-02 11:30:00 \u2502 133900.000 \u2502\n\u2502 1200 \u2502 2020-04-16 12:15:00 \u2502 47873.333 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWhen used as expressions, subqueries ma y be rewritten as joins\u2014with the computa-\ntion of nested aggregates being the exceptio n. For the last example, it would look like\nthe code in the following listing.\nSELECT r1.system_id, read_on, power\nFROM readings r1\nJOIN (\nSELECT r2.system_id, max(power) AS valueFROM readings r2\nGROUP BY ALL\n) AS max_power ON (\nmax_power.system_id = r1.system_id AND\nmax_power.value = r1.power\n)\nORDER BY ALL;\nIt\u2019s up to the reader to judge whether this adds to readability or not. In other rela-\ntional databases, people often do this, as the evaluation of a co rrelated subquery for\nevery row in a large table might be slow. DuckDB, on the other hand, uses a subquery\ndecorrelation optimizer that  always makes subqueries in dependent of outer queries,\nthus allowing users to freely use subqueries  to create expressive queries without hav-\ning to worry about manually rewriting subqueri es into joins. It is  not always possible to\nmanually decorrelate certai n subqueries by rewriting the SQL. Internally, DuckDB\nuses special types of joins th at will decorrelate all subqueries. In fact, DuckDB does\nnot have support for executing subqu eries that are not decorrelated.\n This is a positive for you because it al lows you to focus on the readability and\nexpressiveness of your quer ies and the business problem you are trying to solve.\nIndeed, DuckDB allows you to  spend all your time focu sing on the bigger picture\u2014\nyou don\u2019t need to worry about what type of subquery you use at all.\n4.3.1 Subqueries as expressions\nAll forms of subqueries, both correlated and uncorrelated, th at are not used as a rela-\ntion in a JOIN  are expressions. As such, many othe r operators can be used with them.Listing 4.5 An uncorrelated subquery join with the outer table65 4.3 On subqueries\nThe = operator and the inequality operators <, <=, >=, and > require the subquery to\nbe scalar, returning exactly one row. When  working with both scalar and nonscalar\nsubqueries, additional operators are available\u2014 IN, EXISTS , ANY, and ALL\u2014all of which\nwork via set comparisons. \n Subqueries can also be used in set comp arisons, completing tasks like this one:\nidentify all the rows that compare successfully to all or any of the rows returned by another query.\nThe artificial examples in this section will all return v = 7 .\nEXISTS\nYou might want to select all the rows of a ta ble that have a value that might exist inside\none row of another table. For this, you can use the EXISTS  expression, shown in the\nfollowing listing. \n.mode line\nSELECT * FROM VALUES (7), (11) s(v)\nWHERE EXISTS (SELECT * FROM range(10) WHERE range = v);\nIN\nEXISTS  can usually be rewritten as an  uncorrelated subquery using the IN operator, as\nshown in the following listing. When the oute r value is contained at least once in the\nresults of the subquery, this operator evaluates to true.\n.mode line\nSELECT * FROM VALUES (7), (11) s(v)\nWHERE v IN (SELECT * FROM range(10));\nThis is useful to know when you work with relational databases other than DuckDB\nthat might not do all kinds of  optimizations on subqueries. \nANY\nThe IN operator works with an equal comparison  of each value. You might find your-\nself in a situation in which you want to  answer whether any value does satisfy an\ninequality condition. He re, you need to use the ANY operator together with the\ndesired comparison, as shown in the following listing. When the comparison of the\nouter value with any of the inner values ev aluates to true, the whole expression evalu-\nates to true. \n.mode line\nSELECT * FROM VALUES (7), (11) s(v)\nWHERE v <= ANY (SELECT * FROM range(10));Listing 4.6 A subquery used with the EXISTS  expression\nListing 4.7 A subquery used with the IN expression\nListing 4.8 A subquery used with the ANY expressionDefines an inline table named \ns with one column named v \nPlease take note of the additional \ncomparison prior to ANY.66 CHAPTER  4Advanced aggregation and analysis of data\nALL\nLast but not least, we cover the ALL operator, which evaluates to true when the com-\nparison of the outer value with all of the i nner values evaluates to true. It helps you\nfind rows in which a value satisfies a comp arison between all valu es of a subquery, as\nshown in the following listing. While you can replace = ANY()  with IN() , there is no\nsuch simplification for the ALL operator. \n.mode line\nSELECT * FROM VALUES (7), (11) s(v)\nWHERE v = ALL (SELECT 7);\n4.4 Grouping sets\nIn listing 3.2, we created a table named readings , which contains the date, time, and\nactual value of power produced  at that time. We also suggested several example data-\nsets from the National Renewable Energy Laboratory to import. When looking at sucha dataset, it is always helpful to get an  overview of the minimum and maximum values\nof an attribute, or maybe the average. Sometimes, you may have outliers in there thatyou want to delete, or you may have made a mistake with the units. The easiest way to\ncompute that is by using them  in one query, without any \nGROUP BY  clause, as shown in\nthe following listing, so that the aggregat ion happens in one bucket: the whole table. \nSELECT count(*),\nmin(power) AS min_W, max(power) AS max_W,round(sum(power )/4/1 0 0 0 ,2 )A Sk W h\nFROM readings;\nIf you followed the suggestion, your readings  table should have key figures like the\nfollowing, which is the result of the preceding query:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502 min_W \u2502 max_W \u2502 kWh \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 151879 \u2502 0.000 \u2502 133900.000 \u2502 401723.22 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe readings seem to be reasonable, even  the minimum value of zero\u2014there is just\nno production during nighttime. As we already learned about the GROUP BY  clause in\nfigure 3.2, we could go further and have a look at the production of kWh and system.\nWe will also select the number of readin gs per system. We imported several years,Listing 4.9 A subquery used with the ALL expression\nListing 4.10 Using various aggregates to check if the imports make sense\nWhen finding the amount of power produced, we express the conversion\nfrom units of W per 15 minutes to  units of kW per hour (kWh) by\nsumming the values, dividing them by 4, which gives us a result in W per\nhour (Wh), and then divi ding them by 1,000, achieving a result in kWh.67 4.4 Grouping sets\ntruncating the readings to 15- minute intervals, so  we should find roughly 35,040 read-\nings per year. A GROUP  BY system_id,  year  confirms this assumption, as shown in the\nnext listing.\nSELECT year(read_on) AS year,\nsystem_id,\ncount(*),\nround(sum(power )/4/1 0 0 0 ,2 )A Sk W h\nFROM readings\nGROUP BY year, system_id\nORDER BY year, system_id;\nThe result adds up. We did have a bunch of invalid values, and the second year ends\nhalfway through 2020:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 year \u2502 system_id \u2502 count_star() \u2502 kWh \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2019 \u2502 10 \u2502 33544 \u2502 1549.34 \u2502\n\u2502 2019 \u2502 34 \u2502 35040 \u2502 205741.9 \u2502\u2502 2019 \u2502 1200 \u2502 35037 \u2502 62012.15 \u2502\n\u2502 2020 \u2502 10 \u2502 14206 \u2502 677.14 \u2502\n\u2502 2020 \u2502 34 \u2502 17017 \u2502 101033.35 \u2502\u2502 2020 \u2502 1200 \u2502 17035 \u2502 30709.34 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNow what about the totals (i.e., the total numb er of readings as well as the total power\nproduction per year, per year  and system, and overall)? In  other words, can we create\na drill-down report, showing different levels of detail per group? While we could now\nenter the numbers into a calculator one by  one and sum them manually or write an\nadditional count  query without a grouping key like the initial one, there\u2019s a better\noption, as shown in the following listing: grouping sets . \nSELECT year(read_on) AS year,\nsystem_id,count(*),\nround(sum(power )/4/1 0 0 0 ,2 )A Sk W h\nFROM readingsGROUP BY GROUPING SETS ((year, system_id), year, ())\nORDER BY year NULLS FIRST, system_id NULLS FIRST;\nBefore we dissect\nGROUP BY GROUPING SETS ((system_id, year), year, ())\nlet\u2019s have a look at the result:Listing 4.11 A plain GROUP BY  with essentially one set of grouping keys ()\nListing 4.12 Explicitly using GROUPING SETS68 CHAPTER  4Advanced aggregation and analysis of data\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 year \u2502 system_id \u2502 count_star() \u2502 kWh \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 \u2502 \u2502 151879 \u2502 401723.22 \u2502\n\u2502 2019 \u2502 \u2502 103621 \u2502 269303.39 \u2502\n\u2502 2019 \u2502 10 \u2502 33544 \u2502 1549.34 \u2502\u2502 2019 \u2502 34 \u2502 35040 \u2502 205741.9 \u2502\n\u2502 2019 \u2502 1200 \u2502 35037 \u2502 62012.15 \u2502\n\u2502 2020 \u2502 \u2502 48258 \u2502 132419.83 \u2502\u2502 2020 \u2502 10 \u2502 14206 \u2502 677.14 \u2502\n\u2502 2020 \u2502 34 \u2502 17017 \u2502 101033.35 \u2502\n\u2502 2020 \u2502 1200 \u2502 17035 \u2502 30709.34 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe grouping sets created several bucket s to compute the aggregates as follows:\n\uf0a1Over a bucket defined by the combined values of system_id  and year  (six dif-\nferent combinations in our exam ple, thus leading to six rows)\n\uf0a1Over a bucket defined by the year  alone. For keys not included in this but in\nother sets, NULL  values are provided (here for the system_id ).\n\uf0a1The last one ( ()) can be described as the empty bucket or group\u2014 NULL  values\nare provided for all other keys.\nThe result contains everything listing 4.11 returned plus the number of readings per\nyear (grouping by year  alone) plus the overall count (grouping by nothing).\n The same result can be achieved  by using the shorthand clause ROLLUP . As shown\nin the following listing, the ROLLUP  clause automatically produces the previously dis-\ncussed sets as n + 1 grouping sets, where n is the number of terms in the ROLLUP\nclause. \nSELECT year(read_on) AS year,\nsystem_id,count(*),round(sum(power )/4/1 0 0 0 ,2 )A Sk W h\nFROM readingsGROUP BY ROLLUP (year, system_id)ORDER BY year NULLS FIRST, system_id NULLS FIRST;\nIf we want to see the totals by system in a ll years, this is quite possible too. Instead of\nfalling back from ROLLUP  to GROUP  BY GROUPING  SETS  and manually adding, you\ncan use GROUP  BY CUBE . GROUP  BY CUBE  will not produce subgroups but actual combina-\ntions (2n grouping sets). In ou r example, it produces (year,  system_id) , (year) ,\n(system) , and (). \n Listing 4.13 Using GROUP BY ROLLUP69 4.5 Window functions\n \nSELECT year(read_on) AS year,\nsystem_id,count(*),\nround(sum(power )/4/1 0 0 0 ,2 )A Sk W h\nFROM readingsGROUP BY CUBE (year, system_id)\nORDER BY year NULLS FIRST, system_id NULLS FIRST;\nThis produces the following:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 year \u2502 system_id \u2502 count_star() \u2502 kWh \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 \u2502 \u2502 151879 \u2502 401723.22 \u2502\n\u2502 \u2502 10 \u2502 47750 \u2502 2226.48 \u2502\n\u2502 \u2502 34 \u2502 52057 \u2502 306775.25 \u2502\u2502 \u2502 1200 \u2502 52072 \u2502 92721.48 \u2502\n\u2502 2019 \u2502 \u2502 103621 \u2502 269303.39 \u2502\n\u2502 2019 \u2502 10 \u2502 33544 \u2502 1549.34 \u2502\u2502 2019 \u2502 34 \u2502 35040 \u2502 205741.9 \u2502\n\u2502 2019 \u2502 1200 \u2502 35037 \u2502 62012.15 \u2502\n\u2502 2020 \u2502 \u2502 48258 \u2502 132419.83 \u2502\u2502 2020 \u2502 10 \u2502 14206 \u2502 677.14 \u2502\n\u2502 2020 \u2502 34 \u2502 17017 \u2502 101033.35 \u2502\n\u2502 2020 \u2502 1200 \u2502 17035 \u2502 30709.34 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe now have a complete overview of our po wer production readings in a single com-\npact query, instead of several queries. Al l the drill downs we added on the way can be\nexpressed with grouping sets. The mini mum and maximum values have only been\nomitted to keep the listing readable. \n4.5 Window functions\nWindows and functions applied over window s are an essential part of modern SQL\nand analytics. Window functions , in general, let you look at other rows. Normally, an\nSQL function can only see th e current row at a time, unle ss you\u2019re aggregating. In\nthat case, however, you reduce the number of rows. \n Unlike a regular aggregate function, the use of a window function does not cause\nrows to become grouped into a single ou tput row\u2014the rows retain their separate\nidentities. If you wanted to peek at othe r rows, you would use a window function. A\nwindow is introduced by the OVER()  clause, following the function you want to apply\nto the data inside that window. The window it self is the definition of the rows that are\nworked on, and you can think of it as a wi ndow that moves along a defined order with\na defined size of rows over your dataset.  Windowing works by breaking a relation up\ninto independent partitions, optionally or dering those partitio ns, and then comput-\ning a new column for each row as a function of the nearby values. Listing 4.14 Using GROUP BY CUBE70 CHAPTER  4Advanced aggregation and analysis of data\n For looking at all rows, yo u can use an empty window OVER () . If you want to look\nat all rows that have the same value matc hing another field, use a partition for that\nfield. And last but not least, if you want to look at nearby rows, you can use a frame.\n The size of a window is not equal to th e size of a partition; both can be defined\nindependently. Eventually, the contents of a window are fed to a function to compute\nnew values. While there are a couple of dedica ted functions that work only in the con-\ntext of windows, all regula r aggregate functions can be used as window functions.\n This allows use cases such as\n\uf0a1Ranking\n\uf0a1Computing independent aggregates per window\n\uf0a1Computing running totals per window\n\uf0a1Computing changes by accessing preceding or following rows via lag or lead\nLet\u2019s have a look at a concrete example. Suppose you want to retrieve the system as\nwell as the top three times (by quarter-ho ur) at which the most power was produced.\nOne naive approach would be ordering the results by amount of power produced and\nlimiting to 3, as follows:\nSELECT * FROM readings ORDER BY power DESC LIMIT 3;\nThis approach yields the following results:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 read_on \u2502 power \u2502\u2502 int32 \u2502 timestamp \u2502 decimal(10,3) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 34 \u2502 2019-05-08 12:15:00 \u2502 133900.000 \u2502\u2502 34 \u2502 2019-05-23 10:00:00 \u2502 133900.000 \u2502\n\u2502 34 \u2502 2019-05-23 11:30:00 \u2502 133900.000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWhile these results present readings for syst em 34 at different dates, you should notice\nthat they have the same value in the power  column. This might be good enough, but it\nis not necessarily what we have been asked for. For the raw value of power produced,we only get the time at which the very most  power was produced (i.e., top 1), not read-\nings for each of the top three power values . To compute a proper top three, we will\nuse the window function \ndense_rank() . This function computes the rank for a row\nwithout skipping ranks for equal rankings. dense_rank , shown in listing 4.15, returns\nthe rank of the current row without gaps. That means, for example, that if five rows\nare ranked 1 in power production, the ro w producing the next highest amount of\npower will still be ranked 2, as opposed to 6. If you need gaps to be included (i.e., thenext row to be ranked 6, as opposed to 2), use \nrank  instead. \nWITH ranked_readings AS (\nSELECT *,Listing 4.15 A proper top- n query71 4.5 Window functions\ndense_rank()\nOVER (ORDER BY power DESC) AS rnk\nFROM readings\n)\nSELECT *\nFROM ranked_readingsWHERE rnk <= 3;\nThe result looks very different now, with  three different decreasing values for power ,\nas shown in figure 4.1. We will revisit th e preceding statement when we learn about\nthe QUALIFY  clause in section 4.6.2, avoiding  the somewhat odd condition in the\nWHERE  clause that filters on the rank. \nFigure 4.1 The simplest possible window over the power readings\nThe ORDER  clause as part of the wi ndow definition inside the OVER()  clause is optional,\nand unlike the ORDER BY  clause at the end of a statement, it does not sort the query\nresult. When used as part of the OVER()  clause, ORDER BY  defines the order in which\nwindow functions are executed. If ORDER BY  is omitted, the window function is exe-\ncuted in an arbitrary order. In our preceding example, it would make no sense to omitit, as an unordered, dense rank would always  be one. We will see an example in the\nnext section, where we can safely omit \nORDER BY . \n4.5.1 Defining partitions\nThe preceding ranked power values are better,  but they are not ye t particularly help-\nful, as the systems have pr oduction values that are or ders of magnitudes different.\nComputing ranks without differentiating the systems might not be what you are after.\nWhat you actually need in this case is the top three readings per system, with each sys-\ntem making up its own partition of the data . Partitioning breaks the relation up intoHere, the window is opened \nover one row each, ordered \nby the amount of power, in descending order.\n34system_id\n2019-05-08  12:15:00read_on\n133900.000power\n133900.000\n133900.000133900.000133900.000133700.000\n133700.000\n133600.000\n133600.0001rnk\n1\n1112\n2\n3\n32019-05-23  10:00:00\n2019-05-23  11:30:002019-05-28  11:45:00\n2019-05-09  10:30:00\n2019-05-10  12:15:00\n2019-03-21  13:00:00\n2019-04-02  10:30:002020-04-02  11:30:0034\n34343434\n34\n34\n34\nRank changing here\nwhen the powervalue decreasesWindow moving over\nthe dataset, orderedby power (ascending,1 row height)72 CHAPTER  4Advanced aggregation and analysis of data\nindependent, unrelated pieces, in which the window function is applied. If we don\u2019t\ndefine how a partition is made up by using the PARTITION BY  clause, the entire rela-\ntion is treated as a single partition. Wind ow functions cannot access values outside the\npartition containing the row they are being evaluated at. \n Requesting the top n measurements per system would be a partitioning task. For\nthe sake of readability of the results, in th e following listing, we only request the two\nhighest power-production values per system.\nWITH ranked_readings AS (\nSELECT *,\ndense_rank()\nOVER (\nPARTITION BY system_id\nORDER BY power DESC\n)A Sr n k\nFROM readings\n)\nSELECT * FROM ranked_readings WHERE rnk <= 2\nORDER BY system_id, rnk ASC;\nLook closely at how the number of ranks now repeat in the result in figure 4.2. They\nhave now been computed individually insi de the respective partitions, which makes\nquite a contrasting statement about the dataset.\nFigure 4.2 Partitioning the data before applying a windowListing 4.16 Applying a partition to a window\nStarting the definition \nof the moving window\nDefining a partition: \nthe window\nFirst partition (System 10)\nSecond partition (System 34)\nThird partition (System 1200)Partition function\n(here, rank)resets at thepartition boundary\nWindow moving over\nthe dataset, ordered by power (ascending,1 row height)system_id read_on power rnk\n34 2019-05-08  12:15:00 133900.00010 2019-02-23  12:45:00 1109.293\n133900.000\n133900.000\n133900.000\n133900.000133700.000\n133700.00011\n10 2019-03-01  12:15:00 1087.900 2\n1\n1\n1\n12\n22019-05-23  10:00:00\n2019-05-23  11:30:00\n2019-05-28  11:45:00\n2019-05-09  10:30:00\n2019-05-10  12:15:002020-04-02  11:30:0034\n34\n34\n3434\n34\n47873.333 1 2020-04-16  12:15:001200\n47866.667 2 2020-04-02  12:30:001200\n47866.667 2 2020-04-16  13:15:00120073 4.5 Window functions\nWe see ranks 1 and 2 for all systems; system 34 reaches its highest rate of production,\n133,900 W, five separate time s and its second highest twice.  System 1,200 only reaches\nits top rate of production once and its se cond highest twice. The window was parti-\ntioned by the systems and then ordered by the value of power produced.\n Of course, ranking tasks are not the only things that can be applied within parti-\ntions. Aggregate functions, like avg, sum, max, and min, are other excellent candidates\nto be used in a windowing context. The difference with using aggregates within a win-dow context is that they don\u2019t change the number of rows being produced. Let\u2019s sayyou want to select both the production on each system each day and in an additionalcolumn, the average overall production of system. You might consider using \nGROUP BY\nROLLUP \u2014and you would not be wrong. That gr ouping set would, however, be quite\nlarge (GROUP  BY ROLLUP  (system_id,  day,  kwh) ) and not produce the average value in\nan additional column but additional rows instead. The value you would be looking for\n(the overall production per system) would be  found in the rows that have a value for\nthe system and no value for the day. \n One way to avoid dealing with additional rows is a self-join , in which you select the\ndesired aggregate grouping by a key to join the same tabl e again. While it does pro-\nduce the results you want, it will be hard to read and most like ly will not perform well,\nas the whole table would be scanned twice. Using avg in a partitioned window context\nis much easier to read and will perf orm well. The aggregate\u2014in this case, avg(kWh) \u2014\nis computed over the window that follows; it does not change the number of rows and\nwill be present in each row. It will be computed for every system, as defined by the\npartition:\nSELECT *,\navg(kWh)\nOVER (\nPARTITION BY system_id\n) AS average_per_system\nFROM v_power_per_day;\nAnd you will find the requested value in an additional column:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 system_id \u2502 day \u2502 kWh \u2502 average_per_system \u2502\u2502 int32 \u2502 date \u2502 double \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 \u2502 2019-01-01 \u2502 2.19 \u2502 4.444051896207586 \u2502\u2502 10 \u2502 2019-01-04 \u2502 5.37 \u2502 4.444051896207586 \u2502\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u2502 1200 \u2502 2019-07-25 \u2502 232.37 \u2502 170.75771639042347 \u2502\u2502 1200 \u2502 2019-04-29 \u2502 210.97 \u2502 170.75771639042347 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 1587 rows (4 shown) 4 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Computing an aggregate \nover a partition74 CHAPTER  4Advanced aggregation and analysis of data\nNote that we omitted the ORDER  BY inside the window definition, as it is irrelevant for\nthe average value, in which order values ar e fed to the aggregate. As a rule of the\nthumb, you should probably use a window fu nction every time you consider writing a\nself-join like the one previous ly discussed for adding aggregates to your query without\nchanging the row count. \n4.5.2 Framing\nTop n queries are useful, for example, if you happen to have a streaming service and\nwant to present the top n charts. A more interesting question in our example is this:\nWhat is the seven-day moving average of energy produced system wide?  To answer this ques-\ntion, we must do the following:\n\uf0a1Aggregate the readings per 15-minute interval into days (grouping and\nsumming).\n\uf0a1Partition by day and systems.\n\uf0a1Create frames of seven days.\nThis is where framing comes into play. Framing  specifies a set of rows relative to each\nrow where the function is eval uated. The distance from th e current row is given as an\nexpression either preceding or following th e current row. This distance can either be\nspecified as an integral numb er of rows or as a range de lta expression from the value\nof the ordering expression.\n For readability and to keep the following  examples focused on the window defini-\ntions, we will use the view v_power_per_day  defined in chapter 3, which returns the\namount of energy produced in kWh per system  and day. We could just as easily express\nv_power_per_day  as a CTE.\n The following statement computes the average power over a window per system\nthat moves along the days and is seven days  wide (three days before, the actual day,\nand three days ahead). The statement utili zes all options for defining a window.\nSELECT system_id,\nday,\nkWh,\navg(kWh) OVER (\nPARTITION BY system_id\nORDER BY day ASC\nRANGE BETWEEN INTERVAL 3 Days PRECEDING\nAND INTERVAL 3 Days FOLLOWING\n) AS \"kWh 7-day moving average\"\nFROM v_power_per_dayORDER BY system_id, day;\nThe result will have as many rows as there are full days in the source readings, so we\ncan only show a subset as an example. Figu re 4.3 demonstrates the size of the window\nand how rows are included. Listing 4.17 Using a range partition for applying a window function\nThe window should move over \npartitions defined by the system id. Ordered\n by day\nWith a size of seven \ndays in total75 4.5 Window functions\nFigure 4.3 The result of framing a window\n4.5.3 Named windows\nWindow definitions can be pr etty complex, as we just learned while discussing win-\ndows with ranges. They can include the defini tion of the partition,  the order, and the\nactual range of a window. Sometimes, you ar e interested in more than just one aggre-\ngate over a given window. Repeating the wind ow definition over and over again would\nbe a tedious task. \n For our domain\u2014measuring  power production from photovoltaic systems\u2014we\ncould use quantiles to create a report that essentially takes in both the seasons and the\nweather by computing the quantiles over a seven-day window per month. Sometimes,\na broad monthly average will be enough, bu t a chart would represent only a relatively\nsmooth curve, changing with the months. The fluctuation of the amount of powerSecond partitionFirst partitionMoving direction\nFirst window inside partition \n2, applying the average function to the current row and 3 following rowsFourth window inside partition 1,\napplying the average function to the current row and 3 preceding and following rowssystem_id day kWh kWh 7-day moving average\n1200 1.662019-01-14 39.995714285714291200 0.482019-01-13 64.628571428571451200 58.182019-01-12 72.660000000000011200 186.972019-01-11 79.319999999999981200 188.992019-01-10 83.612857142857141200 65.662019-01-09 105.997142857142861200 53.32019-01-08 102.124285714285721200 31.712019-01-07 82.051428571428571200 157.172019-01-06 62.638571428571421200 31.072019-01-05 56.798571428571431200 46.462019-01-04 55.871428571428571200 53.12019-01-03 59.898333333333331200 24.782019-01-02 40.4441200 46.812019-01-01 42.787510 5.092019-01-15 4.04285714285714310 6.222019-01-14 4.06428571428571510 5.92019-01-13 3.781428571428571610 1.582019-01-12 3.721428571428571510 0.282019-01-11 3.754285714285714410 3.462019-01-10 3.695714285714285310 3.522019-01-09 3.380000000000000310 5.322019-01-08 3.814285714285714510 5.812019-01-07 4.54142857142857110 3.692019-01-06 4.86428571428571510 4.622019-01-05 5.15428571428571410 5.372019-01-04 4.70714285714285710 5.722019-01-03 4.552333333333333410 5.552019-01-02 4.6910 2.192019-01-01 4.707500000000000576 CHAPTER  4Advanced aggregation and analysis of data\nproduced is higher throughout the changing  weather in a week. Outliers and runaway\nvalues would be better caught and represente d by quantiles. The result can easily be\nused to create a moving box- and-whisker plot, for example.\n We need three aggregates ( min, max, and quantiles ) for caching outliers and com-\nputing the quantiles, and we don\u2019t want to define the window each time. We basically\ntake the definition from listing 4.17 and add the month of the reading to the parti-\ntion, as shown in the followin g listing. Otherwise, the window definition is the same.\nWe move the definition after the FROM  clause and name it seven_days . It can be refer-\nenced from as many aggregates as necessary. \nSELECT system_id,\nday,min(kWh) OVER seven_days AS \"7-day min\",quantile(kWh, [0.25, 0.5, 0.75])\nOVER seven_days AS \"kWh 7-day quartile\",\nmax(kWh) OVER seven_days AS \"7-day max\",\nFROM v_power_per_dayWINDOW\nseven_days AS (\nPARTITION BY system_id, month(day)ORDER BY day ASCRANGE BETWEEN INTERVAL 3 Days PRECEDING\nAND INTERVAL 3 Days FOLLOWING\n)\nORDER BY system_id, day;\nThe result now showcases a structured column type\u2014 kWh 7-day quartile :\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502system_id \u2502 day \u2502 7-day min \u2502 kWh 7-day quartile \u2502 7-day max\u2502\u2502 int32 \u2502 date \u2502 double \u2502 double[] \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 \u2502 2019-01-01 \u2502 2.19 \u2502 [2.19, 5.37, 5.55] \u2502 5.72\u2502\u2502 10 \u2502 2019-01-02 \u2502 2.19 \u2502 [4.62, 5.37, 5.55] \u2502 5.72\u2502\u2502 10 \u2502 2019-01-03 \u2502 2.19 \u2502 [3.69, 4.62, 5.55] \u2502 5.72\u2502\u2502 10 \u2502 2019-01-04 \u2502 2.19 \u2502 [3.69, 5.37, 5.72] \u2502 5.81\u2502\u2502 10 \u2502 2019-01-05 \u2502 3.69 \u2502 [4.62, 5.37, 5.72] \u2502 5.81\u2502\u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u2502 1200 \u2502 2020-06-22 \u2502 107.68 \u2502 [149.11, 191.61, 214.68] \u2502 279.8\u2502\u2502 1200 \u2502 2020-06-23 \u2502 0.0 \u2502 [107.68, 191.61, 214.68] \u2502 279.8\u2502\u2502 1200 \u2502 2020-06-24 \u2502 0.0 \u2502 [190.91, 191.61, 214.68] \u2502 279.8\u2502\u2502 1200 \u2502 2020-06-25 \u2502 0.0 \u2502 [191.61, 203.06, 214.68] \u2502 279.8\u2502\u2502 1200 \u2502 2020-06-26 \u2502 0.0 \u2502 [0.0, 203.06, 214.68] \u2502 279.8\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25021587 rows (10 shown) 5 columns\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Listing 4.18 Using a named window with a complex order and partition\nReferencing the window \ndefined after the FROM clause\nThe quantile func tion takes in \nthe value for which the quantiles \nshould be computed and a list of the desired quantiles.\nThe window clause must be \nspecified after the FROM clause, \nand the definition of window \nitself follows inline windows.77 4.5 Window functions\nAll aggregates can be used as windowing functions, as we already learned. That\nincludes complex statistical functions, su ch as computing the exact quantiles in a\ngroup (quantile  and quantile_disc ) or the interpolated ones ( quantile_cont ), as\npreviously shown. The implementations of these functions have been optimized for\nwindowing, and we can use them without worrying about performance. Use a named\nwindow when you\u2019re queryi ng for several aggregates. \n4.5.4 Accessing preceding or fo llowing rows in a partition\nWe already discussed ranking and will see an example of computing running totals in\nsection 4.8, but we haven\u2019t used the ability to jump back and forth between rows inside\na partition. So let\u2019s explore computing ch anges and what might be a better example\nthan prices these days. \n In chapter 3, we created a table named prices , which contained the sales prices\n(in ct/kWH) for feeding energy back to th e grid in Germany. Suppose those sales\nprices have since decreased as a reaction to  a decrease in policy incentives promoting\nrenewable energy. You now want to know by how much the compensation for renew-\nable energy has changed over time. To comp ute a difference, you need the price value\nof row n, and then you need to compare it with the value in row n-1. This is not possi-\nble without windows, as the rows of a table are processed in isolation, essentially row\nby row. If you, however, span a window  over any orderable column, you can use lag()\nand lead()  to access rows outside the current window. This allows you to pick the\nprice from yesterday that you want  to compare with today\u2019s price. \n The lag function will give you the value of the expression in the row preceding the\ncurrent one within the partition or NULL  if there is none. This is the case for the first row\nin a partition. lead  behaves in the opposite manner (i.e., it returns NULL  for the last row\nin a partition). Both functions have several overloads in DuckDB that allow you to spec-\nify not only by how many rows to lag or lead  but also a default window. Otherwise, work-\ning with coalesce  would be an option when NULL  values are not practicable. \nNOTE The coalesce  function will return its first non- NULL  argument. \nUsing lag() , the following query computes the di fference between the original prices\n(i.e., those in the prices  table in chapter 3) and the cu rrent prices (i.e., which have\nincreased in response to new regulations). \nSELECT valid_from,\nvalue,lag(value)\nOVER validity AS \"Previous value\",\nvalue - lag(value, 1, value)\nOVER validity AS ChangeListing 4.19 Computing lagging and leading values of windows\nJumps back a row and picks \nout the value column The change is computed as the difference \nof the price in the current row and the \nprice in the row before that, or the same value if there is no row before.78 CHAPTER  4Advanced aggregation and analysis of data\nFROM prices\nWHERE date_part('year', valid_from) = 2019WINDOW validity AS (ORDER BY valid_from)ORDER BY valid_from;\nAs you can see, in each successive period , the price decreased considerably in 2019:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 valid_from \u2502 value \u2502 Previous value \u2502 Change \u2502\u2502 date \u2502 decimal(5,2) \u2502 decimal(5,2) \u2502 decimal(6,2) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2019-01-01 \u2502 11.47 \u2502 \u2502 0.00 \u2502\u2502 2019-02-01 \u2502 11.35 \u2502 11.47 \u2502 -0.12 \u2502\n\u2502 2019-03-01 \u2502 11.23 \u2502 11.35 \u2502 -0.12 \u2502\n\u2502 2019-04-01 \u2502 11.11 \u2502 11.23 \u2502 -0.12 \u2502\u2502 \u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\n\u2502 \u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\n\u2502 \u00b7 \u2502\u00b7 \u2502 \u00b7 \u2502\u00b7 \u2502\u2502 2019-09-01 \u2502 10.33 \u2502 10.48 \u2502 -0.15 \u2502\n\u2502 2019-10-01 \u2502 10.18 \u2502 10.33 \u2502 -0.15 \u2502\n\u2502 2019-11-01 \u2502 10.08 \u2502 10.18 \u2502 -0.10 \u2502\u2502 2019-12-01 \u2502 9.97 \u2502 10.08 \u2502 -0.11 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows (8 shown) 4 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIf we are interested in computing the total change in prices in 2019, we must use a\nCTE, as we cannot nest window  function calls inside aggreg ate functions. A solution is\nshown in the following listing.\nWITH changes AS (\nSELECT value - lag(value, 1, value) OVER (ORDER BY valid_from) AS vFROM pricesWHERE date_part('year', valid_from) = 2019ORDER BY valid_from\n)SELECT sum(changes.v) AS total_changeFROM changes;\nThe result of this computation gives us th e price difference for Germany in 2019 that\nwe\u2019ve been looking for. For privately produced renewable energy, we find that the\ncompensation has been cut by 1.50 ct/kWh. \n4.6 Conditions and filterin g outside the WHERE clause\nNeither computed aggregates nor the result of  a window function can be filtered via the\nstandard WHERE  clause. Such filtering is necessary to answer questions like the following:\n\uf0a1Selection of groups that  have an aggregated value that exceeds value  x\u2014For this, you\nwould have to use the HAVING  clause.Listing 4.20 Computing the aggregate over a window79 4.6 Conditions and filtering outside the WHERE clause\n\uf0a1Selection of data that exceeds a certain value in a range of days \u2014Here, the QUALIFY\nclause must be used.\nIn addition, you might need to filter out va lues to keep them from entering an aggre-\ngate function at all, using the FILTER  clause. Table 4.1 summar izes the options for fil-\ntering the values that go into aggregates or filtering the results of those aggregates. \n4.6.1 Using the HAVING clause\nPlease give me all the days with  production exceeding 900 kWh!  In chapter 3, you learned\nabout both the WHERE  clause and how GROUP  BY works. You can attempt to combine\nthem like this:\nSELECT system_id,\ndate_trunc('day', read_on) AS day,\nround(sum(power) /4/1 0 0 0 ,2 ) A S kWh,\nFROM readingsWHERE kWh >= 900\nGROUP BY ALL;\nPrior to DuckDB 0.10, the quer y gave you an error like this: Error: Binder Error: Refer-\nenced column \u201ckWh\u201d not found in FROM clause!  Recent versions have improved the word-\ning of this error, now telling you that a WHERE  clause cannot contain aggregates. What\nthat means is this: the computed column kWh is not yet known when the WHERE  clause\nwill be applied, and it can\u2019t be kn own at that point (in contrast to day, which is a com-\nputed column as well). Selecting rows in the WHERE  clause, or filtering rows, in other\nwords, modifies what rows get aggregated  in the first place. Therefore, you need\nanother clause that gets ap plied after aggregation: the HAVING  clause. It is used after\nthe GROUP BY  clause to provide filter criteria a fter the aggregation of  all selected rows\nhas been completed. \n Returning to the initial task, all you need  to do is move the condition out of the WHERE\nclause into the HAVING  clause that follows GROUP  BY, as shown in the following listing. \nSELECT system_id,\ndate_trunc('day', read_on) AS day,\nround(sum(power) /4/1 0 0 0 ,2 ) A S kWh,Table 4.1 Filtering clauses and where to use them\nWhere to use it Effect\nHAVING After GROUP BY Filters rows based on aggregates com-\nputed for a group\nQUALIFY After the FROM  clause referring to any win-\ndow expressionFilters rows based on anything computed \nin that window\nFILTER After any aggregate function Filters the values passed to the aggregate\nListing 4.21 Using the HAVING  clause to filter rows based on aggregated values80 CHAPTER  4Advanced aggregation and analysis of data\nFROM readings\nGROUP BY ALL\nHAVING kWh >= 900ORDER BY kWh DESC;\nThe results are now filtered after they  have been grouped together by the sum\naggregate:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 day \u2502 kWh \u2502\u2502 int32 \u2502 date \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 34 \u2502 2020-05-12 \u2502 960.03 \u2502\u2502 34 \u2502 2020-06-08 \u2502 935.33 \u2502\n\u2502 34 \u2502 2020-05-23 \u2502 924.08 \u2502\n\u2502 34 \u2502 2019-06-09 \u2502 915.4 \u2502\u2502 34 \u2502 2020-06-06 \u2502 914.98 \u2502\n\u2502 34 \u2502 2020-05-20 \u2502 912.65 \u2502\n\u2502 34 \u2502 2019-05-01 \u2502 912.6 \u2502\u2502 34 \u2502 2020-06-16 \u2502 911.93 \u2502\n\u2502 34 \u2502 2020-06-07 \u2502 911.73 \u2502\n\u2502 34 \u2502 2020-05-18 \u2502 907.98 \u2502\u2502 34 \u2502 2019-04-10 \u2502 907.63 \u2502\u2502 34 \u2502 2019-06-22 \u2502 906.78 \u2502\n\u2502 34 \u2502 2020-05-19 \u2502 906.4 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n4.6.2 Using the QUALIFY clause\nLet\u2019s say you want to only return rows wher e the result of a window function matches\nsome filter. You can\u2019t add that filter in the WHERE  clause because that would filter out\nrows that get included in the window, and you need to use the results of the window\nfunction. However, you also can\u2019t use HAVING  because window functions get evaluated\nbefore an aggregation. So QUALIFY  lets you filter on the re sults of a window function. \n When we introduced window functions, we had to use a CTE to filter the results.\nWe can rewrite the query much more simply and clearly by using QUALIFY , still getting\nthe three highest-ranked values.\nSELECT dense_rank() OVER (ORDER BY power DESC) AS rnk, *\nFROM readings\nQUALIFY rnk <= 3;\nLet\u2019s return to our example using a seven-da y moving window (see  listing 4.17). The\nseven-day average production value is a good indicator of the efficiency of a photovol-\ntaic grid, and we might ask for the days at  which a certain thre shold was reached. We\nonly want results for which the average in  a seven-day window was higher than 875\nkWh, as shown in the following listing, so it goes into the QUALIFY  clause. The\nQUALIFIY  clause can refer to the window function by name. Listing 4.22 Filter aggregated values in a window with the QUALIFY  clause81 4.6 Conditions and filtering outside the WHERE clause\n \nSELECT system_id,\nday,avg(kWh) OVER (\nPARTITION BY system_idORDER BY day ASCRANGE BETWEEN INTERVAL 3 Days PRECEDING\nAND INTERVAL 3 Days FOLLOWING\n) AS \"kWh 7-day moving average\"\nFROM v_power_per_dayQUALIFY \"kWh 7-day moving average\" > 875ORDER BY system_id, day;\nWith the example data, we find three date s that represent a typical \u201cgood day\u201d of\nphotovoltaic power production  in the wester n hemisphere:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 day \u2502 kWh 7-day moving average \u2502\n\u2502 int32 \u2502 date \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 34 \u2502 2020-05-21 \u2502 887.4628571428572 \u2502\n\u2502 34 \u2502 2020-05-22 \u2502 884.7342857142858 \u2502\u2502 34 \u2502 2020-06-09 \u2502 882.4628571428572 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n4.6.3 Using the FILTER clause\nSometimes, you want to comput e an aggregate, an average,  or a count of values, and\nyou realize there are some rows you don\u2019t want to include. You could add to the FILTER\nclause, but in a complex quer y, you might need to keep those rows to compute other\nfields. For example, let\u2019s say you sometimes get bad readings, which show up as negative\nvalues. You want to compute the total numb er of readings and the average reading of\nthe sensor. If you were to filt er out the bad readings in the WHERE  clause, you wouldn\u2019t\nbe able to compute the total number of readings. But if you were to simply average allthe readings, you would be including some of the bad, negative values. To solve this type\nof problem, you can use \nFILTER  expressions as part of the aggregation. \n Returning to section 4.1, in which we ha d to deal with inconsistent sensor read-\nings, we are actually presented with the very problem of pulling NULL  values into the\naverage, which is most likely not what we want. Instead of capping NULL  values at zero,\nwe can filter them out of the av erage value altogether like this.\nINSERT INTO readings(system_id, read_on, power)\nSELECT any_value(SiteId),\ntime_bucket(\nINTERVAL '15 Minutes',\nCAST(\"Date-Time\" AS timestamp)\n) AS read_on,Listing 4.23 Using the QUALIFY  clause on windows spawning more than one row\nListing 4.24 Keeping nonsensical data out of the aggregatesHere\u2019s where we \nset the threshold.82 CHAPTER  4Advanced aggregation and analysis of data\ncoalesce(avg(ac_power)\nFILTER (\nac_power IS NOT NULL ANDac_power >= 0\n),0 )\nFROM\nread_csv_auto(\n'https://developer.nrel.gov/api/pvdaq/v3/' ||\n'data_file?api_key=DEMO_KEY&system_id=10&year=2019'\n)\nGROUP BY read_on\nORDER BY read_onON CONFLICT DO NOTHING;\nYou might wonder why we use the coalesce  function; if all data is filtered out, noth-\ning goes into the aggregate, and the whole expression turns to NULL . That means that\nif you filter out all the input from  the aggregate, the value turns to NULL , which would\nviolate the constraint on our reading table. As usual, there is no one correct approach\nhere\u2014it\u2019s OK to prefer the solution in either  listing 4.1 or 4.24. In this case, using the\nFILTER -based solution combined with coalesce  is a slightly better option because its\nintention is slightly clearer. \n4.7 The PIVOT statement\nYou can have many aggregates in one query, and all of them can be filtered individu-\nally. This may help you answer a task like this: I want a report of the energy production per\nsystem and year, and the years should be columns!  Aggregating the production per system is\neasy, and so is aggregating the production per year. Grouping by both keys isn\u2019t diffi-cult either\u2014a statement like \nSELECT system_id, year(day), sum(kWh) FROM v_power_per_day GROUP BY ALL ORDER\nBY system_id;\nwill do just fine and returns the following:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 system_id \u2502 year(\"day\") \u2502 sum(kWh) \u2502\n\u2502 int32 \u2502 int64 \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 \u2502 2019 \u2502 1549.280000000001 \u2502\n\u2502 10 \u2502 2020 \u2502 677.1900000000003 \u2502\n\u2502 34 \u2502 2019 \u2502 205742.59999999992 \u2502\u2502 34 \u2502 2020 \u2502 101033.75000000001 \u2502\n\u2502 1200 \u2502 2019 \u2502 62012.109999999986 \u2502\n\u2502 1200 \u2502 2020 \u2502 30709.329999999998 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWhile we did group the data by system and year, the years per system appear in rows,\nnot columns. We want three rows with two columns, 2019 and 2020, containing the\nvalues, pretty much as you would find th e preceding data in a spreadsheet program.\nThe process of reorganizing such a table is called pivoting , and DuckDB offers a fewValues that are NULL or less \nthan zero are not included \nin the average anymore.\nWe use the ON CONFLICT clause to avoid \nfailures if you run this statement on a \ndatabase that is already populated.83 4.7 The PIVOT statement\nways to achieve this\u2014one of which is the us e of multiple filtered aggregates. Instead of\nhaving only one sum aggregate, we define  several and filter out each value we don\u2019t\nwant for a specific column. We then end up with the following statement. \nSELECT system_id,\nsum(kWh) FILTER (WHERE year(day) = 2019)\nAS 'kWh in 2019',\nsum(kWh) FILTER (WHERE year(day) = 2020)\nAS 'kWh in 2020'\nFROM v_power_per_day\nGROUP BY system_id;\nThe values for the sum are equal, but the ye ars are now columns, rather than individ-\nual groups. Now your fictitious boss can view that data in the way they are used to in\ntheir spreadsheet program:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 kWh in 2019 \u2502 kWh in 2020 \u2502\n\u2502 int32 \u2502 double \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 \u2502 1549.280000000001 \u2502 677.1900000000003 \u2502\n\u2502 34 \u2502 205742.59999999992 \u2502 101033.75000000001 \u2502\n\u2502 1200 \u2502 62012.109999999986 \u2502 30709.329999999998 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThere\u2019s one downside to this approach: th e columns are essentia lly hardcoded, and\nyou need to revisit that query every time a year gets added. If you are sure that your\ndesired set of columns is cons tant, or you find yourself targeting other databases that\nmight not support any other form of pivoting , the static approach might be the right\nsolution for you.\n To solve this problem with DuckDB, use the PIVOT  clause, shown in the following\nlisting, instead. DuckDB\u2019s PIVOT  clause allows for dynamically pivoting tables on arbi-\ntrary expressions. \nPIVOT (FROM v_power_per_day)\nON year(day)\nUSING sum(kWh);\nThe preceding statement is visualized in fi gure 4.4. We see all the steps from listing\n4.26: selecting all columns, then using the ON clause to turn all distinct years into col-\numns and the sum aggregate for computing their va lue. The steps are numbered in\nthe same order as in the listing.Listing 4.25 Statically pivoting a result by applying a filter to all aggregates selected\nListing 4.26 Using DuckDB\u2019s PIVOT  statement\nYou can omit the FROM if you want to select all \ncolumns, but we include d it to demonstrate \nthat this can actually be a full SELECT.\nAll distinct values from this \nexpression ar e turned into columns. The aggregate to be \ncomputed for the columns84 CHAPTER  4Advanced aggregation and analysis of data\nFigure 4.4 Pivoting the power values on the year\nThe result produced by the simplified, dynamic statement matches exactly what we\nstatically constructed in listing 4.25\u2014 year s as columns and systems as rows, with the\nsum of the power produced by a system and year forming the intersection of rows and\ncolumns:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 2019 \u2502 2020 \u2502\n\u2502 int32 \u2502 double \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 \u2502 1549.280000000001 \u2502 677.1900000000003 \u2502\n\u2502 34 \u2502 205742.59999999992 \u2502 101033.75000000001 \u2502\u2502 1200 \u2502 62012.109999999986 \u2502 30709.329999999998 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIn case you are using an aggr egate for the cell values, all columns that are not part of\nthe ON clause will be used as a grouping ke y for the aggregate. However, you do not\nneed to use an aggregate. PIVOT  v_power_per_day  ON day will produce a result of\n1,382 rows and 545(!) columns. Why is that? v_power_per_day  contains 1,382 distinct\nvalues of (system_id,  kWh) , which make up the rows. The system has been asked tosystem_id SELECT year(\"day\") sum (kWh)\n10\n102019 1549.280000000001\n1549.28000000000110 2020 677.1900000000003\n677.190000000000334\n342019 205742.59999999992\n205742.5999999999234 2020 101033.75000000001\n101033.750000000011200 2019 62012.109999999986\n62012.1099999999861200\n12002020 30709.329999999998\n30709.329999999998system_id 2019 2020PIVOT (FROM v_power_per_day)\nON year (day)FROM v_power_per_day)\nGROUP BY ALL\nUSING sum (kWh)(2)\n(3)(1)85 4.7 The PIVOT statement\ncreate a column using the day, not year(day) , and there are 543 different days\nrecorded. The two additi onal columns are the system_id  and kWh columns. So what\u2019s\nin the cells? There are many, many 0s and just a few 1s. Without the USING  clause,\nDuckDB will fill the cells with 0s for days th at didn\u2019t have the specific value and 1s for\ndays that did. So if you are actually interested in a tabula r view of all days, you might\nwant to use the first  aggregate like this in such a case:\nPIVOT (\nFROM v_power_per_day WHERE day BETWEEN '2020-05-30' AND '2020-06-02'\n)ON DAY USING first(kWh);\nNote that we deliberately chose to select on ly a c ou pl e of  days  ins t ead  o f  t ryi ng to\nprint several hundred columns.  The preceding query pivots this result into a tabular\nview on the day:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 day \u2502 kWh \u2502\u2502 int32 \u2502 date \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1200 \u2502 2020-05-30 \u2502 280.4 \u2502\n\u2502 1200 \u2502 2020-05-31 \u2502 282.25 \u2502\u2502 1200 \u2502 2020-06-01 \u2502 288.29 \u2502\n\u2502 1200 \u2502 2020-06-02 \u2502 152.83 \u2502\n\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\n\u2502 \u00b7 \u2502\u00b7 \u2502\u00b7 \u2502\n\u2502 10 \u2502 2020-05-30 \u2502 4.24 \u2502\u2502 10 \u2502 2020-05-31 \u2502 3.78 \u2502\n\u2502 10 \u2502 2020-06-01 \u2502 4.47 \u2502\n\u2502 10 \u2502 2020-06-02 \u2502 5.09 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows (8 shown) 3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThis result would make an y spreadsheet artist happy:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 2020-05-30 \u2502 2020-05-31 \u2502 2020-06-01 \u2502 2020-06-02 \u2502\u2502 int32 \u2502 double \u2502 double \u2502 double \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 \u2502 4.24 \u2502 3.78 \u2502 4.47 \u2502 5.09 \u2502\u2502 34 \u2502 732.5 \u2502 790.33 \u2502 796.55 \u2502 629.17 \u2502\n\u2502 1200 \u2502 280.4 \u2502 282.25 \u2502 288.29 \u2502 152.83 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAll of the preceding queries use the proprietary DuckDB variant of PIVOT . DuckDB\u2019s\nsyntax makes writing pivot statements much  easier and less error-prone, as it com-\npletely eliminates any static enumeration of the rows on which the table should be piv-\noted. DuckDB also supports a more standard SQL form of PIVOT . However, the\nsupport for the PIVOT  clause wildly differs across diffe rent databases systems, and it is86 CHAPTER  4Advanced aggregation and analysis of data\nunlikely other possible target databases will have the exact same flavor of the stan-\ndard. Therefore, we would rather use the prop rietary syntax in this case, which is eas-\nier to read than hoping for more portable SQL. \n In DuckDB, it is perfec tly possible to compute multiple aggregates in the USING\nclause as well as use multiple columns for pi voting. We could use this to not only com-\npute the total production per year (which is the sum of all days) but also add two more\ncolumns that highlight the best day:\nPIVOT v_power_per_day\nON year(day)\nUSING round(sum(kWh)) AS total, max(kWh) AS best_day;\nWe\u2019ve rounded the totals so that the result is more readable:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 system_id \u2502 2019_total \u2502 2019_best_day \u2502 2020_total \u2502 2020_best_day \u2502\u2502 int32 \u2502 double \u2502 double \u2502 double \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 \u2502 1549.0 \u2502 7.47 \u2502 677.0 \u2502 6.97 \u2502\u2502 34 \u2502 205743.0 \u2502 915.4 \u2502 101034.0 \u2502 960.03 \u2502\u2502 1200 \u2502 62012.0 \u2502 337.29 \u2502 30709.0 \u2502 343.43 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n4.8 Using the ASOF JOIN\nImagine you are selling a volatile product at arbitrary times of day.  You are able to pre-\ndict prices at an interval, let\u2019s say 15 minute s, but that\u2019s as precise as you can get. How-\never, people demand your product all the time, which leads to the following fictive\nsituation. The query in listing 4.27 generates two CTEs: a fictive price table with 4entries for an hour of a random day as well as  a sales table with 12 entries. It then joins\nthem together naively, and instead of the pric es of 12 sales, you fi nd only 4 results, as\nshown in the following listing. \nWITH prices AS (\nSELECT range AS valid_at,\nrandom()*10 AS price\nFROM range(\n'2023-01-01 01:00:00'::timestamp,\n'2023-01-01 02:00:00'::timestamp, INTERVAL '15 minutes')\n),sales AS (\nSELECT range AS sold_at,\nrandom()*10 AS num\nFROM range(\n'2023-01-01 01:00:00'::timestamp,\n'2023-01-01 02:00:00'::timestamp, INTERVAL '5 minutes')\n)\nSELECT sold_at, valid_at AS 'with_price_at', round(num * price,2) as price\nFROM salesJOIN prices ON prices.valid_at = sales.sold_at;Listing 4.27 Using an inner join for timestamps87 4.8 Using the ASOF JOIN\nFigure 4.5 The inner join of time-series data gone wrong\nSales are quite poor, as clearly indicated by this result and represented in figure 4.5:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sold_at \u2502 with_price_at \u2502 price \u2502\u2502 timestamp \u2502 timestamp \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023-01-01 01:00:00 \u2502 2023-01-01 01:00:00 \u2502 21.17 \u2502\u2502 2023-01-01 01:15:00 \u2502 2023-01-01 01:15:00 \u2502 12.97 \u2502\n\u2502 2023-01-01 01:30:00 \u2502 2023-01-01 01:30:00 \u2502 44.61 \u2502\n\u2502 2023-01-01 01:45:00 \u2502 2023-01-01 01:45:00 \u2502 9.45 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nEnter the ASOF JOIN \u2014the ASOF  (pronounced as of) JOIN  is a JOIN clause that joins on\ninequality, picking a \u201cgood enough\u201d value for the gaps where the JOIN  columns are\nnot exactly equal. Returning to listing 4.27 , we must change two things: replacing the\nJOIN  keyword with ASOF JOIN  and providing an inequality operator. The following \nprices.valid_at <= sales.sold_at\ninequality condition indicates that all prices valid at or before the point of sale can be\nused to compute the total price.p1:00\np1:15\np1:30\np1:45v1:00\nv1:15\nv1:30\nv1:45v1:05\nv1:10\nv1:20\nv1:25\nv1:35\nv1:40\nv1:50\nv1:55INNER JOIN =p1:00\np1:15\np1:30\np1:45v1:00\nv1:15\nv1:30\nv1:4588 CHAPTER  4Advanced aggregation and analysis of data\n \nWITH prices AS (\nSELECT range AS valid_at,\nrandom()*10 AS price\nFROM range(\n'2023-01-01 01:00:00'::timestamp,\n'2023-01-01 02:00:00'::timestamp, INTERVAL '15 minutes')\n),\nsales AS (\nSELECT range AS sold_at,\nrandom()*10 AS num\nFROM range(\n'2023-01-01 01:00:00'::timestamp,'2023-01-01 02:00:00'::timestamp, INTERVAL '5 minutes')\n)\nSELECT sold_at, valid_at AS 'with_price_at', round(num * price,2) as priceFROM sales\nASOF JOIN prices\nON prices.valid_at <= sales.sold_at;\nNote how DuckDB picks the price closest to th at at the time of the sale. Additionally,\nwe now get the 12 expected rows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sold_at \u2502 with_price_at \u2502 price \u2502\n\u2502 timestamp \u2502 timestamp \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023-01-01 01:00:00 \u2502 2023-01-01 01:00:00 \u2502 1.59 \u2502\n\u2502 2023-01-01 01:05:00 \u2502 2023-01-01 01:00:00 \u2502 3.56 \u2502\u2502 2023-01-01 01:10:00 \u2502 2023-01-01 01:00:00 \u2502 2.71 \u2502\n\u2502 2023-01-01 01:15:00 \u2502 2023-01-01 01:15:00 \u2502 29.12 \u2502\n\u2502 2023-01-01 01:20:00 \u2502 2023-01-01 01:15:00 \u2502 14.92 \u2502\u2502 2023-01-01 01:25:00 \u2502 2023-01-01 01:15:00 \u2502 4.83 \u2502\n\u2502 2023-01-01 01:30:00 \u2502 2023-01-01 01:30:00 \u2502 2.84 \u2502\n\u2502 2023-01-01 01:35:00 \u2502 2023-01-01 01:30:00 \u2502 3.84 \u2502\u2502 2023-01-01 01:40:00 \u2502 2023-01-01 01:30:00 \u2502 4.95 \u2502\n\u2502 2023-01-01 01:45:00 \u2502 2023-01-01 01:45:00 \u2502 23.1 \u2502\n\u2502 2023-01-01 01:50:00 \u2502 2023-01-01 01:45:00 \u2502 30.07 \u2502\u2502 2023-01-01 01:55:00 \u2502 2023-01-01 01:45:00 \u2502 11.6 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows 3 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nFigure 4.6 visualizes the algorithm: given four items p, each with a timestamp increas-\ning by 15 minutes each, and 12 items v, with a timestamp increa sing by five minutes.\nThe ASOF  JOIN  in the figure is defined as p<=v  s o  t h a t  e a c h  p item will be joined\ntogether with three v items that have the same or higher timestamp. \n The ASOF  JOIN  is often used to work with time series data, such as stock quotes,\np ric es ,  o r I o T s en s or s . I n o ur  ex am ple ,  it  can b e u s ed  to  j oi n the  ch anging  s el li ngprices with the readings from  the systems to compute the prices at any given point inListing 4.28 Using an ASOF JOIN  for timestamps\nSpecify the\nJOIN as ASOF .\nNote the <=, in contrast \nto the = in listing 4.27.89 4.8 Using the ASOF JOIN\ntime. The final example, which follows, us es our photovoltaic example data again,\napplying the same logic to pick a valid price. It then demonstrates that the ASOF  JOIN\ncan be used with other constructs we learne d in this chapter, such as using a window\nto accumulate the running total earnings in  a sales period with different prices, as\nshown in the following listing.\nSELECT power.day,\npower.kWh,prices.value as 'ct/kWh',\nround(sum(prices.value * power.kWh)\nOVER (ORDER BY power.day ASC) / 100, 2)AS 'Accumulated earnings in EUR'\nFROM v_power_per_day power\nASOF JOIN pricesON prices.valid_from <= power.day\nWHERE system_id = 34\nORDER BY day;Listing 4.29 Computing a running total earning using ASOF  JOIN  and a window functionp1:00\np1:15\np1:30\np1:45v1:00\nv1:15\nv1:30\nv1:45v1:05\nv1:10\nv1:20\nv1:25\nv1:35\nv1:40\nv1:50\nv1:55p1:00\np1:15\np1:30\np1:45v1:00\nv1:15\nv1:30\nv1:45v1:05\nv1:10\nv1:20\nv1:25\nv1:35\nv1:40\nv1:50\nv1:55ASOF JOIN\np <= v=\nFigure 4.6 Using ASOF JOIN  to join all timestamps that don\u2019t have an exact match90 CHAPTER  4Advanced aggregation and analysis of data\nThe result shows the day, the amount of kWH produced, the price on that day (in\nct/kWH), and the accumulated sum of the product of the amount of power produced\nand the price:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 day \u2502 kWh \u2502 ct/kWh \u2502 Accumulated earnings in EUR \u2502\n\u2502 date \u2502 double \u2502 decimal(5,2) \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2019-01-01 \u2502 471.4 \u2502 11.47 \u2502 54.07 \u2502\n\u2502 2019-01-02 \u2502 458.58 \u2502 11.47 \u2502 106.67 \u2502\u2502 2019-01-03 \u2502 443.65 \u2502 11.47 \u2502 157.56 \u2502\n\u2502 2019-01-04 \u2502 445.03 \u2502 11.47 \u2502 208.6 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502\u2502\u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502\n\u2502 2020-06-23 \u2502 798.85 \u2502 9.17 \u2502 31371.86 \u2502\u2502 2020-06-24 \u2502 741.15 \u2502 9.17 \u2502 31439.83 \u2502\n\u2502 2020-06-25 \u2502 762.6 \u2502 9.17 \u2502 31509.76 \u2502\n\u2502 2020-06-26 \u2502 11.98 \u2502 9.17 \u2502 31510.86 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 543 rows (8 shown) 4 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nDuckDB is positioned as an OLAP database with a broad range of use cases. Dealing\nwith time-series data is ce rtainly one of them, and the ASOF  JOIN  i s  p a r t  o f  t h a t .\nRegardless of the domain\u2014which can take the form of anything from the sensor read-\nings in our example to the readings of a pa tient\u2019s heart rate monitor to fluctuations in\nthe stock market\u2014values recorded at a cert ain time are often enriched by joining\nthem with specific key values that have been valid for a time. Having support for ASOF\nenables all scenarios in which timest amps are not aligne d perfectly well. \n4.9 Using table functions\nMost functions in SQL take parameters and return a single value. Table functions , on\nthe other hand, don\u2019t just return a single value\u2014they return a collection of rows. As\nsuch, they can appear anywhere a table ca n appear. Depending on their function, they\ncan access external resources, such as files or URLs, and turn them into relations that\nare part of standard SQL statements. DuckDB  is not the only relational database sup-\nporting the concept of table-pr oducing functions, but it co mes with an impressive set\nof table functions, catering to many use cases. A list of all ta ble functions in your\nDuckDB installation can be retrieved via th e following statement, which uses a table\nfunction named duckdb_functions() . \nSELECT DISTINCT ON(function_name) function_name\nFROM duckdb_functions()\nWHERE function_type = 'table'ORDER BY function_name;Listing 4.30 Getting a list of all available table functions\nThe FROM clause is the most \ncommon place to call a \ntable-producing function.91 4.9 Using table functions\nIn this chapter\u2019s examples and during th e ingestion of data, we have already made\nextensive use of read_csv* , read_parquet , and others. Additional extensions, such as\nthe spatial extension, can be added to the list of table functions that read external\nresources and produce relational data. \n range(start,  stop)  and generate_series(start,  stop)  are a couple of very use-\nful table functions. Both fu nctions create a list of values in the range between start\nand stop . The start  parameter is inclusive. For the range  function, the stop  parame-\nter is exclusive, while it is inclusive for generate_series . Both functions provide over-\nloads, with an additi onal third parameter step  defining the step size, which defaults\nto 1. Variants that only take the stop  parameter and default at 0 for start  exist too.\nWhile used as normal functions, they pr ovide useful constructs but are much more\npowerful when queried like a table.\n If you need a list of numbers between 1 and 5 and don\u2019t want to hardcode them,\nyou can use SELECT  generate_series(1,  5);. Numbers are helpfu l, but those func-\ntions also work with temporal data. When using temporal data, be aware, though, that\nyou need to specify both start  and end parameters, as there is no sensible default for\neither. Let\u2019s put this to practical use. Th e readings in our example data end in the\nmiddle of 2020. Reports based on this would end prematurely if they were intended\nfor a whole year, as shown in the following snippet:\nSELECT strftime(day, '%Y-%m') AS month, avg(kwh)\nFROM v_power_per_day WHERE year(day) = 2020\nGROUP BY ALL ORDER BY month;\nThe result will look like the following output:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 month \u2502 avg(kwh) \u2502\u2502 varchar \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2020-01 \u2502 222.13169014084497 \u2502\u2502 2020-02 \u2502 133.52356321839076 \u2502\n\u2502 2020-03 \u2502 207.86670454545438 \u2502\n\u2502 2020-04 \u2502 309.7838888888888 \u2502\u2502 2020-05 \u2502 349.5753763440861 \u2502\n\u2502 2020-06 \u2502 337.80820512820515 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIf you are tasked to create a chart, you migh t find yourself in a situation in which you\nneed to think about how to represent the future months. Here\u2019s one way to use the\nrange()  function to cover a whole year an d indicate missing values with 0s. \nWITH full_year AS (\nSELECT generate_series AS dayFROM generate_series(\n'2020-01-01'::date,Listing 4.31 Using a range of dates as a driving table\nA range defined from the first up to the last \nday of the year, in an interval of 1 day92 CHAPTER  4Advanced aggregation and analysis of data\n'2020-12-31'::date, INTERVAL '1 day')\n)\nSELECT strftime(full_year.day, '%Y-%m') AS month,\navg(kWh) FILTER (kWh IS NOT NULL) AS actual\nFROM full_year\nLEFT OUTER JOIN v_power_per_day per_day\nON per_day.day = full_year.day\nGROUP BY ALL ORDER BY month;\nThe result is now a report for a full year, wh ich, sadly, lacks values after June 2020:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 month \u2502 actual \u2502\n\u2502 varchar \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2020-01 \u2502 222.13169014084508 \u2502\n\u2502 2020-02 \u2502 133.52356321839076 \u2502\u2502 2020-03 \u2502 207.86670454545455 \u2502\n\u2502 2020-04 \u2502 309.7838888888888 \u2502\n\u2502 2020-05 \u2502 349.57537634408607 \u2502\u2502 2020-06 \u2502 337.80820512820515 \u2502\u2502 2020-07 \u2502 \u2502\n\u2502 2020-08 \u2502 \u2502\n\u2502 2020-09 \u2502 \u2502\u2502 2020-10 \u2502 \u2502\n\u2502 2020-11 \u2502 \u2502\n\u2502 2020-12 \u2502 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTaking this idea one step further, you co uld use the value of the same month in the\nprevious year to forecast the production value. To do that, you would have to join\nv_power_per_day  a second time, using an offset of one year, as shown in the following\nlisting.\nWITH full_year AS (\nSELECT generate_series AS day\nFROM generate_series(\n'2020-01-01'::date,'2020-12-31'::date, INTERVAL '1 day')\n)\nSELECT strftime(full_year.day, '%Y-%m') AS month,\nround(avg(present.kWh) FILTER (present.kWh IS NOT NULL),3) AS actual,\nround(avg(past.kWh) FILTER (past.kWh IS NOT NULL), 3) AS forecast,\nFROM full_yearLEFT OUTER JOIN v_power_per_day present\nON present.day = full_year.day\nLEFT OUTER JOIN v_power_per_day past\nON past.day = full_year.day - INTERVAL '1 year'\nGROUP BY ALL ORDER BY month;Listing 4.32 Projecting past data into the futureUse the output of the table function \nin the FROM clause as a driving table.\nOUTER JOIN the \nvalues of interest.\nUsing the generated\nseries as a driving table\nin the FROM clause\nJoining power per day a second \ntime but subtracting a year from \nthe values of the generated series93 4.10 Using LATERAL joins\nNote that we also added a call to round  to both the actual  and the forecast  column,\navoiding clutter, as fractions with more th an three digits don\u2019t make much sense for\nkWh values. Additionally, th is change shows that the FILTER  clause can also appear\ninside a function call, as it belongs to the avg aggregate function, not the whole col-\numn. The result is much more pleasa nt and happens to provide a comparison\nbetween this year and last year, essentially for free:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 month \u2502 actual \u2502 forecast \u2502\n\u2502 varchar \u2502 double \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2020-01 \u2502 222.132 \u2502 161.593 \u2502\n\u2502 2020-02 \u2502 133.524 \u2502 111.073 \u2502\u2502 2020-03 \u2502 207.867 \u2502 150.652 \u2502\n\u2502 2020-04 \u2502 309.784 \u2502 316.178 \u2502\n\u2502 2020-05 \u2502 349.575 \u2502 325.369 \u2502\u2502 2020-06 \u2502 337.808 \u2502 351.607 \u2502\n\u2502 2020-07 \u2502 \u2502 334.323 \u2502\n\u2502 2020-08 \u2502 \u2502 314.929 \u2502\u2502 2020-09 \u2502 \u2502 289.605 \u2502\u2502 2020-10 \u2502 \u2502 253.829 \u2502\n\u2502 2020-11 \u2502 \u2502 191.384 \u2502\n\u2502 2020-12 \u2502 \u2502 164.886 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 12 rows 3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n4.10 Using LATERAL joins\nIn section 4.3, we learned about correlated and uncorrela ted subqueries. Listing 4.5\ndemonstrated how an uncorrel ated subquery can be joined  once with the outer query.\nFrom a performance perspective, that might be beneficial, as the subquery only needs\nto be evaluated once and the join is then  performed for each row of the other table\nagainst the memorized values. \n Sometimes, however, you want to evaluate  precisely the inner query for each value\nof an outer query. This is where the LATERAL JOIN  comes into play. You can think of it\nas the inner block of a for loop with the outer query being the control structure.\n Unnesting arrays, fanning out data, and similar tasks can be dealt with using\nLATERAL . Assume you are interested in the inte nsity of the sun, specifically how much\nof its energy reaches your place at certai n hours of the day, past or future. Open\nMeteo ( https:/ /open-meteo.com ) offers a free API that provides a broad range of\nweather data, including the so-called global horizontal irradiance  (GHI)\u2014that is, the\ntotal amount of short-wave radiation receiv ed from above by a surface horizontal to\nthe ground. This value is of particular intere st to photovoltaic installations and is mea-\nsured in W/m\u00b2. As shown in the following listing, their API generates a JSON object\nthat contains two individual arrays, one wi th the timestamps and another with the\nselected values. The latter array is the array of  interest; we want to retrieve specific val-\nues for some given facts. 94 CHAPTER  4Advanced aggregation and analysis of data\n \n{\n\"latitude\": 50.78,\"longitude\": 6.0799994,\"utc_offset_seconds\": 7200,\"timezone\": \"Europe/Berlin\",\"timezone_abbreviation\": \"CEST\",\"elevation\": 178.0,\"hourly_units\": {\n\"time\": \"iso8601\",\"shortwave_radiation_instant\": \"W/m\\u00b2\"\n},\"hourly\": {\n\"time\": [\n\"2023-08-26T00:00\",\"2023-08-26T01:00\",\"2023-08-26T02:00\",\"2023-08-26T03:00\",\"2023-08-26T04:00\",\"2023-08-26T05:00\"\n],\"shortwave_radiation_instant\": [\n0.0,0.0,0.0,0.0,0.0,9.1\n]\n}\n}\nThe preceding JSON is in the code repository of the book under ch04/ghi_past_and_\nfuture.json. Alternatively, you can acce ss fresh data via Open-Meteo\u2019s API: https:/ /\nmng.bz/WE5w .\n At first sight, it might seem like a daunti ng task to use SQL to pick out the morning\nhours, noon, and the evening hours from that array. Let\u2019s see how LATERAL  can solve\nthis task. We already read in chapter 1 th at DuckDB is able to  process JSON, and we\nwill examine this in greater detail in chapter 5. For now, it is enough to know that you\ncan select from a JSON file like from any other table in the FROM  clause. The following\nquery generates a series of seven days and then joins those with the hours 8, 13, and\n19 (7 p.m.) to create indexes. Those inde xes are the day number multiplied by 24 plus\nthe hour of day at which you desire to find the value in the JSON array. That index is\nthe lateral driver for the subquery:\nINSTALL json;\nLOAD json;\nWITH days AS (\nSELECT generate_series AS value FROM generate_series(7)Listing 4.33 Excerpt of a JSON response from Open Meteo containing GHI data95 4.10 Using LATERAL joins\n), hours AS (\nSELECT unnest([8, 13, 18]) AS value\n), indexes AS (\nSELECT days.value * 24 + hours.value AS i\nFROM days, hours\n)SELECT date_trunc('day', now()) - INTERVAL '7 days' +\nINTERVAL (indexes.i || ' hours') AS ts,\nghi.v AS 'GHI in W/m^2'\nFROM indexes,\nLATERAL (\nSELECT hourly.shortwave_radiation_instant[i+1]\nAS v\nFROM 'code/ch04/ghi_past_and_future.json'\n)A Sg h iORDER BY ts;\nThe end of August 2023 did look like this  in Aachen; it was not a great month for\nphotovoltaics:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ts \u2502 GHI in W/m^2 \u2502\u2502 timestamp with time zone \u2502 double \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023-08-26 08:00:00+02 \u2502 36.0 \u2502\n\u2502 2023-08-26 13:00:00+02 \u2502 490.7 \u2502\u2502 2023-08-26 18:00:00+02 \u2502 2.3 \u2502\n\u2502 2023-08-27 08:00:00+02 \u2502 243.4 \u2502\n\u2502 2023-08-27 13:00:00+02 \u2502 124.3 \u2502\u2502\u00b7 \u2502 \u00b7 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502\u2502 2023-09-01 13:00:00+02 \u2502 392.0 \u2502\n\u2502 2023-09-01 18:00:00+02 \u2502 0.0 \u2502\n\u2502 2023-09-02 08:00:00+02 \u2502 451.0 \u2502\u2502 2023-09-02 13:00:00+02 \u2502 265.0 \u2502\n\u2502 2023-09-02 18:00:00+02 \u2502 0.0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 24 rows (10 shown) 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe subquery can produce zero, one, or a la rger number of rows for each row of the\ndriving outer table. In the preceding exampl e, it produced one row for each outer row.\nIf the subquery produces more rows, the valu es of the outer row will be repeated, in a\nmanner similar to a CROSS JOIN . If the subquery does not produce any value, the join\nwon\u2019t produce a value either. We must apply an OUTER JOIN  in this case as well. At this\npoint, the LATERAL  keyword alone is not enough , and we must use the full JOIN  syntax\nlike this. The following query is artificial and of little value, except in demonstrating the\nsyntax. Both queries produce a series of values  from 1 to 4, with the outer in a step size\nof 1 and the inner in a step size of 2. We compare both values in the ON clause:\nSELECT i, j\nFROM generate_series(1, 4) t(i)Recreates data from \nthe hourly index\nArrays are 1 based in \nDuckDB (and SQL in general).\nDuckDB automatically detects that \nthis string refers to a JSON file,  \nloads it, and then parses it.96 CHAPTER  4Advanced aggregation and analysis of data\nLEFT OUTER JOIN LATERAL (\nSELECT * FROM generate_series(1, 4, 2) t(j)\n)s qO Ns q . j=i\nORDER BY i;\nThe result of this query looks like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502i\u2502j\u2502\n\u2502 int64 \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021 \u25021 \u2502\n\u25022 \u2502 \u2502\u25023 \u25023 \u2502\n\u25024 \u2502 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe problem with the prices in section 4.8 can be solved with a subquery and LATERAL\nJOIN  too. In essence, the subq uery must return a row from  the price table that has a\nvalidity as close to the date in time of the sale as possible. For that to work, we cannot\nuse a normal JOIN , as the subquery must produce di fferent values for each incoming\ndate. Therefore, the date  column that would normally be part of the JOIN  must move\ninside the subquery. Thus, th e joined subquery now becomes correlated, or laterally\njoined to the outer query. The correlation in the following example is the validity of\nthe price compared to the day th e power production was recorded. \nSELECT power.day, power.kWh,\nprices.value as 'EUR/kWh'\nFROM v_power_per_day power,\nLATERAL (\nSELECT *\nFROM pricesWHERE prices.valid_from <= power.day\nORDER BY valid_from DESC limit 1\n) AS prices\nWHERE system_id = 34\nORDER BY day;\nFor time-series-related computations with DuckDB, we would most certainly use the\nASOF  JOIN . LATERAL  is attractive when considering portability, and there are probably\nmore databases supporting LATERAL  than ASOF  JOIN . Use LATERAL  in scenarios in\nwhich you want to fan out of a dataset to produce more rows. \n  Listing 4.34 Comparing the ASOF JOIN  from listing 4.29 to a LATERAL JOINWhile the condition is now on the outside \nand cannot be formulated otherwise, it is \nstill a correlated subquery.\nMark the subquery as lateral, \nallowing correlation.\nCorrelate by \ninequality.\nWhile the ASOF JOIN would \nautomatically pick the closest value \nfor us, we must order the values \nourselves when using LATERAL.97 Summary\nSummary\n\uf0a1The SQL standard has evolved greatly si nce its last major revision in 1992\n(SQL-92). DuckDB supports a broad range of modern SQLs, including CTEs\n(SQL:1999), window functions (SQL:2003), list aggregations (SQL:2016), andmore.\n\uf0a1Grouping sets allow the computation of aggregates over multiple groups, per-\nforming a drill down into different levels of detail; ROLLUP  and CUBE  can be used\nto generate subgroups or comb inations of grouping keys.\n\uf0a1DuckDB fully supports window func tions, including named windows and\nranges, enabling use cases such as comp uting running totals, ranks, and more.\n\uf0a1All aggregate functions, including statis tic computations and interpolations, are\noptimized for usage in a windowed context.\n\uf0a1HAVING  and QUALIFY  can be used to select aggreg ates and windows after they have\nbeen computed; FILTER  prevents unwanted data from going into aggregates.\n\uf0a1DuckDB includes ASOF  JOIN , which is necessary in use cases involving time-\nseries data.\n\uf0a1DuckDB also supports LATERAL  joins that help fan out data and can emulate\nloops, to an extent.\n\uf0a1Results can be pivoted,  either with a simpli fied, DuckDB -specific PIVOT  state-\nment or a more static, standard SQL approach.98Exploring data\nwithout persistence\nIn this chapter, we\u2019re going to learn how to query data without persisting the data\nin DuckDB, a technique that is quite unus ual for a database and seems counterintu-\nitive, but which is useful in the right situ ations. For example, if we need to trans-\nform data from one format to another, we might not necessarily want to create an\nintermediate storage model while doing this.\n This chapter also demonstrates the po wer of DuckDB\u2019s analytical engine, even\nwhen your data isn\u2019t stored in the native  format. We\u2019ll show how to query several\ncommon data formats, including JSON, CSV, and Parquet, as well as other data-\nbases, such as SQLite.This chapter covers \n\uf0a1Converting CSV files to Parquet files\n\uf0a1Auto-inferring file type and data schema\n\uf0a1Creating views to simplify the querying of nested \nJSON documents\n\uf0a1Exploring the metadata of Parquet files\n\uf0a1Querying other databases, such as SQLite99 5.2 Inferring file type and schema\n The JSON and CSV sources we are working with in this chapter are located in the\nch05 folder of our example repository on GitHub: https:/ /github.com/duckdb-in\n-action/examples . We assume you have navigated to the root of this repository before\ninvoking the DuckDB CLI for the examples in this chapter.\n5.1 Why use a database with out persisting any data?\nExploring and analyzing data without pers isting makes sense when we are working\nwith data stored in a remote location. For example, you might have files living in Ama-\nzon S3. We don\u2019t know yet whether we want  to build a production pipeline with this\ndata, so we don\u2019t want to spend a bunch of time defining a data model, like we did in\nchapter 3, and ingest the remote data into  DuckDB\u2019s storage format. Or it might con-\ntain data that we don\u2019t want to persist for privacy reasons. However, we still want to uti-\nlize what we learned about Du ckDB and SQL in the previous chapters so that we can\nuse it to understand the shape and volume of the data. Depending on the file format\nand storage location, DuckDB may not even need to download the entirety of the file\ncontents. At a later stage, we may then choose to ingest the data into DuckDB. \n Most likely, you already have some kind of databa se in your infrastructure. DuckDB\nis able to use the store systems of several other databases. Most notably, there are the\nSQLite and Postgres integrations. The former  works directly on the SQLite store files,\nand the latter works with the binary transfer mode  of the Postgres client\u2013server protocol.\nIn either case, you don\u2019t have the data insi de your DuckDB process but are still able to\ntake advantage of DuckDB\u2019s fast query engi ne and SQL support. In  many cases, for 22\nTPC-H benchmark queries, DuckDB had been faster when using the Postgres integra-\ntion than Postgres itself (and faster in  all queries when using its own storage). \nNOTE What is the TPC-H benchmark ? It is a decision support benchmark, con-\nsisting of a suite of bu siness-oriented ad hoc qu eries and concurrent data\nmodifications. The benchmark uses a typi cal star schema for sales order, with\nsales and line items as fact tables and a couple of dimension tables, such as\nproducts and customers. This benchmar k illustrates decisi on support systems\nthat examine large volumes of data, ex ecute queries with a high degree of\ncomplexity, and give answer s to critical bu siness questions. TPC-H is the most\ncommonly used benchmark fo r analytics, although ma ny people only run the\nread queries, not the updates, which are also part of the specification. \n5.2 Inferring file type and schema\nDuckDB has two features that make it very easy  to process files or se ts of files. It can\ndetermine both what kind of file is  being read (also referred to as auto-inferring file\ntypes) and the schema of the data in the file: \n\uf0a1Auto-inferring files types \u2014With DuckDB, you can query the content of supported\nfile formats, such as CSV, JSON, and Parquet, as simply as using100 CHAPTER  5Exploring data without persistence\nFROM  'flights.csv'; . The functionality is provided  out of the box, and it sup-\nports all SQL clauses and functions ment ioned in the previous chapters. When\nyou issue such a query, Duck DB works out first that you are not querying a table\nor view in your current schema. If the f ile exists in the filesystems, DuckDB uses\nits extension to determine the file type  and calls an appropriate function that\nknows how to process that data format to  read the file. Thos e functions provide\nsensible defaults for both behavior (e.g ., the number of samp les to take before\na data type is decided) and formats (e .g., date and time formats). If those\ndefaults don\u2019t work for you, l ook out for the table functions read_csv_auto ,\nread_json_auto , and read_parquet_auto . These functions are called internally\nwhen you issue a query like the preceding one. They provide a plethora of argu-\nments to change singular details but st ill automatically de rive column names,\nobject structures, and so on. So  instead of just querying FROM  'a_file.csv'  or\nFROM  'data*.json' , you would use FROM  read_json_auto('data*.json')  with\nthe appropriate arguments you need.\n\uf0a1Auto-inferring schema \u2014DuckDB automatically infers the schema of any data\nsources we ask it to process. With data form ats like Parquet, this is easier, as files\nhave an embedded schema that DuckDB can use. With others, like CSV or JSON,\nit will infer the schema from a configur able number of samp le objects. DuckDB\nalso infers the dialect of CSV files and detects whether they contain a header row.\nIf we aren\u2019t completely happy with the inferences made, we can choose to over-\nride the types of all, or just some, of the columns. In such a case, you would not\nuse a plain filename but instead use a \u201cnon-auto\u201d function, like read_csv :\nFROM read_csv(\n'flights.csv',auto_detect=true,\ncolumns={\n'FlightDate': 'DATE','UniqueCarrier': 'VARCHAR',\n'OriginCityName': 'VARCHAR',\n'DestCityName': 'VARCHAR'\n}\n);\nDuckDB can read multiple files of different types (CSV, Parquet, JSON files, etc.) at\nthe same time, using either the glob syntax or by providing a list of files to read. Whenreading from multiple files,  DuckDB must combine schemas from those files. That is\nbecause each file may have its own schema , which can differ from the other files.\nDuckDB offers two ways of un ifying schemas of multiple files: by column position and\ncolumn name. By default, DuckDB reads th e schema of the first file provided, and\nthen it unifies columns in subsequent files by column position. Th is works correctly as\nlong as all files have the same schema, wi th the same names, at identical positions.\nOtherwise, you can use the \nunion_by_name  option for the read_xxx  functions, which\nallows DuckDB to construct the schema by reading all of the names instead. The file \nto read\nUse auto-detection \nfor all arguments.\nEnsure the listed columns are being \nconverted into the given data types.101 5.2 Inferring file type and schema\n5.2.1 A note on CSV parsing\nCSV parsing can be surprisingly difficult, even though the format is simple and, at first\nglance, straightforward. DuckDB uses  sampling in all cases in which read_csv_auto  or\nread_csv  with auto_detect  is set to true . A certain number of  rows from the file\n(20,480 by default) are read to detect the following:\n\uf0a1The dialect of the CSV file (delimiter, quoting rule, escape, etc.)\n\uf0a1The types of each of the columns\n\uf0a1Whether the file has a header row\nWe consider type detection to be the most  important factor in ensuring good data\nquality in any later step. DuckDB tries to determine the following types in descending\npriority:\n\uf0a1BOOLEAN\n\uf0a1BIGINT\n\uf0a1DOUBLE\n\uf0a1TIME\n\uf0a1DATE\n\uf0a1TIMESTAMP\n\uf0a1VARCHAR\nTo the end, everything can be cast to VARCHAR . This type has the lowest priority\u2014i.e.,\ncolumns are converted to VARCHAR  if they cannot be cast to anything else. \n There\u2019s a good chance you want or  need to control the behavior of read_csv_auto\nor read_csv . The DuckDB documentation lists th e most important arguments in a\ndedicated section ( https:/ /mng.bz/8w5B ). These parameters apply to the corre-\nsponding export functions as well.  Arguments we found helpful were names , to config-\nure the column names in absence of header rows; dateformat , timestampformat , and\ndecimal_separator  for date and number formats; and filename  when dealing with\nmultiple files at once (this op tion adds an artificial colu mn containing the filename of\nthe processed file). \n If you find yourself in a scenario in which you don\u2019t have access to the aforemen-\ntioned documentation, you ca n always query DuckDB to gi ve you a list of arguments\nto its functions:\nSELECT distinct function_name,\nunnest(parameters) as parameter\nFROM duckdb_functions()\nWHERE function_name = 'read_csv'\nORDER BY parameter;\nNOTE JSON and Parquet processing are configurable too. The focus is dif-\nferent in each format. The most rele vant options for JSON are the format\nspecifications for dates and numb ers, while Parquet requires some\nthought\u2014especially when writing\u2014abou t the size of row groups and the\ncompression. This can be any other \nfunction too (e.g., read_json).102 CHAPTER  5Exploring data without persistence\n5.3 Shredding nested JSON\nDuckDB\u2019s built-in JSON exte nsion has functions to create, read, and manipulate\nJSON strings. This is achieved by automa tically detecting the types and column names\nand then converting the values within  the JSON into DuckDB\u2019s vectors. \n We\u2019re going to explore this capability usin g a set of JSON files to represent shots\nthat were taken in Premier League football matches. These files were created usingthe understatapi library ( https:/ /pypi.org/project/understatapi/ ). The source data is\nin JSON lines format, with one match per ro w. The files are stor ed in a subdirectory\nxg, which stands for expected goals , a term borrowed from the understat site. \n We can explore the high-level structure of each JSON document by executing the\nfollowing SQL query from the DuckDB CLI. The example shows that it is possible to\nquery not only a single file but many files at once. \nxg/shots_*.json  is a wildcard\nexpression that will find all the files in the xg directory with the prefix shots_ and the\nsuffix .json. Our example file s have slightly different schemas, which don\u2019t work well\nwith either union, by position or name, so we must deal with th ose ourselves and stick\nwith the default: union by position. We will unnest  the JSON objects into separate\nrows and fix any conformity problems that become apparent in the process. We rec-\nommend using the line  in the DuckDB CLI mode because there are a lot of fields,\nwhich would get truncated if we were to use the default duckbox  presentation mode:\n.mode line\nDESCRIBE FROM 'xg/shots_*.json';\nNOTE By default, unnest  only unpacks the first level of a JSON object. If you\nwant to unpack deeply nested objects, you can use the recursive := true\nparameter. \nThe output from running this  query is shown in the following code. Please take note\nthat we line-wrapped the output. In line  mode, the DuckDB CLI will produce the type\ninformation without any line breaks. The schema that has been inferred for the\nexpected goals JSON data looks like this:\ncolumn_name = h\ncolumn_type = STRUCT(\nid BIGINT, \"minute\" BIGINT, result VARCHAR,X VARCHAR, Y VARCHAR, xG VARCHAR,\nplayer VARCHAR, h_a VARCHAR, player_id BIGINT,\nsituation VARCHAR, season BIGINT, shotType VARCHAR, match_id BIGINT,h_team VARCHAR, a_team VARCHAR, h_goals BIGINT, a_goals BIGINT,\ndate TIMESTAMP, player_assisted VARCHAR, lastAction VARCHAR)[]\nnull = YES\nkey =\ndefault =\nextra =\ncolumn_name = a\ncolumn_type = STRUCT(\nid BIGINT, \"minute\" BIGINT, result VARCHAR,103 5.3 Shredding nested JSON\nX VARCHAR, Y VARCHAR, xG VARCHAR,\nplayer VARCHAR, h_a VARCHAR, player_id BIGINT,\nsituation VARCHAR, season BIGINT, shotType VARCHAR, match_id BIGINT,h_team VARCHAR, a_team VARCHAR, h_goals BIGINT, a_goals BIGINT,\ndate TIMESTAMP, player_assisted VARCHAR, lastAction VARCHAR)[]\nnull = YES\nkey =\ndefault =\nextra =\nFrom this output, we learn that each entry has h and a properties that point to STRUCT\narrays, where each STRUCT  represents a single event. The content is identical, but\nthere was no way for DuckDB to create a un ified schema for our input files because it\ncouldn\u2019t match the co lumns by either position or na me. The data will be easier to\nwork with if we unpack those arrays. Unpacking  here means that we turn an array of\nelements into individual elements  so that we can easily use the LIMIT  clause to restrict\nthe number of results returned. The unnest  function will unpack those arrays. We use\nit to create a row for each valu e in the arrays stored in the h and a properties. This is\ndone in two separate SELECT  statements, which are then combined using the UNION\nALL clause. We also will limi t the result to a small value greater than 1 (JSON docu-\nments don\u2019t have to be unif orm, so one result may be too few, but a result that\nincludes all rows will be overwhelming too):\nFROM 'xg/shots_*.json'\nSELECT unnest(h) AS row\nUNION ALLFROM 'xg/shots_*.json'\nSELECT unnest(a) AS row\nLIMIT 3;\nIf we run this query, we will receive three rows. While the source of each row is a JSON\nobject, DuckDB does return a DuckDB struct, an d though it shares some similarities\nwith JSON, it does not parse as JSON. We \u2019ve reproduced one ro w as follows, again\nwrapping the lines to make the code more readable:\nrow = {'id': 54521, 'minute': 43, 'result': MissedShots,\n'X': 0.9419999694824219, 'Y': 0.52, 'xG': 0.07078909873962402,\n'player': Chancel Mbemba, 'h_a': h, 'player_id': 849,\n'situation': FromCorner, 'season': 2015, 'shotType': Head,'match_id': 229, 'h_team': Newcastle United, 'a_team': Liverpool,\n'h_goals': 2, 'a_goals': 0, 'date': 2015-12-06 20:00:00,\n'player_assisted': Papiss Demba Ciss\u00e9, 'lastAction': Pass}\nBefore we do any further analysis, let\u2019s create  a view based on the results of that query.\nA view is not physically materialized but in stead runs the underlying query each time.\nThe benefit of defining a view is that it pr ovides us with a shorthand for querying the\ndata rather than having to  write out the full location  of the file each time:This combines the resu lts of the statements \nbefore and after this clause, including duplicates (indicated by the ALL keyword).\nChoose a number to limi t the results returned \nthat gives you enough results to judge \ndata quality but does not overwhelm you.104 CHAPTER  5Exploring data without persistence\nCREATE VIEW shots AS\nFROM (\nFROM 'xg/shots_*.json'SELECT unnest(h) AS row\nUNION ALL\nFROM 'xg/shots_*.json'SELECT unnest(a) AS row\n);\nNext, we\u2019re going to look at the schema of that view:\nDESCRIBE shots;\nWe should see output si milar to the following:\ncolumn_name = row\ncolumn_type = STRUCT(\nid BIGINT, \"minute\" BIGINT, result VARCHAR, X VARCHAR, Y VARCHAR,xG VARCHAR, player VARCHAR, h_a VARCHAR, player_id BIGINT,\nsituation VARCHAR, season BIGINT, shotType VARCHAR, match_id BIGINT,\nh_team VARCHAR, a_team VARCHAR, h_goals BIGINT, a_goals BIGINT,date TIMESTAMP, player_assisted VARCHAR, lastAction VARCHAR)\nnull = YES\nkey =\ndefault =\nextra =\nDuckDB\u2019s inference wasn\u2019t perfect. X, Y, and xG are coordinates in the original JSON\nand should all have a numeric type (e.g., DOUBLE ); otherwise, we won\u2019t be able to per-\nform numeric operations on those fields . To fix this, we take the inferred STRUCT  type\nand change the type definition of the fields in question and then cast each row to that\nnew STRUCT . \n The following listing shows how we cast row to a STRUCT , with these fields set to the\nDOUBLE  type. Let\u2019s replace the view with one in which we\u2019ve cast those fields to the cor-\nrect types. \nCREATE OR REPLACE VIEW shots AS\nFROM (\nFROM 'xg/shots_*.json'SELECT unnest(h) AS row\nUNION ALL\nFROM 'xg/shots_*.json'SELECT unnest(a) AS row\n)\nSELECT CAST(ROW AS STRUCT(\nid BIGINT, \"minute\" BIGINT, result VARCHAR,\nX DOUBLE, Y DOUBLE, xG DOUBLE,\nplayer VARCHAR, h_a VARCHAR, player_id BIGINT,situation VARCHAR, season BIGINT, shotType VARCHAR,\nmatch_id BIGINT, h_team VARCHAR, a_team VARCHAR,Listing 5.1 Using CAST  on a whole STRUCT  to fix the inferred types for some attributes\nNote the different \ndata type.105 5.3 Shredding nested JSON\nh_goals BIGINT, a_goals BIGINT, date TIMESTAMP,\nplayer_assisted VARCHAR, lastAction VARCHAR)) AS row;\nNow let\u2019s look at the sc hema of the view again:\nDESCRIBE shots;\nWe can see that those types have been updated to DOUBLE :\ncolumn_name = row\ncolumn_type = STRUCT(\nid BIGINT, \"minute\" BIGINT, result VARCHAR,\nX DOUBLE, Y DOUBLE, xG DOUBLE,\nplayer VARCHAR, h_a VARCHAR, player_id BIGINT,situation VARCHAR, season BIGINT, shotType VARCHAR,\nmatch_id BIGINT, h_team VARCHAR, a_team VARCHAR,\nh_goals BIGINT, a_goals BIGINT, date TIMESTAMP,player_assisted VARCHAR, lastAction VARCHAR)\nnull = YES\nkey =\ndefault =\nextra =\nIf you are lucky, as in this case, you can ju st cast the whole type, as DuckDB\u2019s sampling\nwas wrong. If DuckDB\u2019s sampling, however, was right, and there truly are fields that\ncannot be automatically cast into a non-stri ng type, queries on the view will be prob-\nlematic. Numeric types, for example, might contain surpri ses, and from our experi-\nence, the representation of timestamps is often the hardest to deal with, despite thefact that there is an adequate ISO standard  for formatting dates and timestamps alike.\n While DuckDB does validate the property names of the JSON stru cture, it does not\ntry to cast each value at the moment the view  is defined, so there may be data lurking\nin your input that does not conform to an  expected format. Addi tionally, if you hap-\npen to add new files to your pipeline, new, invalid data may appear. In this case, you\nend up with an error such as \nError:  Conversion  Error:  Could  not convert  string\n'abc'  to DOUBLE  when you query for SELECT  row.x  FROM  shots . Sometimes this is\nexactly what you want, bu t sometimes it is not.\n There are two ways you can address this. Your first option is to examine each of the\nattributes at the field level and fix them in dividually (see section 5.4). If you take this\napproach, be sure to use one of the date formatting functions DuckDB offers (any of\nthem will do) prior to casting. In an expl orative use case, such as this, going down to\nthe field level is a task that requires a lot of effort, which may no t be worth it. Instead,\nwe recommend switching from the cast  function to try_cast  (see listing 5.1, which\ndetails the use of try_cast(row  AS STRUCT(\u2026))  AS row). Any fields with data that can-\nnot be cast to a DOUBLE  will now be returned as literal NULL  values. \n At this point, we could start writing queries directly against this view, but one more\nneat feature DuckDB supports is unpackin g structs into columns. This means that\nrather than having a single column with ne sted fields, each field would be its own col-\numn. As such, you can easily address that column to begin with or apply any kind of106 CHAPTER  5Exploring data without persistence\nfunction or aggregation on it. You can comp are this to the way we unnested, or flat-\ntened, the lists in the original JSON file s, which meant transfor ming a structure like\n[1, 2, 3] into three rows: 1, 2, and 3. This is also supported for structs and map-\nshaped types; unnesting them  will result in one column per attribute or key. The\nSELECT unnest({'x'  :1, 'y':2,  'z':  3}) statement unnests, or flattens, the anony-\nmous struct into three columns:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502x\u2502y\u2502z\u2502\n\u2502 int32 \u2502 int32 \u2502 int32 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021 \u25022 \u25023 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe * operator can be used as shorthand for expanding a struct to  columns when the\nstruct can be referenced via a variable, as in  this statement. This yields the same out-\nput as our previous result:\nWITH src AS (SELECT {'x' :1, 'y':2, 'z': 3} AS row)\nSELECT row.* FROM src;\nThis technique saves you from the hassle of  needing to qualify access to the elements\nof your struct at all times. With that knowle dge, let\u2019s create a new view that flattens the\nrows we extracted from the lists too:\nCREATE OR REPLACE VIEW shotsFlattened AS (\nSELECT row.*FROM shots\n);\nWe can then describe that new view:\n.mode duckboxDESCRIBE shotsFlattened;\nWe can see the new schema with individua l columns. The output  has been slightly\ntruncated for readability:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502\u2502 varchar \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id \u2502 BIGINT \u2502 YES \u2502\u2502 minute \u2502 BIGINT \u2502 YES \u2502\n\u2502 result \u2502 VARCHAR \u2502 YES \u2502\n\u2502 X \u2502 DOUBLE \u2502 YES \u2502\u2502 Y \u2502 DOUBLE \u2502 YES \u2502\n\u2502 xG \u2502 DOUBLE \u2502 YES \u2502\n\u2502 player \u2502 VARCHAR \u2502 YES \u2502\u2502 h_a \u2502 VARCHAR \u2502 YES \u2502\n\u2502 player_id \u2502 BIGINT \u2502 YES \u2502\n\u2502 situation \u2502 VARCHAR \u2502 YES \u2502The .* syntax creates one column \nfor every top-level field in a struct.107 5.3 Shredding nested JSON\n\u2502 season \u2502 BIGINT \u2502 YES \u2502\n\u2502 shotType \u2502 VARCHAR \u2502 YES \u2502\n\u2502 match_id \u2502 BIGINT \u2502 YES \u2502\u2502 h_team \u2502 VARCHAR \u2502 YES \u2502\n\u2502 a_team \u2502 VARCHAR \u2502 YES \u2502\n\u2502 h_goals \u2502 BIGINT \u2502 YES \u2502\u2502 a_goals \u2502 BIGINT \u2502 YES \u2502\n\u2502 date \u2502 TIMESTAMP \u2502 YES \u2502\n\u2502 player_assisted \u2502 VARCHAR \u2502 YES \u2502\u2502 lastAction \u2502 VARCHAR \u2502 YES \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25022 0r o w s \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe\u2019ll conclude our exploration of nested JSON files with a query that finds the teams\nwith the highest number of expected goal s in the 2022 season since providing data\nrelated to this metric is Understat\u2019s mission ( https:/ /understat.com ):\nSELECT CASE\nWHEN h_a = 'h' AND result <> 'OwnGoal' THEN h_team\nWHEN h_a = 'a' AND result = 'OwnGoal' THEN h_team\nELSE a_teamEND AS team,round(sum(xg), 2) AS totalXG,count(*) FILTER(WHERE result IN ('Goal', 'OwnGoal')) AS goals\nFROM shotsFlattenedWHERE season = 2022GROUP BY ALLORDER BY totalXG DESCLIMIT 10;\nNOTE Expected goals  (xG) is a football metric which allows you to evaluate\nteam and player performance. In a lo w-scoring game, such as football, the\nfinal match score does not always provid e a clear picture of each team\u2019s per-\nformance. This is why more and more sports analysts turn to advanced mod-els, like xG, which is a statistical meas ure of the quality of chances to score a\ngoal created and conceded in a match. Essentially, this is a problem field in\nwhich an analytical database li ke DuckDB can be very useful.\nIf you follow Premier League football, you won\u2019t be surprised to see that the best team\nin the league, Manchester City, also re corded the highest xG metric in 2022:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 team \u2502 totalXG \u2502 goals \u2502\n\u2502 varchar \u2502 double \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Manchester City \u2502 45.66 \u2502 53 \u2502\n\u2502 Arsenal \u2502 41.2 \u2502 45 \u2502\u2502 Liverpool \u2502 39.96 \u2502 34 \u2502\n\u2502 Newcastle United \u2502 38.17 \u2502 33 \u2502\n\u2502 Brighton \u2502 34.2 \u2502 37 \u2502\u2502 Manchester United \u2502 34.03 \u2502 32 \u2502108 CHAPTER  5Exploring data without persistence\n\u2502 Tottenham \u2502 33.49 \u2502 40 \u2502\n\u2502 Brentford \u2502 31.7 \u2502 32 \u2502\n\u2502 Fulham \u2502 30.12 \u2502 32 \u2502\u2502 Leeds \u2502 27.32 \u2502 26 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows 3 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n5.4 Translating CSV to Parquet\nA common task in data engineering is conver ting one data format to another. The ini-\ntial tools in the space assumed that you\u2019d ne ed to load the entire source dataset into\nmemory before converting it to the target  data format. DuckDB lets you set a memory\nlimit such that it will load a limited number  of source data rows into memory. This is\nuseful if you\u2019re dealing with large datasets  or using a machine with limited resources. \n In this section, we\u2019re going to learn how to convert CSV files to Parquet format. Par-\nquet is a commonly used columnar storage file format that wa s designed for big data pro-\ncessing frameworks, like Apache Spark.  Its efficient compression and encoding\ntechniques provide the benefit of reduced storage requirements and improved query\nperformance compared to te xt-based formats without metadata, like CSV and JSON.\nThis file type also uses predicate and proj ection pushdown, which enable the execution\nof selective queries and minimize data transf er\u2014which are especially beneficial in dis-\ntributed environments. With these mechanisms, you can tell the storage layer to only\nfetch selected columns or spec ific segments of data releva nt for your query and match\nconditions, leaving the remaining stored data completely untouched. \n From the book\u2019s GitHub repository , navigate to the ch05 directory:\ncd ch05\nYou should see an atp directory that cont ains a set of files with the prefix atp_\nrankings_. These files contain data on the rankings of professional tennis playersgoing back to the 1970s. You\u2019ll also see a single atp_players.csv file, which contains\nplayer metadata.\n Depending on your operating system, you ca n view this directory either in your UI\nor using a command-line tool, like \nls or du. The latter gives you a nice, human-\nreadable size of files to look at:\ndu -h atp/*.csv\nRegardless of your approach, you should se e the following files and approximate sizes:\n2,1M atp/atp_players.csv\n20M atp/atp_rankings_00s.csv\n20M atp/atp_rankings_10s.csv\n3,3M atp/atp_rankings_20s.csv412K atp/atp_rankings_70s.csv\n5,7M atp/atp_rankings_80s.csv\n16M atp/atp_rankings_90s.csv\n2,1M atp/atp_rankings_current.csv109 5.4 Translating CSV to Parquet\nWe are essentially looking at normalized tables \u2014the players are stored independent of\ntheir rankings, as one would do in a relation al model. Our Parquet file should contain\ndenormalized data in the end, and we will join the rankings and players together in\nour pipeline. \n None of the most common file  browsing programs, such as du, Windows Explorer,\nand Finder for macOS, can give  you direct insight into the files, such as the number of\nrows or records in each file. This is an im portant quality for our task of taking the con-\ntent of all CSV files and converting them to one Parquet file. As a columnar format type,\nParquet files group rows together. Finding a good size for the number of rows in a\ngroup is important. If the row group size is too small, compression is  less effective. A sig-\nnificant portion of the file will be take n up with row group headers, and compression\nworks better over larger blocks. This means th e file is going to be larger, and more pro-\ncessing time will be spent re ading headers. On the other hand, if the row group size is\ntoo large, DuckDB cannot parallelize the reads, so performance may suffer.\n Let\u2019s start by writing a query that counts the number of records in each of the rank-\nings files. The query is a nice example of how we want to use automatic structure and\ntype inference, while changing some  aspects. Therefore, we use the read_csv_auto\nfunction, which automatically infers the type of each field in the file, as FROM  'atp/\natp_rankings_*.csv';  would do, but it also allows us to pass in the filename=true\nflag. This will add a computed column to the result, containing the filename of each\nCSV file read:\nSELECT filename, count(*)\nFROM read_csv_auto(\n'atp/atp_rankings_*.csv',\nfilename=true\n)GROUP BY ALL\nORDER BY ALL;\nThe individual filenames and the number of rows per file will be printed when run-\nning the statement:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 filename \u2502 count_star() \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 atp/atp_rankings_00s.csv \u2502 920907 \u2502\n\u2502 atp/atp_rankings_10s.csv \u2502 915618 \u2502\n\u2502 atp/atp_rankings_20s.csv \u2502 149977 \u2502\u2502 atp/atp_rankings_70s.csv \u2502 20726 \u2502\u2502 atp/atp_rankings_80s.csv \u2502 284809 \u2502\n\u2502 atp/atp_rankings_90s.csv \u2502 725606 \u2502\n\u2502 atp/atp_rankings_current.csv \u2502 95618 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNOTE On some systems, you could use a counting utility, such as wc, using the\n-l option for counting lines, wc -l atp/atp_rankings_*.csv , but where\u2019s the\nfun in that?Add the filename of each \nfile to the resulting rows.110 CHAPTER  5Exploring data without persistence\nWe have just over 3 million records, spread acro ss seven files. A coup le of those files\nhave almost 1 million records, and the sma llest file has only 20,000. We can safely\nassume there will be a relatively large row group size for the Parquet file further downthe road.\n In contrast to CSV files, Parquet files contain a self-describing schema with dedi-\ncated data types. Whereas with CSVs, everythi ng is typically a string by default, and\neach column must be sampled to determine wh at kind of data is actually inside, Par-\nquet files already contain this  information. Parquets only  natively store a small num-\nber of data types\u2014called physical data types . However, they can represent a larger\nnumber of types by adding annotations\u2014called logical types . For example, Parquet files\nstore dates as numeric values, but additional  metadata tells readers they should inter-\npret those numeric values as dates. \n To create a valuable Parquet file, we shou ld first examine the CSV file to see if each\ncolumn we deal with contains the most specific and precise data type. These types willbe written into the schema of the target fi le. Let\u2019s explore what the individual records\nlook like by running the following query:\nSELECT *\nFROM 'atp/atp_rankings_*.csv'\nLIMIT 5;\nThe output represents the rankings of seve ral players from the first week of January\n2000, limited to five records:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ranking_date \u2502 rank \u2502 player \u2502 points \u2502\u2502 int64 \u2502 int64 \u2502 int64 \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 20000110 \u2502 1 \u2502 101736 \u2502 4135 \u2502\u2502 20000110 \u2502 2 \u2502 102338 \u2502 2915 \u2502\n\u2502 20000110 \u2502 3 \u2502 101948 \u2502 2419 \u2502\n\u2502 20000110 \u2502 4 \u2502 103017 \u2502 2184 \u2502\u2502 20000110 \u2502 5 \u2502 102856 \u2502 2169 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe ranking_date  column is recognized as a nu merical value by DuckDB. Looking\nclosely, we can be quite sure that it actually represents dates, formatted as %Y%m%d .\nWhile this looks a bit like an ISO date format , it isn\u2019t, and you can do only some oper-\nations over dates in this format, such as sort them. Otherwise, dates formatted this wayare awkward to use, and you can\u2019t pass them  as arguments to SQL date manipulation\nfunctions. Notably, they won\u2019t translate to  the proper logical data type in Parquets\neither. Let\u2019s convert that column to a date, using the \nstrptime  function. strptime\ntakes two character arguments:  the string to be parsed into a date and the format.\nDuckDB will implicitly cast ranking_date , which is an int64 , to a string before being\npassed to strptime . The result of strptime  is then cast to a date, stripping away the\ntime information:111 5.4 Translating CSV to Parquet\nSELECT * REPLACE (\ncast(strptime(ranking_date::VARCHAR, '%Y%m%d') AS DATE)\nAS ranking_date\n)FROM 'atp/atp_rankings_*.csv'LIMIT 5;\nNow the result looks like this, showing the ranking_date  as a proper date:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 ranking_date \u2502 rank \u2502 player \u2502 points \u2502\u2502 date \u2502 int64 \u2502 int64 \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 2000-01-10 \u2502 1 \u2502 101736 \u2502 4135 \u2502\u2502 2000-01-10 \u2502 2 \u2502 102338 \u2502 2915 \u2502\u2502 2000-01-10 \u2502 3 \u2502 101948 \u2502 2419 \u2502\u2502 2000-01-10 \u2502 4 \u2502 103017 \u2502 2184 \u2502\u2502 2000-01-10 \u2502 5 \u2502 102856 \u2502 2169 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nOnly the first column has changed; the others are the same as before.\nNOTE We could have used FROM read_csv_auto('atp/atp_rankings_*\n.csv', dateformat='%Y%m%d'); to specify that dateformat for all possible\ncolumns. This feels like a bold move, as  any 8-digit number would fit that for-\nmat, so we decided to fix this per individual column.\nAt the moment, we don\u2019t know which player each row refers to, but we can work this\nout by joining the atp_players.csv file. We \u2019re also going to fix a problem with the dob\nfield in the players CSV file, which is also formatted as a string in the %Y%m%d  format.\nWe end up with the following query:\nSELECT * EXCLUDE (\nplayer,wikidata_id,name_first,name_last, player_id, hand, ioc\n)REPLACE (\ncast(strptime(ranking_date::VARCHAR, '%Y%m%d') AS DATE) AS ranking_\ndate,\ncast(strptime(dob, '%Y%m%d') AS DATE) AS dob\n),name_first | |''| |n a m e_last AS name\nFROM 'atp/atp_rankings_*.csv' rankingsJOIN (FROM 'atp/atp_players.csv' ) players\nON players.player_id = rankings.player\nORDER BY ranking_date DESCLIMIT 5;\nIf we run this query, we\u2019ll see the follow ing results, including the players\u2019 names!Remember * REPLACE() selects all \ncolumns and replaces  some of them.\nExclude some \ncolumns for brevity.\nJoin the atp_player s.csv file on the \nplayer_id column ma tching the player \ncolumn in the rankings CSV files.112 CHAPTER  5Exploring data without persistence\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ranking_date \u2502 rank \u2502 points \u2502 dob \u2502 height \u2502 name \u2502\u2502 date \u2502 int64 \u2502 int64 \u2502 date \u2502 int64 \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2022-12-26 \u2502 1 \u2502 6820 \u2502 2003-05-05 \u2502 185 \u2502 Carlos Alcaraz \u2502\u2502 2022-12-26 \u2502 2 \u2502 6020 \u2502 1986-06-03 \u2502 185 \u2502 Rafael Nadal \u2502\n\u2502 2022-12-26 \u2502 3 \u2502 5820 \u2502 1998-12-22 \u2502 183 \u2502 Casper Ruud \u2502\n\u2502 2022-12-26 \u2502 4 \u2502 5550 \u2502 1998-08-12 \u2502 193 \u2502 Stefanos Tsitsipas \u2502\u2502 2022-12-26 \u2502 5 \u2502 4820 \u2502 1987-05-22 \u2502 188 \u2502 Novak Djokovic \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNext, we\u2019re going to export the results to a Parquet file. We\u2019d likely run the command\nto generate our Parquet file as part of a scri pted data pipeline, so let\u2019s first exit the CLI\nby typing .exit . The amount of data we\u2019re exporting easily fits in memory, but we can\nrestrict the amount of memory DuckDB uses by adjusting the memory_limit  setting.\nThe ability to restrict the me mory being used is valuable in pipelines with a restricted\namount of memory or in a se rverless setting. By default, DuckDB will use 80% of all\navailable RAM. After adjusting this setting, we\u2019ll use the COPY..TO  clause to convert\nthe contents of the CSV files into a single Parquet file; take note that the full state-\nment we developed in this section doesn\u2019t include the LIMIT  clause anymore. That was\nuseful for our investigation and exploration,  but in the end, we want all rows pro-\ncessed. DuckDB allows us to configure the compression algorithm to use when writing\nParquet files, and we opted for the SNAPPY  codec here over GZIP . While the latter gen-\nerally achieves a higher compression rati o, the former is op timized for speed\u2014which\nwas our main concern when creating the f ile. The row group size  appears sensible,\ngiven the sheer amount of data:\nduckdb -s \"SET memory_limit='100MB';\nCOPY (\nSELECT * EXCLUDE (player, wikidata_id)\nREPLACE (\ncast(strptime(ranking_date::VARCHAR, '%Y%m%d') AS DATE)\nAS ranking_date,\ncast(strptime(dob, '%Y%m%d') AS DATE) AS dob\n)\nFROM 'atp/atp_rankings_*.csv' rankingsJOIN (\nFROM 'atp/atp_players.csv'\n) players ON players.player_id = rankings.player\n)\nTO 'atp_rankings.parquet'\n(FORMAT PARQUET, CODEC 'SNAPPY', ROW_GROUP_SIZE 100000);\"\nThe -s flag lets us pass a command, which it wi ll run before exiting. That should only\ntake a few seconds to run, after which we can check the size of the generated Parquet\nfile:\ndu -h *.parquet\nThe output is as follows:\n36M atp_rankings.parquet113 5.5 Analyzing and querying Parquet files\nNOTE Try exporting the query output to CSV and JSON formats so that you\ncan see the difference in the file size. \n5.5 Analyzing and quer ying Parquet files\nNot only are Parquet files used extensively in  data processing pipelines, but they can\nalso be a great data source to query from within DuckDB. They are much closer to adatabase than CSV files or JSON files, as  they provide a schema in their metadata.\nTherefore, it might be the case that you didn\u2019t create the atp_rankings.parquet file\nbut received it from someon e else. You don\u2019t know its st ructure or content but are\ntasked to create reports on it. In this se ction, we\u2019ll learn how to retrieve the schema\nand additional metadata from Parquet files, an d we will use the file we just created for\nthe sake of simplicity and adhering to our example. \n If we\u2019re just interested in the column names and type s contained within a Parquet\nfile, we can use the \nDESCRIBE  clause against the Parquet f iles, like you would with any\nother supported data source:\nDESCRIBE FROM 'atp/atp_rankings.parquet';\nThe output of this query is shown in the fo llowing snippet. Have a close look at the\nranking_date  and dob columns\u2014both have a DATE  type, indicating that the type coer-\ncion we did in the previous section was successful:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ranking_date \u2502 DATE \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 rank \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 points \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 player_id \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 name_first \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 name_last \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 hand \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 dob \u2502 DATE \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 ioc \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 height \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 rows 6 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAll the columns from the previous section are there, and the types seem reasonable.\n This schema is enough when you just want to query and analyze the data. Accord-\ningly, you could stop here, treat the file as a table, and apply your SQL knowledgefrom chapters 3 and 4. If you really did just create the file using DuckDB, you may\nwant to go deeper. Remember, Parquet has only a few physical types\u2014Boolean, num-\nbers of various size, and byte arrays\u2014and needs to convert from those types to some-\nthing \u201chigher level.\u201d When working with hu ge datasets, numbers may be of special\ninterest\u2014Parquet has \nint32 , int64 , and int96  for integers. More vector-based opera-114 CHAPTER  5Exploring data without persistence\ntions can be executed in parallel over small er, numerical data type s. If optimal perfor-\nmance is the end goal, you should try to us e the smallest data type possible that can\nstill hold your data. \n The parquet_schema  function can be used to query the internal schema contained\nwithin a Parquet file. This is the schema stor ed as metadata inside the file. It will give\nus some insight into which columns can use an existing physical Parquet data type as\nwell as those that don\u2019t need conversions and those that do. This function returns\nmany fields, so let\u2019s first prefix it with DESCRIBE  to get a list of those fields:\nDESCRIBE FROM parquet_schema('atp/atp_rankings.parquet');\nThe resulting description of the parquet_schema  function is as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502 key \u2502 default \u2502 extra \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 file_name \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 name \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 type \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 type_length \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 repetition_type \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 num_children \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 converted_type \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 scale \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 precision \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\u2502 field_id \u2502 BIGINT \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u2502 logical_type \u2502 VARCHAR \u2502 YES \u2502 \u2502 \u2502 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 11 rows 6 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe most interesting fi elds, in this case, are the name and types, so let\u2019s write a query\nthat returns only those values:\nFROM parquet_schema('atp/atp_rankings.parquet')\nSELECT name, type, converted_type, logical_type;\nThe updated results look like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name \u2502 type \u2502 converted_type \u2502 logical_type \u2502\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 duckdb_schema \u2502 \u2502 \u2502 \u2502\u2502 ranking_date \u2502 INT32 \u2502 DATE \u2502 \u2502\n\u2502 rank \u2502 INT64 \u2502 INT_64 \u2502 \u2502\n\u2502 points \u2502 INT64 \u2502 INT_64 \u2502 \u2502\u2502 player_id \u2502 INT64 \u2502 INT_64 \u2502 \u2502\n\u2502 name_first \u2502 BYTE_ARRAY \u2502 UTF8 \u2502 \u2502\n\u2502 name_last \u2502 BYTE_ARRAY \u2502 UTF8 \u2502 \u2502\u2502 hand \u2502 BYTE_ARRAY \u2502 UTF8 \u2502 \u2502\n\u2502 dob \u2502 INT32 \u2502 DATE \u2502 \u2502115 5.5 Analyzing and querying Parquet files\n\u2502 ioc \u2502 BYTE_ARRAY \u2502 UTF8 \u2502 \u2502\n\u2502 height \u2502 INT64 \u2502 INT_64 \u2502 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 11 rows 4 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe type  field describes the actual type used on disk, which is intended to be as mini-\nmal as possible. The converted_type  and logical_type  fields contain a description\nof how the type  should be interpreted. For example, ranking_date  is stored as an\nINT32  but should be treated as a DATE  when that field is processed. converted_type\nhas been deprecated in Parquet, but as you can see, it is still written to Parquet fieldsfor backward compatibility. \n Something that stands out when l ooking at this metadata is that \nrank , points ,\nplayer_id , and height  are all represented as 64-bit in tegers. A signed 64-bit integer\nhas a maximum value of 9,223,372,036,854,775,807 . It would be surprising if the\nvalues for the points  or height  fields needed so much spac e, but we can write a query\nto check the maximum values being stored:\nfrom 'atp/atp_rankings.parquet'\nselect max(rank), max(points), max(player_id), max(height);\nWe can see those maximum va lues in the next output:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 max(rank) \u2502 max(points) \u2502 max(player_id) \u2502 max(height) \u2502\n\u2502 int64 \u2502 int64 \u2502 int64 \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2271 \u2502 16950 \u2502 211767 \u2502 211 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNone of these values are anywhere near th e upper bound of even a 32-bit integer, so\nwe could potentially optimize further operat ions on the data by casting the fields to\nINT32  before exporting to Parquet format. \nNOTE See if you can work out how to export the data to Parquet format while\nusing int32  for those fields. In essence, you want to cast the relevant fields,\nsuch as points . We previously examined the structure of the source CSV files,\nseeing that the fields are recognized as int64  or BIGINT  in DuckDB terms. A\ncast can be written as CAST(points AS INTEGER)  or points::integer , with\nINTEGER  corresponding to Parquet\u2019s int32 . DuckDB supports TINYINT\n(int8 ), SMALLINT  (int16 ), INTEGER  (int32 ), BIGINT  (int64 ), as well as their\nunsigned variants and a HUGEINT  (int128 ). \nWe can also explore the structure of the Parquet file itself by using the parquet_\nmetadata  function. This function returns on e record per row group per column:\n.mode line\nFROM parquet_metadata('atp/atp_rankings.parquet')LIMIT 1;116 CHAPTER  5Exploring data without persistence\nThis function returns many columns, so we \u2019re using line mode again to prevent the\nfollowing from being printed as a tabular resu lt. It shows the column with an ID of 0\nin the first row group of the file:\nfile_name = atp/atp_rankings.parquet\nrow_group_id = 0\nrow_group_num_rows = 20726\nrow_group_num_columns = 10\nrow_group_bytes = 2374571\ncolumn_id = 0\nfile_offset = 0\nnum_values = 20726\npath_in_schema = ranking_date\ntype = INT32\nstats_min = 1973-08-27\nstats_max = 1979-12-26\nstats_null_count = 0\nstats_distinct_count =\nstats_min_value = 1973-08-27stats_max_value = 1979-12-26\ncompression = SNAPPY\nencodings = PLAIN\nindex_page_offset =\ndictionary_page_offset =\ndata_page_offset = 4\ntotal_compressed_size = 5479\ntotal_uncompressed_size = 82934\nThis record is for the ranking_date  column in the first row group ( row_group_id =\n0). From looking at stats_min , we learn that the smallest value in this row group is\nAugust 27th, 1973, and from looking at stats_max , we know that the largest value is\nDecember 26th, 1979.\n DuckDB uses this metadata when execut ing queries. For example, if you wrote a\nquery that was looking for records where the ranking_date  was after 1980, it could\nsafely ignore all the values in this row group since it knows that the latest value is in1979. Parquet is an excellent file format th at provides a lot of information for a data-\nbase engine, allowing it to  optimize its queries for yo u in the best way possible. \n5.6 Querying SQLite and other databases\nAnother interesting feature of DuckDB is that  we can attach it to other databases and\nquery their contents. One such database is SQLite , an embedded OLTP database. We\nmight want to query existing SQLite file s from DuckDB if we\u2019re writing demanding\nanalytical queries and would like the benefit of DuckDB\u2019s query engine. \n While the SQL standard partially define s data types and behaviors, there may be\nmany differences in naming  and semantics between diffe rent vendors\u2019 implementa-\ntions. Attaching foreign databases to Duck DB sometimes means you will have to work\naround mismatches. Often, th e automatic inference works; sometimes, it doesn\u2019t. By\nthat token, you may be required to comple te the same conversion work as for CSV\nfiles\u2014covered in the previous section\u2014when querying foreign stores.117 5.6 Querying SQLite and other databases\n We\u2019ve downloaded the Kaggle European Soccer Database ( https:/ /www.kaggle\n.com/datasets/hugomathien/soccer ), which contains data on over 25,000 European\nprofessional football matches, players, and team attributes. It\u2019s available as a 300 MB\nSQLite database, which we provide in a co mpressed form in the example repository as\nwell. Before you follow the next examples , you must uncompress the database using\nunzip  or a Windows program of your choice . Here\u2019s how to uncompress it using\nunzip :\nunzip database.sqlite.zip\nTo query SQLite, we\u2019ll need to first install and load the sqlite  extension:\nINSTALL sqlite;\nLOAD sqlite;\nOnce we\u2019ve done that, we can attach all the tables to the fifa  database. The TYPE\nsqlite  can also be inferred from the file extension:\nATTACH 'database.sqlite' AS fifa (TYPE sqlite);\nUSE fifa;\nThe tables from SQLite are registered as views in DuckDB. We can list them by run-\nning the following SQL command (or the .tables  CLI command):\nPRAGMA show_tables;\nThis database has several tables:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name \u2502\u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Country \u2502\u2502 League \u2502\u2502 Match \u2502\u2502 Player \u2502\u2502 Player_Attributes \u2502\u2502 Team \u2502\u2502 Team_Attributes \u2502\u2502 sqlite_sequence \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIt looks like everything has been attached successfully. Let\u2019s see if we can query the\nPlayer  view:\nFROM Player\nLIMIT 5;\nAt the time of writing, we unex pectedly got the following error:\nError: Invalid Error: Mismatch Type Error: Invalid type in column \"height\":\n\u27a5column was declared as integer, found \"182.88\" of type \"float\" instead.118 CHAPTER  5Exploring data without persistence\nThis may be a bug in the current DuckDB version (0.10), which may be fixed in the\nfuture. However, it is actually quite an interesting problem. SQLite is a weakly typed\ndatabase system , which means types aren\u2019t enforced when storing data. DuckDB, on the\nother hand, is a strongly typed database system  and requires all columns to have defined\ntypes. DuckDB remains faithful to SQLite\u2019s  type system, which hasn\u2019t quite worked in\nthis case\u2014the height  column in SQLite was defined as an INT, even though it con-\ntains some float  values. \n Let\u2019s inspect the Player  view with DESCRIBE Player; , which returns the type of\neach field, as shown in the next output. We can see that height  is a BIGINT , even\nthough we saw from the previous query that it contains float  values. It looks like we\ncould have a problem with the weight  field as well, although perhaps we\u2019ve been\nlucky that there aren\u2019t any de cimal values in that field:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 null \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 id \u2502 BIGINT \u2502 YES \u2502\n\u2502 player_api_id \u2502 BIGINT \u2502 YES \u2502\n\u2502 player_name \u2502 VARCHAR \u2502 YES \u2502\u2502 player_fifa_api_id \u2502 BIGINT \u2502 YES \u2502\n\u2502 birthday \u2502 VARCHAR \u2502 YES \u2502\n\u2502 height \u2502 BIGINT \u2502 YES \u2502\u2502 weight \u2502 BIGINT \u2502 YES \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTo fix this, we\u2019re going to manually create the Player  view, but first, we\u2019ll need to\ndetach the SQLite database:\nUSE memory;\nDETACH fifa;\nNext, we\u2019re going to have all SQLite columns converted into the VARCHAR  type so that\nwe don\u2019t run into any conversion errors:\nSET GLOBAL sqlite_all_varchar=true;\nWe can now use the sqlite_scan  command to get all the records from the Player\ntable. We\u2019ll then manually cast each fi eld to the correct type and redefine the Player\nview accordingly:\nUSE main;\nCREATE OR REPLACE VIEW Player AS\nFROM sqlite_scan('database.sqlite', 'Player')SELECT * REPLACE (\nid :: BIGINT AS id,\nplayer_api_id :: BIGINT AS player_api_id,player_fifa_api_id :: BIGINT AS player_fifa_api_id,\nbirthday :: DATE AS birthday,First, we must ensure we use a \ndifferent database , as we cannot \ndetach the database we are using.119 5.6 Querying SQLite and other databases\nheight :: FLOAT AS height,\nweight :: FLOAT AS weight\n);\nWe can then query the Player  view as we tried to do earlier:\nFROM Player SELECT * EXCLUDE player_fifa_api_idLIMIT 5;\nAnd this time it works! The IDs are proper int64  columns:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 id \u2502 player_api_id \u2502 player_name \u2502 birthday \u2502 height \u2502 weight \u2502\u2502 int64 \u2502 int64 \u2502 varchar \u2502 date \u2502 float \u2502 float \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1 \u2502 505942 \u2502 Aaron Appindangye \u2502 1992-02-29 \u2502 182.88 \u2502 187.0 \u2502\u2502 2 \u2502 155782 \u2502 Aaron Cresswell \u2502 1989-12-15 \u2502 170.18 \u2502 146.0 \u2502\n\u2502 3 \u2502 162549 \u2502 Aaron Doran \u2502 1991-05-13 \u2502 170.18 \u2502 163.0 \u2502\n\u2502 4 \u2502 30572 \u2502 Aaron Galindo \u2502 1982-05-08 \u2502 182.88 \u2502 198.0 \u2502\u2502 5 \u2502 23780 \u2502 Aaron Hughes \u2502 1979-11-08 \u2502 182.88 \u2502 154.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nSo far, so good. Now let\u2019s set the sqlite_all_varchar  back to false and manually cre-\nate the other views:\nSET GLOBAL sqlite_all_varchar=false;\nCREATE OR REPLACE VIEW Player_Attributes AS\nFROM sqlite_scan('database.sqlite', 'Player_Attributes');\nCREATE OR REPLACE VIEW Country AS\nFROM sqlite_scan('database.sqlite', 'Country');\nCREATE OR REPLACE VIEW League AS\nFROM sqlite_scan('database.sqlite', 'League');\nCREATE OR REPLACE VIEW Match AS\nFROM sqlite_scan('database.sqlite', 'Match');\nCREATE OR REPLACE VIEW Team AS\nFROM sqlite_scan('database.sqlite', 'Team');\nCREATE OR REPLACE VIEW Team_Attributes AS\nFROM sqlite_scan('database.sqlite', 'Team_Attributes');\nWe could now, for example, write a query to  find the top players, based on the most\nrecent rankings:\nSELECT player_name, arg_max(overall_rating, date) AS overall_rating\nFROM Player\nJOIN Player_Attributes PA ON PA.player_api_id = Player.player_api_idWHERE overall_rating is not null\nGROUP BY ALL\nORDER BY overall_rating DESC, player_nameLIMIT 10;Exclude the player_fifa_api_id \nfield for brevity.120 CHAPTER  5Exploring data without persistence\nThis query joins together two SQLite  tables before finding the highest overall_\nrating  for each player. The top 10 players are as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 player_name \u2502 overall_rating \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Lionel Messi \u2502 94 \u2502\n\u2502 Cristiano Ronaldo \u2502 93 \u2502\n\u2502 Luis Suarez \u2502 90 \u2502\u2502 Manuel Neuer \u2502 90 \u2502\n\u2502 Neymar \u2502 90 \u2502\n\u2502 Arjen Robben \u2502 89 \u2502\u2502 Zlatan Ibrahimovic \u2502 89 \u2502\n\u2502 Andres Iniesta \u2502 88 \u2502\n\u2502 Eden Hazard \u2502 88 \u2502\u2502 Mesut Oezil \u2502 88 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows 2 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThat all looks like it\u2019s working well, an d we\u2019ve successfully queried SQLite from\nDuckDB.\nNOTE DuckDB also has a postgres  extension for querying Postgres data-\nbases. The installation is similar to the SQLite extension\u2014just run INSTALL\npostgres; LOAD postgres;  in your DuckDB session. After that, you must use\nthe ATTACH  command to connect to the Postgres database and provide the\nconnection information for the instance. \nSee the following DuckDB documentation topic for more information on the\nextension: https:/ /duckdb.org/docs/e xtensions/postgres.html . Querying\nany Postgres table will be fully opaq ue, and all SQL features DuckDB offers\nwill work. \n5.7 Working with Excel files\nDuckDB can read and write Excel files stored as Microsof t Office Open XML\n(OOXML; file extension .xlsx). This format has been used by Microsoft Office since\n2007, and other applications, including Libr eOffice and Google Sheets, support it as\nwell. It requires the spatial extension to be  installed in DuckDB. While this extension\nis primarily used to deal with spatial da ta, its underlying mach inery supports OOXML\ntoo. The following listing shows how to  install it inside the DuckDB CLI.\nINSTALL spatial;\nLOAD spatial;\nThe INSTALL  statement is required only once: for the LOAD  statement in each session\nin which you want to use the extension. To read Excel files, you need to use theListing 5.2 Installing the spatial extension121 5.7 Working with Excel files\nst_read  function. We took a subset of the CSV data we dealt with in section 5.4 and\nprovided it as an Excel file in the exam ple repository. The following statement reads\nthe first sheet from that file:\nSELECT ranking_date, rank, name_last\nFROM st_read('atp_rankings.xlsx')\nORDER BY ranking_date limit 5;\nThe machinery for detecting types is not directly implemented in DuckDB but rather\nin the extension being used; it is less opti mized than the type detection for CSV and\nJSON files. In the result of the preceding query, we notice that date  columns can be\nread correctly, but for the rank, it picks a ge neric double, whereas in reality, it should\nbe an integer:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ranking_date \u2502 rank \u2502 name_last \u2502\n\u2502 date \u2502 double \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 1973-08-27 \u2502 129.0 \u2502 Gonzalez \u2502\u2502 1973-08-27 \u2502 114.0 \u2502 Ulrich \u2502\n\u2502 1973-08-27 \u2502 6.0 \u2502 Rosewall \u2502\n\u2502 1973-08-27 \u2502 19.0 \u2502 Emerson \u2502\u2502 1973-08-27 \u2502 82.0 \u2502 Phillips Moore \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nExcel files often contains formulas. These w ill be read as the raw formula string by\ndefault and will not be evaluated. Some au thoring tools store the last value with the\nformula\u2014in which case, that value can be read.\n There is also limited suppor t for writing Excel files. Some datatypes, such as dates\nand timestamps, are not supported and must be cast to a string or formatted as a\nstring, as we do in the following statement.  The statement takes about a minute to run\non the author\u2019s machine and produces a hefty 299 MB Excel file (the Parquet file is\njust about 36 MB in size):\nCOPY (\nSELECT * EXCLUDE (player, wikidata_id)\nREPLACE (\nstrftime(strptime(ranking_date, '%Y%m%d'), '%Y-%m-%d')\nAS ranking_date,\nstrftime(strptime(dob, '%Y%m%d'), '%Y-%m-%d') AS dob\n)\nFROM 'atp/atp_rankings_*.csv' rankings\nJOIN (\nFROM 'atp/atp_players.csv'\n) players ON players.player_id = rankings.player\nORDER BY ranking_date ASC\n)TO 'atp_rankings_full.xlsx' WITH (FORMAT GDAL, DRIVER 'xlsx');\nThis is essentially the same statement that  we used to create one Parquet file from\nour set of CSV files representing ATP rank ings. If the target file already exists,122 CHAPTER  5Exploring data without persistence\nGDAL  Error  (1):  File  extension  should  be XLSX  will be printed as an error message,\nwhich is a bit misleading and may be fixe d in future versions of the extension.\n In general, we recommend exporting Ex cel files to CSV before processing them\nwith DuckDB, if that\u2019s possible for you. Th e general integration with DuckDB makes it\neasier to work with. \nSummary\n\uf0a1You can use DuckDB\u2019s powerful query language and engine to process data,whether stored in files or flowing throug h a pipeline, even if you don\u2019t use its\ndatabase storag e functionality.\n\uf0a1DuckDB\u2019s query engine can be used with many different sources, such as files in\ndifferent formats or the stores of othe r databases. DuckDB does a great job of\ninferring the right content and data types for JSON, CSV, and Parquet files.\n\uf0a1DuckDB\u2019s JSON processing capabilities allow you to query and normalize even\ncomplex, denormalized JSON  documents so that they feel like a natural source\nof tabular data in any query.\n\uf0a1Data transformation with Du ckDB\u2014for example, filtering, type conversion, flat-\ntening, or enriching by joining other sources\u2014doesn\u2019t require persistence in\nDuckDB.\n\uf0a1The vector-based DuckDB query engine deals with some wo rkloads and queries\nso efficiently that using it with an ex ternal database store offers performance\nadvantages without losing any capabilities of the external database and without\nrequiring two different datasets to be kept in synchronization.\n\uf0a1Views are a helpful tool to encapsulat e necessary transfor mation on external\ndata types. 123Integrating with\nthe Python ecosystem\nUp until now, we\u2019ve consiste ntly used the DuckDB CLI to manage and execute our\nqueries. This tool is highly effective for on-the-spot analysis and for CLI-based pipe-\nlines. Many data workflows, however, in volve Python and its ecosystem to a large\nextent. For example, pandas DataFrames can\u2019 t be ignored. In this chapter, we will\nlearn that DuckDB\u2019s Python API goes wa y beyond just implementing the Python\nDB-API. DuckDB\u2019s Python API will let you not only use the embedded database inThis chapter covers \n\uf0a1The differences between DuckDB\u2019s \nimplementation of Python DB-API 2.0 and the DuckDB relational API\n\uf0a1Ingesting data from pandas DataFrames, Apache Arrow tables, and more via the Python API\n\uf0a1Querying pandas DataFrames with DuckDB methods\n\uf0a1Exporting data to various DataFrames formats and Apache Arrow Tables\n\uf0a1Using DuckDB\u2019s relational API to compose queries124 CHAPTER  6Integrating with the Python ecosystem\nyour Python process but also query Python objects like you would tables. At the same\ntime, you can easily convert results from qu eries to DataFrames. In this chapter, we\nfocus on integrations that are directly bundled with the DuckDB Python package.\nNOTE We will not cover SQLAlchemy ( https:/ /www.sqlalchemy.org ), a popu-\nlar Python SQL tool kit. SQLAlchemy abstracts away over many different data-bases and brings a full suite of well-kn own enterprise-level persistence patterns\nto Python, which are just beyond the scop e of this book. You can get a driver for\nSQLAlchemy under the name \nduckdb_engine  (https:/ /pypi.org/project/\nduckdb_engine/ ), which supports almost all of SQLAlchemy features. \n6.1 Getting started\nLet\u2019s get started by installing the DuckDB Python package and learning which depen-\ndencies to import into your programs first.  Next, we discuss the different options to\neither acquire an in-memory DuckDB connection or open a database file. Getting the\nidea of this is important for this chapter but also for the following ones, as we will\nlearn about more tools in the Python ecosystem that interact with DuckDB. \n6.1.1 Installing the Python package\nThe DuckDB Python package is published to PiPI, and we can install it by running the\nfollowing command:\npip install duckdb\nOnce you\u2019ve done that, open up a Python  command prompt, and import the follow-\ning libraries:\nimport duckdb\nNext, run the following command to return the version of DuckDB:\nduckdb.__version__\nYou should see an output similar to the fo llowing, although the exact version you see\nmay be different:\n'0.10.0'\n6.1.2 Opening up a database connection\nOne of the questions that often comes to mind when using a database from any pro-gramming environment is, How do I open a connection?  In the case of DuckDB, the\nanswer is simple: You don\u2019t have to because as an embedded database, it is already running\ninside your Python process.  Once you have installed the package, as shown in the preced-\ning code snippets, you can go ahead and use \nduckdb  in your Python interpreter to\ninteract with an in-memory database. sql is the entry point into DuckDB\u2019s relational\nPython API. The following example uses the show  method of the object returned to\nprint the result of the statement:125 6.2 Using the relational API\nresult = duckdb.sql('SELECT 42')\nresult.show()\nThe result will be printed in a tabular fash ion, similar to what the DuckDB CLI would\noutput.\n Please take note that we ran the SQL statement on the duckdb  object we imported\ninto our program. We didn\u2019t acquire or use a dedicated connection object. The\nduckdb  object provided us with a default, in -memory connection. Of course, you can\nuse a dedicated connection too:\n\uf0a1To start an in-memory database \u2014con = duckdb.connect(database=':memory:')\n\uf0a1To use a database file \u2014con = duckdb.connect('my-db.duckdb')\n\uf0a1To use a database file and control whether it\u2019s read-only or not \u2014con = duckdb\n.connect(database='my-db.duckdb', read_only=True)\nThe default connection can be acquired through both duckdb.default_connection\nand duckdb.connect(database=':default:') , assuming you have the database name\nin a parameter. The importan t takeaway here is that wi thout any further configura-\ntion, DuckDB will use an in-m emory database that is stored globally inside the Python\nmodule. \n Both the global duckdb  and the dedicated connection object offer various methods\nto interact with a database. sql()  will trigger the relational  API of DuckDB as shown,\nand execute()  will use the Python DB-API 2.0. \n The preceding example rewritten using the DB-API looks similar but behaves like\na traditional database API. It will execute the statement immediately and return an\nobject that allows fetching one or all resulting rows:\nresult = duckdb.execute('SELECT 42')\nrow = result.fetchone()print(row)\nThere are some use cases for the DB-API, su ch as the ability to use prepared state-\nments and queries that utilize named parame ters, but we will focus on the relational\nAPI in this chapter. We think the latter is  a unique approach to querying data, blur-\nring the lines between relational database s and queryable objects in memory, paving\nthe way for new, interesting solutions. The Python DB-API will appear again when wediscuss querying via SQL later in this chapter. \n6.2 Using the relational API\nWhile we managed to avoid diving too deep into the mathematical concept of a relation\nin chapters 3 and 4, there\u2019s no way around covering some relational concepts now. We\nalready learned that yo u can not only query tables but views and functions too. You can\nalso query the result of another query. Tables , views, projections, and functions that act\nas tables are all relations. Relations are essentially a generalized version of tables andDefines an object but does \nnot execute a query yet Executes the query \nand prints the result\nExecutes the query and \nreturns a connection object Fetches one row \nfrom the connection Prints \nthat row126 CHAPTER  6Integrating with the Python ecosystem\nare composed of tuples and attributes instead of rows and columns. In a relational data-\nbase, a tuple is defined as a list of named\u2014and ordered\u2014attributes. Each tuple of a rela-\ntion corresponds to one record, and each a ttribute of a tuple corresponds to a column.\nThink of relations as more or less equivalent to a table. \n In essence, it boils down to the fa ct that relations pass the duck tests\u2014 If it looks like\na duck, swims like a duck, and quacks like a duck, then it probably is a duck.  If it looks like a\ntable, you can query it. While working in a database and running queries, this usuallyfeels quite natural, and you don\u2019t expect it  any other way. The DuckDB Python pack-\nage brings this concept into Python itse lf and lets you query different objects as\nthough they would be tables or views in a database. The line between having a per-\nsistent store with a relational schema and objects that just happen to behave like rela-\ntions is quite blurry here.\n6.2.1 Ingesting CSV data with the Python API\nSimilar to chapter 3, we have a bit of a hen-and-egg problem at hand when demon-\nstrating different ways of qu erying a database: without da ta, there\u2019s nothing to query.\nSo let\u2019s start again with ingesting data. We won\u2019t, however, create  a relational schema\nfor the data but instead, just use the provided objects as relations. \n In this section, we\u2019re going to revisit th e Populations CSV file introduced in chap-\nter 2. As before, our approach will involve the use of the httpfs extension. However,\nthis time around, we are not using the DuckDB SQL function read_csv  for file pro-\ncessing but the function  with the same name from the Python API\u2019s read_csv . \nNOTE In the example, we are using read_csv , as we would like to reuse the\nCSV file that we prepared in chapter 2, containing countries and their rele-\nvant statistics. The same  concepts and techniques apply as well to read_\nparquet  and read_json ; these functions exist both as SQL functions and as\nPython functions for DuckDB.\nWhile executing SQL commands in DuckDB is powerful, using the read_csv  function\ndirectly through the Python API offers seamless integration with Python-based work-flows. This method provides a \u201cpythonic\u201d approach to data manipulation, bridging the\ngap between SQL databases and Python data structures, making it a preferred choice\nfor Python-based projects. The object being returned can be treated as a queryable rela-\ntion right from within your Python code. In the code that follows, we will use the default\nin-memory database, but you can easily chan ge the connection, as shown previously.\nimport duckdb\ncon = duckdb.connect(database=':memory:')con.execute(\"INSTALL httpfs\")\ncon.execute(\"LOAD httpfs\")Listing 6.1 Querying a CSV file\nInstall the httpfs extension. \nYou only need to do this once.\nLoad the httpfs extension. You need to do \nthis each time you init ialize a new database.127 6.2 Using the relational API\npopulation = \\\ncon.read_csv(\"https://bit.ly/3KoiZR0\")\nThis won\u2019t print any data, but we get an  object back, which we did assign to\npopulation . We can check its type using Python\u2019s type  function, which returns the\nPython type of the population  variable:\ntype(population)\nThe result is as follows:\n<class 'duckdb.DuckDBPyRelation'>\nDuckDBPyRelation  is the centerpiece of DuckDB\u2019s relational API.  It\u2019s a queryable rela-\ntion and an API. \n If you want to query it, you can just use the execute  method the same way you\nwould execute any other query from within Python against DuckDB. execute  gives\nyou a new Python DB connection object, fr om which you can fetch the result, either\nvia fetchone  until there are no more results, or via fetchall , like in the following\nexample.:\ncon.execute(\"SELECT * from population limit 2\").fetchall()\nfetchall  will return the result as a list of two unformatted and difficult-to-read\nPython tuples, so we won\u2019t reproduce it he re. The relational API, however, is much\nmore and can be used as an alternative to wr iting SQL statements. It is essentially a flu-\nent API, which allows incremental constr uction of queries. It consists of DuckDB-\nPyRelation  nodes and relations that can be seen  as symbolic representations of SQL\nqueries. It supports the reuse of those nodes,  as well as set operations, filters, projec-\ntions, and aggregations. None  of the objects involved hold any data, and no query is\nexecuted until a trigger method , such as explicit fetch, sh owing, or similar is called.\n Let\u2019s count the number of records in ou r relational object. Unsurprisingly, the\nbuilder method to count records is called count . The result of this method is a relational\nobject again. The actual  query will not execute yet. We use the show  method in the exam-\nple to trigger the execution, but we could also just print the resulting object via print :\n(population\n.count(\"*\").show()\n)\nBoth the show  method and the implicit string repr esentation will give us a nice ren-\ndering of the resu lt, as shown here:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 227 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Read the \nCSV files.\nThis is the equivalent of\n SQL\u2019s SELECT count(*).128 CHAPTER  6Integrating with the Python ecosystem\nIf you have a slow internet connection, you may notice that it takes quite a long time\nto return the result. This is because the CSV file is being downloaded every time we\ncall the show  function, which isn\u2019t ideal! We can fix this problem by persisting a\nDuckDB table, using the to_table  function. This function will perform a Create\nTable as Select statement behind the scenes  on your behalf, creating a table named\npopulation , selecting from the relational object:\npopulation.to_table(\"population\")\nWe now have a table called population , which we can access using the table  function,\nlike this:\npopulation_table = con.table(\"population\")\nBear in mind that while the population  object created in listing 6.1 will always down-\nload and read the CSV anew, the table, once created, represents a snapshot of the data\nthat was available at that point in time. Th is isn\u2019t changed by the fact that you access\nthat table now through the relational API; it is not a view that would be recomputed\nwhen accessed. \nNOTE If you call type(population_table) , you will notice that it too is a\nDuckDBPyRelation , and it thus has the same traits and capabilities as before.\nWe can then rerun the code to count the number of countries, and this time, the\nresults will be displayed instantly:\npopulation_table.count(\"*\").show()\nWith the data now being held in a table in  memory, we are going to focus on compos-\ning queries with the relational API. \n6.2.2 Composing queries\nUp to this point, we\u2019ve primarily explored how to count records in a relation using the\nDuckDB API. However, this is just a glimpse into its capabilities. The API offers a suite\nof functions for the DuckDBPyRelation  to enhance data manipulation:\n\uf0a1filter \u2014Only include records that satisf y a provided predicate function.\n\uf0a1project \u2014Only return the specified columns.\n\uf0a1limit \u2014Return the first n records.\n\uf0a1aggregate \u2014Apply the provided aggregation expressions.\n\uf0a1order \u2014Sort the records by the provided columns.\nUsing the relational API as a builder so lves some problems people might run into\nwhen using plain SQL; there are many case s in which queries are generated or are\nbased on user input. While SQL allows for parameters in queries representing values,\nit does not allow table or column names to  be parameterized, so people often use\nsome kind of string concatenation for bu ilding queries that select from dynamic\ntables. The relational API offers an ad vantage for creating these queries in a129 6.2 Using the relational API\nprogrammatic fashion; instead of messing with  string concatenation, you can call ded-\nicated methods that are context aware. Th us, the likelihood of creating an invalid\nquery or a query that is prone to SQL inje ction decreases, and the code will be more\nreadable and composable.\n Let\u2019s have a look at what we can do if we combine a few of these functions. We\u2019ll\nstart by finding out which countries have a population of over 10,000,000 people,\nreturning the country and population for the first five countries. We can do this by\nusing the filter , project , and limit  functions. We only want  to include the first five\nrows that have a Population  greater than 10,000,000, and we are only interested in\nthe country name and the actual population:\n(population_table\n.filter('Population > 10000000')\n.project(\"Country, Population\").limit(5)\n.show()\n)\nWhen using the relational API, the order of  the operations won\u2019t affect the perfor-\nmance, as no results will be materialized in between. This  is different than the pandas\nintegration we are going to discuss in the fo llowing text. When a result set gets trans-\nformed to a pandas DataFrame, that Data Frame will have pand as characteristics,\nwhich usually means values will be computed  eagerly. The output  of running the pre-\nceding query is as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Country \u2502 Population \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Afghanistan \u2502 31056997 \u2502\n\u2502 Algeria \u2502 32930091 \u2502\n\u2502 Angola \u2502 12127071 \u2502\u2502 Argentina \u2502 39921833 \u2502\n\u2502 Australia \u2502 20264082 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIn our exploration, it\u2019s crucial to note th at the query does not get executed until the\nshow  function is invoked. The object returned  by the methods is still the same query\nbuilder. For example, you could still call offset  after limit  to specify the number of\nrows to skip. Now let\u2019s say we want to use another query that operates only on popula-tions with more than 10 million people. A simple approach would involve copying the\ncode and changing the filter criteria. Howe ver, this method lacks the elegance and\nefficiency inherent in the relational API of DuckDB. Instead of  copy\u2013pasting, we\u2019ll\ncreate a variable for the \nfilter  part of the query and create another relational object\nnamed over_10m . This relational object will be re used with several different queries,\nproviding a well-defined filter that ca n be seamlessly and coherently reused:\nover_10m = population_table.filter('Population > 10000000')This corresponds to the filter \ninside the WHERE clause.\nThis will eventually turn into a SELECT \nCountry, Population, which is called a projection in relational terms.130 CHAPTER  6Integrating with the Python ecosystem\nWe could then find the average population  of the medium to large-sized continents\nand regions, ordered by the largest population, using the aggregate  and order  func-\ntions. The relation over_10m  just acts like a table, a vi ew, or a common table expres-\nsion would act. In the following example, we use the aggregate  function too. This\nfunction takes in one expression, which can be made up of one or many calls to any\nS Q L  a g g r e g a t e  a s  w e l l  a s  z e r o  o r  m o r e  columns not being part  of an aggregate.\nDuckDB will automatically group by these columns, thus using Region  as the grouping\nkey while computing the average population:\n(over_10m\n.aggregate(\"Region, CAST(avg(Population) AS int) as pop\")\n.order(\"pop DESC\")\n)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 pop \u2502\n\u2502 varchar \u2502 int32 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 ASIA (EX. NEAR EAST) \u2502 192779730 \u2502\n\u2502 NORTHERN AMERICA \u2502 165771574 \u2502\n\u2502 LATIN AMER. & CARIB \u2502 48643375 \u2502\u2502 C.W. OF IND. STATES \u2502 48487549 \u2502\n\u2502 WESTERN EUROPE \u2502 38955933 \u2502\n\u2502 NORTHERN AFRICA \u2502 38808343 \u2502\u2502 NEAR EAST \u2502 32910924 \u2502\n\u2502 SUB-SAHARAN AFRICA \u2502 30941436 \u2502\n\u2502 EASTERN EUROPE \u2502 23691959 \u2502\u2502 OCEANIA \u2502 20264082 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows 2 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe result underscores the demographic heft of ASIA  (EX.  NEAR  EAST)  and NORTHERN\nAMERICA , with both regions signific antly surpassing others in  their average population.\n Alternatively, we might be interested in  the economic standing of these populous\nnations. By applying an additional filter  clause to the over_10m  relation, we zero in\non countries with a GDP per capita greater than $10,000:\n(over_10m\n.filter('\"GDP ($ per capita)\" > 10000')\n.count(\"*\")\n)\nThe result reveals that 20 countries from ou r previously filtered  set meet this eco-\nnomic benchmark, highlighting a subset of nations that are not only populous but\nalso have a relatively higher economic output per individual. In the previous twoexamples, we have been able to use the \nover_10m  relation unchanged to drive an\naggregate over regions and extended by an ad ditional filter; in both cases, we could\neasily reuse the or iginal definition:131 6.2 Using the relational API\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25022 0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWhen working with databases, there are often scenarios where interactions between\nmultiple relations are required. The relational  API provides a suite of functions specif-\nically designed for such multi-relation operations:\n\uf0a1except_ \u2014Returns all rows in the first relation that aren\u2019t in the second\n\uf0a1intersect \u2014Returns all rows that appear in both relations\n\uf0a1join \u2014Joins the relations on the provided keys or conditions\n\uf0a1union \u2014Combines relations, returning all rows  in the first relation followed by\nall rows in the second relation\nTo illustrate the capabilities of the except_  function in the relational API, consider\nthe following scenario: you have been assi gned the task of analyzing countries with\npopulations under 10 million. The following query achieves this by excluding coun-\ntries in the over_10m  relation, and then it further aggregates the results by region, cal-\nculating both the average population an d the number of countries in each:\n(population_table\n.except_(over_10m)\n.aggregate(\"\"\"\nRegion,\nCAST(avg(population) AS int) AS population,\ncount(*)\"\"\")\n)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 population \u2502 count_star() \u2502\n\u2502 varchar \u2502 int32 \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 EASTERN EUROPE \u2502 5426538 \u2502 9 \u2502\n\u2502 OCEANIA \u2502 643379 \u2502 20 \u2502\u2502 WESTERN EUROPE \u2502 2407190 \u2502 19 \u2502\n\u2502 LATIN AMER. & CARIB \u2502 2154024 \u2502 35 \u2502\n\u2502 C.W. OF IND. STATES \u2502 5377686 \u2502 7 \u2502\u2502 NEAR EAST \u2502 2773978 \u2502 11 \u2502\n\u2502 SUB-SAHARAN AFRICA \u2502 3322228 \u2502 30 \u2502\n\u2502 NORTHERN AMERICA \u2502 43053 \u2502 3 \u2502\u2502 ASIA (EX. NEAR EAST) \u2502 2796374 \u2502 9 \u2502\n\u2502 BALTICS \u2502 2394991 \u2502 3 \u2502\n\u2502 NORTHERN AFRICA \u2502 3086881 \u2502 2 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 11 rows 3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Include records that aren\u2019t \nin the over_10m relation.\nGrouping by region, compute \nthe average population, and \ncount the number of records.132 CHAPTER  6Integrating with the Python ecosystem\nThe resulting table provides a breakdown by  region, showcasing the average popula-\ntion and number of countries with fewe r than 10 million inhabitants. Notably,\nEASTERN  EUROPE  stands out, with an average popu lation of approximately 5.4 million\nacross 9 countries, while OCEANIA  has a smaller average po pulation but encompasses\n2 0  c o u n t r i e s .  T h i s  d a t a  o f f e r s  a  n u a n c e d  view of regions with smaller countries in\nterms of population.\n Exploring further, let\u2019s consider a more sp ecific subset of countries: those located\nin Eastern Europe with populations exceedin g 10 million. To achieve this, we initiate\nby filtering out countries within the region of EASTERN  EUROPE . Take note that we are\nusing the POSIX-style operator ~ for filtering with a regula r expression\u2014in this case,\nan expression that finds all reco rds with a region that contains EASTERN  EUROPE :\neastern_europe = population_table \\\n.filter(\"Region ~ '.*EASTERN EUROPE.*'\")\nHaving established the eastern_europe  relation, the next step involves pinpointing\nthose countries that intersect with our previously defined over_10m  relation:\n(eastern_europe\n.intersect(over_10m)\n.project(\"Country, Population\")\n)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Country \u2502 Population \u2502\n\u2502 varchar \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Czech Republic \u2502 10235455 \u2502\n\u2502 Poland \u2502 38536869 \u2502\u2502 Romania \u2502 22303552 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe output table distinctly lists three countries from Eastern Europe\u2014Czech Repub-\nlic, Poland, and Romania\u2014each with a popul ation that breaches the 10 million mark.Pattern matching\nDuckDB supports four ways of pattern matching:\n1LIKE testing whether a string matches a pattern as a whole, allowing % and _\nas wildcards\n2SIMILAR  TO testing whether a string matches a regular expression as a whole\n3GLOB testing whether a string matches a GLOB pattern, which is useful when\nsearching for filenames that  follow a specific pattern\n4Generally applicable regular expression via functions\nThere are shorthands for the SQL operators: ~~ for LIKE, ~ for SIMILAR  TO, and ~ for\nGLOB.\nKeep relations that are both in \nEastern Europe and have a \npopulation of more than 10 million.\nReturn only the Country and \nPopulation fields.133 6.2 Using the relational API\nThis refined analysis underscores the power of DuckDB\u2019s relational API in catering to\nspecific data requirements. \n6.2.3 SQL querying\nRelational objects can be treated with the sa me flexibility as SQL tables. This capabil-\nity allows for a fluid transition between Python-based data operations and SQL-like\nquerying. Taking a practical example into co nsideration, let\u2019s say our objective is to\nascertain the count of medium- to large-si zed nations, where the GDP per capita sur-\npasses $10,000. The process for achieving this is illustrated as follows:\ncon.sql(\"\"\"\nSELECT count(*)FROM over_10m\nWHERE \"GDP ($ per capita)\" > 10000\n\"\"\")\nWhile the relational API of Du ckDB offers a vast array of functionalities, there are cer-\ntain scenarios, particularly those involv ing parameterized quer ies, where its innate\ncapabilities might feel restrictive.\nNOTE A parameterized query  uses placeholders in the statement text. The state-\nment text and the actual values fo r the placeholders\u2014the parameters\u2014are\npassed independently to the engine. Usin g parameters has a couple of advan-\ntages: user input, for example, should  always be passed as a parameter and\nnever put into the statement text directly . If you just concatenate string frag-\nments of a statement with user input,  you have a high risk of so-called SQL\ninjection attacks , in which a specially crafted text alters the semantics of your\nquery. When passed as parameters to the query engine, the engine will treat\ninput in such a way that a string will  never mess up a query. Additionally, a\nparameterized statement may need to be parsed only once by the queryengine, as its content is constant and only parameters change. \nParameterized queries allow for dynamic input , making the queries both reusable and\ns e c u r e  b y  m i t i g a t i n g  S Q L  i n j e c t i o n  r i s k s .  T o  b r i d g e  t h i s  g a p  i n  t h e  r e l a t i o n a l  A P I ,\nDuckDB provides the \nexecute  function, adhering to the Python DB-API 2.0. This\nfunction not only facilitates the use of pa rameterized queries but also yields a connec-\ntion object. The latter adheres to the Py thon Database API Specification v2.0 and\nexposes all the methods you expect; once th e query execution is done, extracting the\nresults can be approached in various wa ys. Commonly used methods in the Python\nDB-API 2.0 are fetchall  and fetchone . The former retrieves all rows, and the latter\nretrieves the values of the first re cord in the result set. We use fetchone  here, as the\ncount  aggregate used in the query returns on ly one row, and we can spare ourselves\nthe effort of iterating over a list of rows:\ncon.execute(\"\"\"\nSELECT count(*)\nFROM over_10mWe\u2019re referencing over_10m, \nwhich is a variable, but it\u2019s \ntreated like a table in the query.134 CHAPTER  6Integrating with the Python ecosystem\nWHERE \"GDP ($ per capita)\" > $gdp\n\"\"\", {\n\"gdp\":10000\n}).fetchone()\nThe result, (20,) , indicates that 20 countries meet the criteria. The more relevant fact\nis this: the query we passed to execute  does use named parameters, and it looks like\nany other prepared statemen t. It does, however, query a relational object, over_10m ,\nnot an actual table. The relational object might have been built in such a way that it\nused parameters as well, either for the co lumns that should be projected or for the\nunderlying tables to be queried. Thus, yo u avoid using string concatenation in all\ncases\u2014the relational API and the standard DB-API complement each other here. The\nDB-API can only execute well-formed SQL, which does not allow tables and columns\nto be parameterized. The relational API al lows it, and the relational objects can then\nbe queried like any other table. \n6.3 Querying pandas DataFrames\nDuckDB\u2019s popularity in the data ecosystem is largely attributed to its robust query\nengine. Not limited to just file-based operat ions, this engine seamlessly integrates with\nin-memory data structures from  various data infrastructure tools. One notable tool in\nthis domain is pandas , a well-established open source data analysis library widely used\nin the data community. The combination of DuckDB and pandas allows for powerfuldata operations, bridging the capabilities of a database engine with the flexibility of a\ndata manipulation library. In this sectio n, we\u2019ll explore how to employ DuckDB to\nquery pandas DataFrames. To kick things off, we need to ensure pandas is installed inour environment:\npip install pandas\nNext, import the pandas and DuckDB libraries:\nimport duckdb\nimport pandas as pd\nWith the libraries in place, we proceed to create a pandas  DataFrame that holds infor-\nmation about your authors:\npeople = pd.DataFrame({\n\"name\": [\"Michael Hunger\", \"Michael Simons\", \"Mark Needham\"],\n\"country\": [\"Germany\", \"Germany\", \"Great Britain\"]\n})\nThe DataFrame people  now contains data about three authors and the countries they\nlive in. One of the salient features of DuckDB is its ability to interact directly with pandas\nDataFrames; they are treated just like any re lation we\u2019ve dealt with before. This means\nwe can run SQL-like queries on people , just as we would a regular database table:The GDP per capita filter is set \nbased on the $gdp parameter.\nDefines the parameters \nfor the query\nFetches the results as a tuple\ncontaining the firs t record\u2019s values135 6.3 Querying pandas DataFrames\nduckdb.sql(\"\"\"\nSELECT *\nFROM peopleWHERE country = 'Germany'\n\"\"\")\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name \u2502 country \u2502\n\u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Michael Hunger \u2502 Germany \u2502\n\u2502 Michael Simons \u2502 Germany \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe result of the query gives us two authors living in the country Germany . This exam-\nple illustrates the seamless inte gration and querying capabilities of DuckDB inside the\nPython ecosystem; it does not matter whethe r the relation has been created as a table,\nsourced from a file, or represented by a pandas DataFrame.\n To query for people not living in Germ any, we could use a parameterized query as\nin this example. Take note that we don\u2019t use execute.fetchall  or execute.fetchone\nbut fetchdf . This returns the result as a DataFrame instead of a Python DB-API cur-\nsor. Whether you would use a cursor to iterate the rows or a DataFrame depends on\nyour use case\u2014the cursor is more lightwe ight, while the DataFrame integrates much\nmore efficiently in further analysis with pandas. DuckDB will take care of the dull\nwork, creating a DataFrame from a result set for you:\nparams = {\"country\": \"Germany\"}\nduckdb.execute(\"\"\"SELECT *\nFROM people\nWHERE country <> $country\"\"\", params).fetchdf()\nname country\n0 Mark Great Britain\nIf the relational API suits your needs be tter, you can query the DataFrame as such:\n(duckdb.sql(\"FROM people\")\n.filter(\"country <> 'Germany'\").show()\n)\nExtending this capability, DuckDB also facilitates querying other in-memory data\nstructures, like Polars DataFrames and Py Arrow tables. The mechanics are akin to\nwhat we\u2019ve seen with pandas. Detailed ex amples and implementations for these data\nstructures can be found in the book\u2019 s accompanying GitHub repository.\nNOTE The SQLAlchemy driver duckdb_engine  supports querying DataFrames\nas well. You can register them on an instance of the driver (see https:/ /\nmng.bz/EZpj ). Creates a new relational \nobject from the DataFrame136 CHAPTER  6Integrating with the Python ecosystem\n6.4 User-defined functions\nAs data practitioners, we often encounter scenarios where pre-existing database func-\ntions might not cater to our specific need s. Recognizing this, DuckDB has a powerful\nfeature: the ability to create user-defined functions  (UDFs) within its Python package.\nThe beauty of UDFs lies in their ability to  extend the SQL language\u2019s native capabili-\nties. With UDFs, users can define their ow n custom functions, benefitting from the\nvast ecosystem of Python libraries. Whethe r it\u2019s complex data manipulations, mathe-\nmatical calculations, or even integrations wi th external tools and APIs, the possibilities\nbecome virtually limitless. In practical terms, this means that  if you\u2019ve ever wished for\na specific function while writing an SQL qu ery in DuckDB, you can now create it using\nPython and then subsequently in voke it within your SQL code. \n Data ingestion, while a fundamental step in any data analysis pipeline, often comes\nwith its own set of challenges. Raw data  can be messy, and it\u2019s not uncommon to\nencounter errors that need rectification be fore any meaningful analysis can be con-\nducted. In our current scenario, the CSV file ingested at  the beginning of this chapter\nhas presented an anomaly that needs addressing.\n A discernible problem lies in the Region  field of our dataset. This field seems to be\npadded with excessive space characters, maki ng data processing and analysis cumber-\nsome. To visualize the extent of this pr oblem, consider the fo llowing query, which\nretrieves unique values of the Region  field and computes the total character length\nfor each distinct Region :\ncon.sql(\"\"\"\nselect DISTINCT Region, length(Region) AS numCharsfrom population\n\"\"\")\nFrom the following table, we can see that ther e are lots of trailing spaces, which is con-\nfirmed by the numChars  column:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 numChars \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 LATIN AMER. & CARIB \u2502 23 \u2502\n\u2502 ASIA (EX. NEAR EAST) \u2502 29 \u2502\n\u2502 EASTERN EUROPE \u2502 35 \u2502\u2502 WESTERN EUROPE \u2502 35 \u2502\n\u2502 NEAR EAST \u2502 35 \u2502\n\u2502 C.W. OF IND. STATES \u2502 20 \u2502\u2502 SUB-SAHARAN AFRICA \u2502 35 \u2502\n\u2502 OCEANIA \u2502 35 \u2502\n\u2502 NORTHERN AFRICA \u2502 35 \u2502\u2502 BALTICS \u2502 35 \u2502\n\u2502 NORTHERN AMERICA \u2502 35 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 11 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Returns the region and \nnumber of characters137 6.4 User-defined functions\nFor instance, while BALTICS  comprises just 7 characters , its total length, including\ntrailing spaces, is 35 characters. Of cour se, DuckDB has a built-in SQL function trim ,\nbut we are going to build our own version as an easy example that allows us to put the\nfocus on how to define a UDF, instead of  distracting with implementation details:\ndef remove_spaces(field:str) -> str:\nif field:\nreturn field.lstrip().rstrip()\nelse:\nreturn field\nThe function, aptly named remove_spaces , is designed to trim spaces from both the\nbeginning and the end of the given string. No tice the type annotations: they signify\nthat the function expects a string in put and will also return a string. \n Once our function is defined, we need to register it with DuckDB:\ncon.create_function('remove_spaces', remove_spaces)\nTo confirm its registra tion, you can query the duckdb_functions  SQL function:\ncon.sql(\"\"\"\nSELECT function_name, function_type, parameters, parameter_types, return_typefrom duckdb_functions()\nwhere function_name = 'remove_spaces'\n\"\"\")\nThe outcome is a table that provides meta-information about our function:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502function_name \u2502function_type \u2502 parameters \u2502 parameter_types \u2502 return_type \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar[] \u2502 varchar[] \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502remove_spaces \u2502scalar \u2502 [col0] \u2502 [VARCHAR] \u2502 VARCHAR \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nYou can call this function now in any SQL statement issued on the connection in\nwhich you defined the function:\ncon.sql(\"select length(remove_spaces(' foo '))\")\nWe wrapped it in a call to length , returning the length of the new string; otherwise,\nspotting that the leading and trailing spaces  have been trimmed might be difficult in\nthe output of the Python program. \n SQL is a typed language, so DuckDB needs to know the types of both parameters and\nreturn types of a function. DuckDB is usuall y able to infer those types from the Python\ntype annotations. This inference capability, however, might not always be spot-on, espe-\ncially if we had decided to not use Python type  hints in our code or if the function is dis-\ntributed in a third-party library without ty pe hints. In such sc enarios, it becomes\nnecessary to explicitly define the types to ensure accurate function execution.Defines a Python function\nwith type annotations\nChecks that the \nfield isn\u2019t null\nStrips spaces from the \nbeginning and en d of the value138 CHAPTER  6Integrating with the Python ecosystem\n Explicitly specifying types is beneficial for clarity, preventing potential type infer-\nence pitfalls, and ensuring consistent behavior across different environments. To\nredefine our function with expl icit types, we first need to remove the previously regis-\ntered version of the function to avoid conflicts:\ncon.remove_function('remove_spaces')\nHaving done that, we can now reregister ou r function, but this time, we\u2019ll explicitly\ndefine the types. As shown in the following snippet, the function\u2019s parameter type and\nreturn type are distinctly specified:\n\uf0a1The function expects a single input parameter of type VARCHAR .\n\uf0a1It returns a value of type VARCHAR :\nfrom duckdb.typing import *\ncon.create_function(\n'remove_spaces',remove_spaces,\n[(VARCHAR)],\nVARCHAR\n)\nBeing explicit in such definitions can serv e as a clear contract, stipulating how the\nfunction should be used and what to expe ct in return, and ensuring the system and\nthe developers are in sync.\n Next, let\u2019s use this function to write a query showing what we\u2019ll see if we remove\nthe spaces from the Region  column, shown in the following snippet. This query does\ntwo things:\n\uf0a1Displays the original Region  values along with their character lengths ( len1 )\n\uf0a1Showcases the cleaned Region  values (using remove_spaces ) and their charac-\nter lengths ( len2 ):\ncon.sql(\"\"\"\nSELECT DISTINCT Region, length(Region) AS len1,\nremove_spaces(Region) AS cleanRegion,\nlength(cleanRegion) AS len2\nFROM population\nWHERE len1 BETWEEN 20 AND 30\nLIMIT 3\"\"\")\nA glance at the difference between len1  and len2  immediately makes it evident that\nour function does, in fact, tr im those unwarranted spaces:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 len1 \u2502 cleanRegion \u2502 len2 \u2502\u2502 varchar \u2502 int64 \u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ASIA (EX. NEAR EAST) \u2502 29 \u2502 ASIA (EX. NEAR EAST) \u2502 20 \u2502\u2502 LATIN AMER. & CARIB \u2502 23 \u2502 LATIN AMER. & CARIB \u2502 19 \u2502\n\u2502 C.W. OF IND. STATES \u2502 20 \u2502 C.W. OF IND. STATES \u2502 19 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The function has one \nVARCHAR input parameter.\nThe function returns \na VARCHAR.\nReturns the region and \nthe number of characters\nReturns regions with no spaces \nand the number of characters\nLimited for \nbrevity of output139 6.4 User-defined functions\nWith the confidence that our function is working as expected, we can then proceed to\nupdate the original dataset:\ncon.sql(\"\"\"\nUPDATE populationSET Region = remove_spaces(Region);\"\"\")\nAnd once that\u2019s done, let\u2019s return the uniq ue regions and the number of characters:\ncon.sql(\"\"\"\nselect DISTINCT Region, length(Region) AS numCharsfrom population\"\"\")\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Region \u2502 numChars \u2502\n\u2502 varchar \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ASIA (EX. NEAR EAST) \u2502 20 \u2502\n\u2502 EASTERN EUROPE \u2502 14 \u2502\u2502 NORTHERN AFRICA \u2502 15 \u2502\u2502 OCEANIA \u2502 7 \u2502\n\u2502 WESTERN EUROPE \u2502 14 \u2502\n\u2502 SUB-SAHARAN AFRICA \u2502 18 \u2502\u2502 LATIN AMER. & CARIB \u2502 19 \u2502\n\u2502 C.W. OF IND. STATES \u2502 19 \u2502\n\u2502 NEAR EAST \u2502 9 \u2502\u2502 NORTHERN AMERICA \u2502 16 \u2502\n\u2502 BALTICS \u2502 7 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 11 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe spaces are gone! Now we have a much cleaner dataset. The extra spaces around\neach Region  value have been eliminated, bringi ng more structure and uniformity to\nour data.\n Working with data from diverse source s often introduces challenges stemming\nfrom regional disparities. An excellent exam ple of this is the representation of deci-\nmal numbers. In the European  region, a comma (,) is typically used as the decimal\nseparator, as opposed to the period (.) used in other regions. When ingesting data\ninto databases, these locale-specific notati ons can introduce complexity, especially if\nthe system\u2019s locale doesn\u2019t align with the data\u2019s format.\n For our dataset in DuckDB, we\u2019ve encountered such a challenge with fields repre-\nsenting decimal values. Due to the European format of these values, DuckDB hasinterpreted them as \nVARCHAR s, potentially hindering numerical analyses. \n To rectify this, we can make use of Python\u2019s extensive library ecosystem. The locale\nmodule offers a solution to this particul ar challenge. We can define a function,\nconvert_locale , that will transition these European-formatted decimal values into a\nformat DuckDB can interpret as numeric types:140 CHAPTER  6Integrating with the Python ecosystem\nfrom duckdb.typing import *\nimport locale\ndef convert_locale(field:str) -> float:\nlocale.setlocale(locale.LC_ALL, 'de_DE')return locale.atof(field)\nHaving defined our function, the next step is to make DuckDB aware of its existence.\nRegistering this function allows us to use it in our SQL queries:\ncon.create_function('convert_locale', convert_locale)\nTo visualize the efficacy of this function, le t\u2019s apply it to a couple of columns, namely\nCoastline (coast/area ratio)  and Pop. Density (per sq. mi.) :\ncon.sql(\"\"\"\nSELECT \"Coastline (coast/area ratio)\" AS coastline,\nconvert_locale(coastline) as cleanCoastline,\"Pop. Density (per sq. mi.)\" as popDen,\nconvert_locale(popDen) as cleanPopDen\nFROM populationLIMIT 5\"\"\")\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 coastline \u2502 cleanCoastline \u2502 popDen \u2502 cleanPopDen \u2502\u2502 varchar \u2502 double \u2502 varchar \u2502 double \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0,00 \u2502 0.0 \u2502 48,0 \u2502 48.0 \u2502\u2502 1,26 \u2502 1.26 \u2502 124,6 \u2502 124.6 \u2502\n\u2502 0,04 \u2502 0.04 \u2502 13,8 \u2502 13.8 \u2502\n\u2502 58,29 \u2502 58.29 \u2502 290,4 \u2502 290.4 \u2502\u2502 0,00 \u2502 0.0 \u2502 152,1 \u2502 152.1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nUpon examining the results, the distinctio n between the original  and cleaned values\nis evident. Our function has su ccessfully converted values like 0,00  to 0.0. It has done\nso not by blindly replacing a colon with a dot but with semantic awareness that it is\ndealing with a localization problem.\n Once confident in the function\u2019s operat ion, it\u2019s prudent to make these changes\npermanent in our dataset. The ALTER TABLE  clause allows us to modify column types\nand update values simultaneously:\ncon.sql(\"\"\"\nALTER TABLE population\nALTER \"Coastline (coast/area ratio)\"SET DATA TYPE DOUBLE\nUSING\nconvert_locale(\"Coastline (coast/area ratio)\")\n\"\"\")\nThis process underscores the importance of understanding and adapting to regional\ndata nuances. It also highli ghts the flexibility and integr ation capabilities of DuckDB,Updates the data \ntype to double Uses our convert_locale \nfunction to update all the values in this column141 6.5 Interoperability with Apache Arrow and Polars\nallowing users to bridge the gap between Python\u2019s vast library ecosystem and SQL-\nbased data manipulation. \nNOTE We\u2019ll leave repeating this process for the other columns that need\ncleaning up as an exercise for the re ader. The following columns still need to\nbe cleaned up: Pop.  Density  (per  sq. mi.) , Coastline  (coast/area  ratio) ,\nBirthrate , and Deathrate .\n6.5 Interoperability with Apache Arrow and Polars\nIn the realm of data analysis, adaptability is  a defining quality of a robust system. The\nability to seamlessly tr ansition between different format s or platforms allows for effi-\ncient data manipulation, storage, and visual ization. One of DuckDB \u2019s strengths is its\ncapability to interact with a diverse arra y of data formats, both in-memory and exter-\nnal. This interoperability is often invaluab le, especially when integrating with other\ntools or exporting result s for further analysis. \n Now, while the data science ecosystem is replete with tools, there\u2019s a constant evo-\nlution of libraries that offer improved performance or unique features. One such\nemerging star is Polars. Though pandas has been the de facto standard for data analy-\nsis in Python for many years, Polars presents  itself as an exciti ng alternative. Devel-\noped in Rust\u2014a language known for its performance characteristics\u2014Polars offers\nDataFrame operations that are both fast and memory efficient. The memory model\nused by Polars is based upon Apache Ar row, a cross-language development platform\nfor in-memory data that specifies a standa rdized and language-independent columnar\nmemory format for flat and hierarchical data. Arrow allows for zero-copy reads and\nfast data access and interchange without serialization overhead between languages\nand systems. As a matter of fact, Polars is not the only framework that uses Arrows as\nan in-memory format: pandas DataFrames an d other Python libraries, such as NumPy\nand PySpark, do too.\n Given our prior exploration of pandas in this chapter, it might be enlightening to\ntake Polars for a spin to give yourself a chance to experience the nuances and advan-\ntages it brings to the table firsthand. Both libraries must be installed into your Python\nenvironment for the DuckDB integration to work. You want to execute the following\ncommand in a different shell without quitti ng the running Python interpreter so that\nthe objects and relations we defined earlier are kept intact:\npip install polars pyarrow\nWe can then convert the population  table to Polars using the pl function and select\nsome columns from the first five rows:\nimport polars\npopulation_table = con.table(\"population\")\n(population_table\n.limit(5).pl()Selects the first five rows \n(using the relational API) Converts the population \ntable to Polars DataFrame142 CHAPTER  6Integrating with the Python ecosystem\n[[\"Country\", \"Region\", \"Population\"]]\n)\nThe output of this code fragment is as follows:\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 Country | Region | Population \u2502\n\u2502 --- | --- | --- \u2502\n\u2502 str | str | i64 \u2502\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Afghanistan | ASIA (EX. NEAR EAST) | 31056997 \u2502\n\u2502 Albania | EASTERN EUROPE \u2026 | 3581655 \u2502\u2502 Algeria | NORTHERN AFRICA \u2026 | 32930091 \u2502\n\u2502 American Samoa | OCEANIA \u2026 | 57794 \u2502\n\u2502 Andorra | WESTERN EUROPE \u2026 | 71201 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nExecuting this code results in a concise Polars DataFrame with just the Country ,\nRegion , and Population  columns for the first five entries. As the output suggests, the\ntransition from DuckDB to Polars is seam less; the DataFrame can partake in any com-\nputation Polars offers. In your code, you will want to delay the transition as long as\npossible, though. We could have limited the results to five rows by converting to a\nDataFrame first and then using head(5)  to get the first five rows; this would have\nmaterialized all rows into the Python runtime followed by a client-side transformation.\nOur recommendation for both Polars and pandas is to stick to the relational or data-\nbase API as long as possible and convert to a DataFrame only if you need to combine it\nwith external data or if a computatio n via SQL would just not be feasible.\n We could also convert the DuckDB table to an Apache Arrow table to take advan-\ntage of the myriad tools and platforms that  support Arrow. To do the conversion, we\ncan use the to_arrow_table  function:\narrow_table = population_table.to_arrow_table()\nAfter transforming our DuckDB table into Arrow\u2019s format, we\u2019re now in a position to\nharness the computational capa bilities of Arrow\u2019s Python API. Suppose we are inter-\nested in countries from the NEAR EAST  region. We\u2019d like to retrieve the country ,\nregion , and population  of each of the top five NEAR EAST  entries. In the following\nsnippet, we\u2019ll complete the following operations:\n\uf0a1Filtering \u2014We\u2019re keen on countries that fall within the NEAR EAST  region. This is\nour primary criterion for data extraction.\n\uf0a1Column selection \u2014For our analysis, we requ ire just three columns: Country ,\nRegion , and Population .\n\uf0a1Row limitation \u2014To keep our output concise, we\u2019ll limit it to the top five entries:\nimport pyarrow.compute as pc\n(arrow_table\n.filter(pc.field(\"Region\") == \"NEAR EAST\")Extracts the Country, Region, and \nPopulation for the first five rows\nOnly includes rows with a \nregion in the NEAR EAST143 Summary\n.select([\"Country\", \"Region\", \"Population\"])\n.slice(length=5)\n)\nThe resulting table is a reflection of our sp ecifications: compact, focusing only on the\ndesired columns, and limited to the to p five entries. The countries listed\u2014 Bahrain ,\nCyprus , Gaza Strip , Iraq , and Israel \u2014are representative of the NEAR EAST  region,\nwith their respective popu lations displayed alongside:\npyarrow.Table\nCountry: string\nRegion: stringPopulation: int64\n------\nCountry: [[\"Bahrain \",\"Cyprus \",\"Gaza Strip \",\"Iraq \",\"Israel \"]]Region: [[\"NEAR EAST\",\"NEAR EAST\",\"NEAR EAST\",\"NEAR EAST\",\"NEAR EAST\"]]\nPopulation: [[698585,784301,1428757,26783383,6352117]]\nThose examples not only showcase the capabilities of Apache Arrow and Polars but\nalso its potential to bridge  the divide between data storage and computation. Having\nthe ability to transform any result into an  Apache Arrow object opens up the possibil-\nity of using all streaming, serialization, and interprocess communication  (IPC) based on\nArrow. \nSummary\n\uf0a1DuckDB\u2019s Python API blurs the lines betw een tables and views in a database and\nobjects outside.\n\uf0a1DuckDB enables many kind s of objects to be quer ied in a uniform fashion.\n\uf0a1Polar DataFrames, pandas DataFrames, Ap ache Arrow tables, and other sources\ncan be treated as if they were a table in an SQL query.\n\uf0a1The relational API makes it easy to wr ite maintainable applications utilizing\nreusable quer y fragments.\n\uf0a1The relational API allows the reuse of dynamic relations in a similar way to what\na view would do statically.\n\uf0a1User-defined functions allow us to implem ent the functionality of the entire set\nof libraries in the Python ecosystem, integrating seamlessly within SQL queries,\nbringing more flexibility and a tailored wo rkflow to any data analysis experience.\n\uf0a1Data can be exported to a variety of formats, including pandas DataFrames,\nPolars DataFrames, and Apache Arrow tables.\n\uf0a1Transitioning between various platfor ms becomes a lot easier with DuckDB\u2019s\nconversion capabilities that treat Data Frames, Apache Arrow tables, and other\nsources the same way. Returns the Country, \nRegion, and Population Returns the \nfirst five rows144DuckDB in the cloud\nwith MotherDuck\nUp until this point, our focus has been on  using DuckDB\u2019s capa bilities for querying\ndatasets\u2014whether they\u2019re stored locally or remotely\u2014directly from our own com-\nputers. While this approach addresses a broad range of needs, there are specific\nscenarios in which a remote database server offers additional advantages.\n Enter MotherDuck: a solution for enha ncing SQL analytics through a simplified\nscale-up strategy. In this chapter, we will learn how MotherDuck enables hybrid\nquery execution, making use of a remote ly hosted DuckDB alongside one operat-\ning on our own machine.This chapter covers \n\uf0a1The idea behind MotherDuck\n\uf0a1Understanding how the architecture works under \nthe hood\n\uf0a1Use cases for serverless SQL analytics\n\uf0a1Creating, managing, and sharing MotherDuck databases\n\uf0a1Tips for optimizing your MotherDuck usage145 7.1 Introduction to MotherDuck\n7.1 Introduction to MotherDuck\nMotherDuck  (https:/ /motherduck.com ) is a collaborative serv erless analytics platform\nthat lets you query and analyze data in cl oud databases and from cloud storage, using\nyour browser or any of the DuckDB APIs. Serverless , in this context, means that you as a\nuser won\u2019t have to deal with spinning up  servers, clusters, or  configuring database\ninstances. Instead, you can just create a data base, and the service will take care of the\nrest for you.\n A closed beta of the platform was launched in June 2023, with general availability\nintroduced in September 2023. MotherDu ck works closely with the DuckDB Labs\nteam to ensure the best interoperability an d availability of all features in the cloud\nplatform. You can find the documentat ion for the MotherDuck service at https:/ /\nmotherduck.com/docs/ .\n7.1.1 How it works\nThere are several ways to us e MotherDuck. When you sign up for the service, you\u2019ll\nend up in the MotherDuck web UI running in  the browser. This UI is running a spe-\ncial DuckDB version in the browser that  knows how to communicate with Mother-\nDuck. The UI is both a tool to manage your MotherDuck databases and a notebook-based approach to enter and execute queries and to view their results. We will cover\nthis later in the chapter in section 7.2.1. \n The other entry points to MotherDuck ar e, of course, the CLI and the integrations\nwith languages, such as Python. MotherDuck is presented as an opt-in feature to the\nopen source database DuckDB via an extension. This extension will be automatically\nloaded when you open a database using the \nmd: or motherduck:  protocol and will\nintegrate both with the query parser and en gine. The parser is enhanced with func-\ntionality around database and share manage ment. In the query engine, the extension\nanalyzes if tables are available locally or remotely and then uses the appropriate exe-\ncution engine and joins the data accordingly. If needed, parts of the local data aresent to the server for joins or filters, or data from the remote side is fetched and\njoined locally.\n MotherDuck\u2019s architecture is shown in figure 7.1, and at the core of its operation\nare the following components:\n\uf0a1A service layer \u2014Sharing, admin, services, and monitoring\n\uf0a1Ducklings \u2014Serverless DuckDB compute instances\n\uf0a1A catalog \u2014Database and tables\n\uf0a1Storage \u2014Internal storage and maintenance\nThe service layer of MotherDuck provides ca pabilities like secure identity, authoriza-\ntion, administration, monitoring, billing , and so on. Where the serverless DuckDB\n\u201cDuckling\u201d instances execute the \u201cremote\u201d parts of your query, the catalog exposes146 CHAPTER  7DuckDB in the cloud with MotherDuck\nthe databases, tables, and views that are ma naged in the storage layer. The storage is\ndurable, secure, and automatically optimi zed for best performance. MotherDuck\u2014as\nwith other modern cloud data platforms\u2014 separates storage an d compute facilities,\nwhich will eventually be important for the cost that occurs when using MotherDuck.\nFigure 7.1 Hybrid query execution as implemented within MotherDuck\nMotherDuck is designed so that you can focu s on your queries, not on the size of the\nmachines you need to spin up in the cloud to make the queries run quickly. The sepa-\nration of the storage\u2014and compute\u2014layers wi ll have some impact on the cost; we will\ndiscuss this later in the chapter in section 7.3.\n7.1.2 Why use MotherDuck?\nFirst and foremost, MotherDuck provides a simplified, performant, and efficient data\nwarehouse based on DuckDB. Most folks are in the long tail of cloud data warehouse\nusers and don\u2019t need analytics-processing ca pabilities for tens or hundreds of tera-\nbytes of hot data. Those use cases can benefi t from a simpler, more efficient architec-\nture that is not based on a distributed sy stem but instead uses the cloud as the main\ndata storage and DuckDB as the query engine . Think back to the fictional system pre-\nsented in chapters 3 and 4, monitoring en ergy production. Even if you monitored sev-\neral hundreds or thousand s of sites and their daily output in quarterly hour\nmeasurements, you would not find anything close to a billion records per year, and\nmost likely, they would only total a few giga bytes in size. This could hardly be consid-\nered big data, and it\u2019s something MotherDu ck would be able to deal with easily. \n Another way to utilize MotherDuck is as a query engine for data lakes made up of\nheterogeneous source s, such as cold data stored as Parquet, CSV files in S3, or data\nWeb UI\nNotebooks, SQL IDE, and\ninteractive results \nexplorer\nPython DuckDB\nLocal compute and storage\nCLI DuckDB\nLocal compute and storageService Layer\nSharing, admin, secrets,\nand monitoring\nDucklings\nServerless DuckDB \ncompute standard SQL\nCatalog\nTables and databases\nStorage\nInternal storage and \ntransparent maintenanceDatabases\nPostgres and SQLiteS3\nParquet, CSV and JSON\nExternal\nquery/\neasy copyHybrid\nquery\nexecutionManaged DuckDB in the Cloud147 7.2 Getting started with MotherDuck\nstored in Apache Iceberg. You can easily join that data with hot data stored directly in\nMotherDuck.\n MotherDuck can serve as a serverless ba ckend for data applications, dashboards,\nand APIs. Instead of running analytics queries on the main  transactional database, you\ncan use MotherDuck to run those querie s on a dedicated analytics database.\n Last but not least, MotherDuck allows sharing read-only snapshots of your data-\nbases to other MotherDuck users. They can use your sources, as they are shared, or\njoin data from their instance together with your datasets as well.\n Independent of what you are planning to  do with MotherDuck, you will need to\nsign up with the MotherDuck service to use their cloud offering. We will cover this in\nthe next section. \n7.2 Getting started with MotherDuck\nThroughout the book, we used the CLI or th e integration with Python. To get started\nwith MotherDuck, you will need to bring up yo ur browser of choice first. Navigate to\nhttps:/ /motherduck.com/ , and click on the Sign Up button. You can create a free\naccount with your GitHub account, with your Google account, or by providing anemail address. Once you\u2019ve done that, you\u2019ll find yourself in the MotherDuck UI. TheUI shown in figure 7.2 displays your databa ses and their schema in  a navigable tree to\nthe left, and a query and its result, including inline bar-charts, in the main view. \nFigure 7.2 MotherDuck UI\n148 CHAPTER  7DuckDB in the cloud with MotherDuck\n7.2.1 Using MotherDuck through the UI\nThe web-based MotherDuck UI ( https:/ /app.motherduck.com ) provides a central\nplace for accessing and queryi ng all your remote database s, managing your account\nsettings, and storing any secrets necessary fo r querying remote data sources on S3. It\nalso gives access to your Mo therDuck API token, which is  needed to access Mother-\nDuck outside the UI. \nNOTE The database running in the web UI  is actually a local, embedded\ndatabase too! It is using a version of  Duck that is compiled to WebAssembly\n(WASM), thus running locally in your browser. The queries are executed, of\ncourse, in the MotherDuck cloud, as ex plained earlier. Alternatively, if you\ndon\u2019t have or want a Mother Duck account, you can visit https:/ /shell.duckdb\n.org, which behaves essentially like your CL I but without being able to persist\ndata or connect to MotherDuck.\nResults of your SQL querie s are cached in the DuckDB\ninstance local to your browser, enabling you to instantlysort, pivot, and filter query re sults! The UI lists databases,\ntheir tables with their columns, and uploaded files on theleft side. With a context menu , you can use, share, drop,\ndetach, or copy the names of  databases. The menu shown\nin figure 7.3 will appear as a context item when navigating\nthe tree-like structure.\n DuckDB offers a Jupyter-Notebook-like experience for\nrunning queries, which allows  you to write SQL statements\nwith auto-complete and then run them and see queryresults rendered in a data gr id below the cell. The output\ndata grids support local sorting, selecting output columns,showing histograms and aggregations in the column header, and pi voting and filter-\ning the data. The state of the UI with your queries is kept across sessions, so you can\nclose the browser and continue from where you left off at a later date.\n The web-based UI is a great work environment for pre-existing, shared, and new\ndatabases. Everything that you\u2019ve learne d so far about SQL and writing queries and\nworking with different data sources as explored in chapter 5 can be applied. There issupport for uploading CSV and Parquet files di rectly from the UI. They will be accessi-\nble in any query, and you will utilize a \nCREATE  TABLE  AS SELECT  statement or transform\nthem to your needs. To import existing da tabases from the DuckDB CLI, or use Moth-\nerDuck from any of the suppor ted language bindings, you\u2019ll need to authenticate the\nCLI or the language binding of choice with MotherDuck. \n7.2.2 Connecting to MotherDuck with DuckDB via token-based authentication\nPlease make sure you have a MotherDuck ac count and are logged in before proceeding.\nDuckDB triggers the authentication process with MotherDuck at the moment you try to\nopen a shared database instance. This can be a named database or the default one. \nFigure 7.3 Contextual \nmenu of the navigator149 7.2 Getting started with MotherDuck\nNOTE When you connect to MotherDuck wi thout specifying a database, you\nconnect to a default database called my_db . This is the current database. You\ncan then query any table in this databa se by specifying the table name. The\nUSE command allows you to switch the current database.\nLet\u2019s open the default database at Mo therDuck for your account by running\n.open md:  in the CLI. Unless you\u2019re already authen ticated, it will prompt you with the\nfollowing message:\nAttempting to automatically open the SSO authorization page in your\n\u27a5default browser.\n1. Please open this link to login into your account:\n\u27a5https://auth.motherduck.com/activate\n2. Enter the following code: XXXX-XXXX\nIf you follow through the auth flow to comple tion, you\u2019ll see the following lines in the\nterminal:\nToken successfully retrieved [\u221a]\nYou can store it as an environment variable to avoid having to log in again:\n$ export motherduck_token='eyJhbGciOiJI..._Jfo'\nY o u r  b r o w s e r  w i l l  h a v e  o p e n e d  w i t h  a  d e vice confirmation message similar to that\nshown in figure 7.4.\nFigure 7.4\nMotherDuck confirmation message150 CHAPTER  7DuckDB in the cloud with MotherDuck\nYou\u2019re now able to access databases on Mo therDuck, and as the message says, if you\nwant to be automatically logged in during future sessions, you sh ould add the Mother-\nDuck token as an environment variable. Al ternatively, you can use the token as a\nparameter to the md: protocol like this:\nD .open 'md:?motherduck_token=eyJhbGciOiJI..._Jfo'\nThis URL format is also applicable to the individual language bindings. Think back to\nchapter 6, in which we disc ussed connection ma nagement from within the integration\nwith the Python ecosystem. To open a co nnection to MotherDuck from Python, you\nwould use\nimport duckdb\ncon = duckdb.connect('md:?motherduck_token=eyJhbGciOiJI..._Jfo')\nNext, we will discuss how to perform various task s in MotherDuck. Whether to use the\nweb UI or the CLI is ultimately up to yo u, but please make sure you set up your\nMotherDuck account and can access it with either the UI or the CLI. \n7.3 Making the best possible use of MotherDuck\nIn this section, you will learn about the features added to DuckDB via the Mother-\nDuck extension that let you interact with MotherDuck and use it to its fullest poten-\ntial. We start by discussing al l features helping you to get your data into the cloud and\npotentially sharing your database s with colleagues and partners:\n\uf0a1Uploading databases from your local machine to the cloud\n\uf0a1Managing databases (creation, deletion, listing)\n\uf0a1Sharing databases via URL, refreshing the shares, and attaching them to your\nlocal DuckDB instance\n\uf0a1Accessing data from S3 buckets\nAfter that, we will discuss how to control whether a query runs on a database that is\ncompletely remote, completely local, or partially remote and partially local.\n Last but not least, MotherDuck ships a coup le of AI-related features, such as func-\ntions that will automatically describe your schema and, ba sed on that, generate or fix\nSQL statements for you. Let\u2019s have a look at  these features, starting with uploading a\nlocal DuckDB database.\n7.3.1 Uploading databases to MotherDuck\nThink back to chapter 6, in which we used the Python integration to build up a database\ncontaining data about countrie s. We finished our work ther e, we cleaned up issues in\nthe dataset, and now we want to share that work with colleagues. One way to do this is\nvia the \u201cspreadsheet way of life\u201d: just take the countries.duckdb  store, attach it to an\nemail or copy it to a network folder, and mo ve on. A better, less-fragile way of sharing\nis MotherDuck. To begin, start your DuckDB  CLI, and open your database with the151 7.3 Making the best possible use of MotherDuck\nfollowing command (if you don\u2019t have that store, don\u2019t worry\u2014the command will cre-\nate it for you):\n.open countries.duckdb\nLet\u2019s add a cities  table to that database, especially if you created a fresh database:\nCREATE TABLE cities AS\nSELECT *\nFROM (VALUES ('Amsterdam', 1), ('London', 2)) cities(Name, Id);\nBefore we share the database, we need to deta ch so that all the locks will be released.\n This can either be done by switching back to an in-memory database, using .open\nwithout arguments followed by a LOAD motherduck;  to load the extension, or in one\ngo by calling .open md: . The latter will attach the CLI to the MotherDuck default\ndatabase. We can then create the remote database with the CREATE DATABASE  state-\nment as follows:\n.open md:\nCREATE DATABASE \"countries\" FROM 'countries.duckdb';\nDepending on the size of your database and the speed of your internet connection,\nthe upload process can take so me time. In our experiment s, the upload of a 16 GB\ndatabase from a regular laptop accessing a 40 Mbps upstream home internet connec-\ntion took about 40 minutes. At the time of writing, this is a topic MotherDuck is\nactively working on, and the upload performance is expected to improve in the near\nfuture. For now, it may be faster to export  your database into Parquet files, upload\nthose to cloud storage, and then create a database in MotherDuck from those files.\n Once the database is uploaded, you can check whether you are able to use it. Your\nsession is still attached to  the default database in Mo therDuck. That means you\u2019ll\nneed to either prefix your tables with the database name or switch to the new data-\nbase. FROM countries.cities  uses the prefix. In the following snippet, we first switch\nto the database by issuing a USE statement. That allows us to omit the prefix to any\ntable in that database, as we\u2019ve done in all previous examples:\nUSE countries;\nFROM cities;\nIf you happened to already leave the CLI, you can connect directly to the new data-\nbase via .open md:countries  too. Either way, we can then run a query to check that\nthe data is there. So if we select everything from the cities  table with FROM cities ,\nwe should see everythi ng we just created:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name \u2502 Id \u2502\n\u2502 varchar \u2502 int32 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Amsterdam \u2502 1 \u2502\n\u2502 London \u2502 2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The USE statement will look up the database \nname in the remote MotherDuck catalog.152 CHAPTER  7DuckDB in the cloud with MotherDuck\nWhen uploading a local database to MotherDu ck, the following rule is very important\nto remember: the local and remote names must always be different . If you forget this rule\nand give them the same name, you risk receiving the following error:\ncreate database \"countries\" from 'countries' ;\nError: Catalog Error: Database 'countries' has already been\n\u27a5created and attached\nAnd if you try to upload your cu rrent database, you get this error:\nError: Binder Error: Database \"countries.duckdb\" is already attached\n\u27a5with alias \"countries\"\nWe avoided these errors in our example by calling .open md: , which did two things\nfor us: it loaded the MotherDuck extens ion and connected to the default Mother-\nDuck database. Another option is using .open , which will switch to an in-memory\ndatabase. \n7.3.2 Creating databases in MotherDuck\nIn the previous section, we uploaded an existing database to MotherDuck. Alterna-\ntively, you can start building your schema fr om scratch directly in the cloud. If the\ncontent of your schema depends largely on  files stored in another public cloud, it\nwould be a waste of time and resources to  download them first into a local DuckDB\ninstance and then up load that database to MotherDu ck. In most ca ses, MotherDuck\nwill run closer to an S3 bucket than your  local system, so an ingress from the cloud\ndirectly into MotherDuck ma y save you time and money. \n You create a new database in MotherDuck using the CREATE  DATABASE  command.\nThe database name can\u2019t have any specia l characters\u2014only alphanumeric characters\nand underscores are allowed. \n To create a database called my-test , run the following:\nCREATE DATABASE \"my-test\";\nYou can confirm that the database has been created with the SHOW  DATABASES\ncommand:\nSHOW DATABASES;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 database_name \u2502\n\u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 my-test \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAlternatively, you can use the .databases  CLI command. You can run either of these\ncommands from your local DuckDB CLI, or you can navigate to the MotherDuck UI(https:/ /app.motherduck.com/ ) and run them there. 153 7.3 Making the best possible use of MotherDuck\n The CREATE DATABASE  statement just creates the database\u2014it doesn\u2019t change your\nsession to it\u2014so we need to run USE 'my-test';  first. We can then check that we\u2019re\nconnected to this database using the current_database  function:\nSELECT current_database();\nIf everything worked as expected, you should see the correct database name:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 current_database() \u2502\u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 my-test \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nLet\u2019s now create a people  table in our MotherDuck database and add a couple of rows:\nCREATE TABLE people (name varchar, born date);\nINSERT INTO people VALUES ('Mark', '1989-01-01'), ('Hannes', '1990-01-01');\nWe can then return a coun t of the records in the people  table by running the follow-\ning query:\nSELECT count(*)\nFROM people;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25022 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe can also run that same query with the database name prefixed  to the table name:\nSELECT count(*)\nFROM \"my-test\".people;\nAnd once we\u2019re happy that we\u2019ve gotten the hang of how MotherDuck works, we can\nremove that test database. To do that, we \u2019ll need to first sw itch to the default my_db\ndatabase before running the DROP DATABASE  command. \nUSE my_db;\nDROP DATABASE \"my-test\";\nThe my-test  test database should no longer appear in the output of SHOW  DATABASES; . \n7.3.3 Sharing databases\nMotherDuck offers the ability to share a read -only snapshot of your databases. This is\ngreat not only for sharing the data but for collaborative analysis and shared function-\nality. The snapshot will not only contain your  data but all views, and with them all the\neffort you put into them during creation. Th ink of it as a \u201cspreadsheet on steroids.\u201d154 CHAPTER  7DuckDB in the cloud with MotherDuck\n To make your data available to others ( https:/ /mng.bz/NRW7 ), you can use the\nCREATE SHARE  statement. If you run it, you will ge t a shareable link that others can\nconnect to using the ATTACH  command ( https:/ /mng.bz/Ddea ). \nNOTE The following generated links will be di fferent if you create the shares for\nthe country database based on your Mo therDuck account and your database.\nAssuming you followed our initial instructions to create the countries  database, you\ncould share it like this:\nCREATE SHARE shared_countries\nFROM countries;\nYou will receive a link similar to the following:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 share_url \u2502\u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 md:_share/countries/1acb80cf-d872-4fab-8077-64975cce0452 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe amount of time it takes to create a sh ared database currently depends on the size\nof the source database. A 16 GB database took about a minute to share in our case.\n To attach this database, our friend or colleague will need to have a MotherDuck\naccount too. They can then run the foll owing command on thei r DuckDB database:\nATTACH 'md:_share/countries/1acb80cf-d872-4fab-8077-64975cce0452'\nAS shared_countries;\nWe can describe the contents of a shared database using the DESCRIBE  SHARE  command:\n.mode lineDESCRIBE SHARE shared_countries;\nThis results in the following output, mirroring the source link and the original name\nas well as the IDs of the database and the latest snapshot:\nshare_name = shared_countries\nshare_link = md:_share/countries/1acb80cf-d872-4fab-8077-64975cce0452\ndatabase_name = countries\ndatabase_id = 9d7586ac-add9-46dc-a4fb-def6b42f0f7csnapshot_id = 041a5ba9-8cf4-471b-af13-1bec75a0b3ce\nWe can also list the shares that we\u2019ve created:\nLIST SHARES;\nAt the time of publication, shares are not automatically updated when you change the\ncontent of your shared database. Changes to  both schema and data must be explicitly\npropagated through the UPDATE SHARE  statement from the sharing site:The name of the \nshared database The name of the \ndatabase to share155 7.3 Making the best possible use of MotherDuck\nUPDATE SHARE shared_countries;\nTo get rid of any share you are not interested in anymore, run the DETACH  statement. If\nyou are connected to the share, you need to switch to a different database first:\nUSE my_db;\nDETACH shared_countries;\nWe did actually prepare a couple of hopefully  interesting shares for you, including the\ncomplete database of chapters 3 and 4, co ntaining all tables and views we created\nthroughout the chapters. Youc can access it with the following:\nATTACH\n'md:_share/duckdb_in_action_ch3_4/d0c08584-1d33-491c-8db7-cf9c6910eceb'\nAS duckdb_book_ch3_and_4;USE duckdb_book_ch3_and_4;\nSHOW tables;\nWe also ingested a complete dump from St ack Overflow, which provides a great data-\nset to play along with:\nATTACH\n'md:_share/stackoverflow/6c318917-6888-425a-bea1-5860c29947e5'\nAS stackoverflow_analysis;\nUSE stackoverflow_analysis;\nSELECT count(*) FROM posts;\nThe share contains a whopping number of 58,329,356 posts and an swers\u2014an interest-\ning corpus of data to play with. We colle cted some other interesting shares you can\nload into a sample_data  database via\nATTACH 'md:_share/share_sample_data/23b0d623-1361-421d-ae77-62d701d471e6'\nAS sample_data;\nThe tables are prefixed, and you can access them via their fully qualified names (e.g.,\nsample_data.nyc.yellow_cab_nyc_2022_11 ) (see table 7.1).\nTable 7.1 List of tables in the sample database\nName Table name Rows Description\nHacker News hn.hacker_news 3.9M A sample of comments from Hacker \nNews\nNYC 311 \nComplaint Datanyc.service_requests 32.5M Requests to NYC\u2019s 311 complaint \nhotline via phone and web\nAir Quality who.ambient_air_quality 41k Historical air quality data from the \nWorld Health Organization\nTaxi Rides nyc.taxi 3.3M NYC yellow cab trips data from \nNovember 2020\nRideshare nyc.rideshare 18.1M Rideshare trips (Lyft, Uber, etc.) in NYC156 CHAPTER  7DuckDB in the cloud with MotherDuck\nThese datasets provide many different topi cs, which you can use to train your SQL\nskills or to explore the possibili ties of DuckDB and MotherDuck. \n7.3.4 Managing S3 secrets and loading Data from S3 buckets\nIn previous chapters, we ingested data fr om files directly available over http:/ / or\nhttps:/ / protocols. Oftentimes, people use Amazon S3 for storing files, accessible via\nthe s3:/ / protocol. \n MotherDuck\u2014and DuckDB\u2014speak that pr otocol as well, but they need your\nsecrets to authenticate on your behalf against Amazon S3. When using MotherDuck,\nyou can store your secrets in their systems so  that they are available in all sessions con-\nnected to MotherDuck. You do this either  via the web UI or a dedicated statement\u2014\nthe CREATE OR REPLACE SECRET  statement:\nCREATE OR REPLACE SECRET (\nTYPE S3,\nKEY_ID 'access-key',\nSECRET 'secret-key',REGION 'us-east-1'\n);\nOnce you\u2019ve done that, you can query data in a secure S3 bucket, just like any CSV or\nParquet file we queried over http or the file system before:\nCREATE OR REPLACE TABLE mytable AS\nFROM 's3://...';\nOnce you\u2019ve finished working with the bu cket, you\u2019ll want to remove the secret:\nDROP SECRET (TYPE s3);\nThere are a couple of things to keep  in mind when creating a secret:\n\uf0a1In versions of DuckDB prior to 0.10.0, you can only have one SECRET  object.\n\uf0a1You can only use permanent S3 secrets\u2014 temporary S3 secret s, which are only\nvalid during a session, are currently not supported. \n7.3.5 Optimizing data ingest ion and Mother Duck usage\nRunning a cloud-based solution incurs a vari ety of cost types, including, among oth-\ners, the costs of compute, storage, data  ingress, and data egress. The MotherDuck\nextension gives you close control over where a function is to be executed: in the cloud\nor on your local machine. If you want to process a large Parquet file you already have\non your local machine, it\u2019s pointless to uplo ad it to S3 first and then run the process-\ning in MotherDuck, as you will pay for both the computational costs at MotherDuck\nand the egress cost at S3. Given a fast inte rnet connection, you are most likely better\noff doing this locally and simply uploadin g from your machine to MotherDuck. The\nextension enhances all functions starting with the read_  prefix\u2014such as read_json\nand read_csv_auto \u2014to support the MD_RUN  parameter. That parameter lets you con-\ntrol where the function will be executed , and it supports th e following values:157 7.3 Making the best possible use of MotherDuck\n\uf0a1MD_RUN=LOCAL \u2014This executes the function in your local DuckDB environment.\n\uf0a1MD_RUN=REMOTE \u2014This executes the function in MotherDuck-hosted DuckDB\nruntimes in the cloud.\n\uf0a1MD_RUN=AUTO \u2014This remotely executes all s3:/ /, http:/ /, and https:/ / requests,\nexcept those to localhost/127.0.0.1. This is the default option.\nFor example, if you wanted to query the DuckDB IP\u2019s dataset on the MotherDuck-\nhosted DuckDB runtime, you could execute the following query. In the example, we\nuse .timer on  to get the query execution time:\n.timer on\nSELECT count(*)\nFROM read_csv_auto(\n'https://github.com/duckdb/duckdb/raw/main/data/csv/ips.csv.gz',\nMD_RUN=REMOTE\n);\nAs of writing, the query took less than a second to run in the MotherDuck cloud.\nUsing MD_RUN=LOCAL  instead, it took close to 2 seco nds on the author \u2019s slow internet\nconnection.\n Beyond its free tier, Moth erDuck offers a Standard tier of its service for a base\nprice of $25 per month for 100 GB storage and 100 compute hours. In the event that\nresources are needed beyond the standard usage allotments, users may purchase addi-\ntional cold storage or comp ute time at rates of $0.08 per GB and $0.40 per hour,\nrespectively. However, keep in  mind that this is only for cloud usage, not your local\nexecution (see https:/ /motherduck.com/pricing/ ). \n Cold storage  refers to the persistent storage for your databases and files. Hot storage ,\non the other hand, is used to execute your queries, equivalent to memory usage, and it\ncan be limited to a maximum va lue, fine-tuned to your needs. In DuckDB, hot storage\nis automatically scaled up to the upper limit and metered per second and gigabyte.\n The separation actually makes a lot of sens e, as studies show that a huge percentage\nof the data that gets processed is less than 24  hours old. By the time data gets to be a\nweek old, it is about 20 times less likely to be queried than data from the most recent\nday. Additionally, the workload sizes are ofte n smaller than overall data sizes. For exam-\nple, dashboards are usually built from aggregat ed data: any data from the last hour can\nbe freshly aggregated so that  you won\u2019t miss out on the late st changes. Anything reach-\ning back later than a week can be stored pr e-aggregated as an ad ditional table. The\naggregate will usually have many fewe r rows and consume much less storage.\n If you want to keep your costs unde r control, avoid storing superfluous data\ndirectly in MotherDuck; instead, load or process it as needed, and set a reasonable\nmaximum amount of hot storage you\u2019ll keep  at one time. The amount of hot storage\navailable does affect the performance of your queries. \n7.3.6 Querying your data with AI\nWhile SQL is somewhat similar to the English language, it\u2019s not always easy to master.\nAnd if you\u2019re new to it, some SQL cons tructs might be downright intimidating. 158 CHAPTER  7DuckDB in the cloud with MotherDuck\nNOTE An earlier name for SQL was SEQUEL  (Structured English QUEry Lan-\nguage), which was a pun on QUEL, another query language based on therelational model. The name was eventu ally dropped due to trademark issues. \nQueries in a structured lang uage can, however, be nice ly generated, and you will be\ndelighted to hear that MotherDuck offers a generative AI feature ( https:/ /mng.bz/\nlMjB ) that allows you to query your data usin g natural language. It is able to describe\nyour data and will generate SQL statements for you or fix existing ones. The feature\nworks by sending the database schema along with a detailed prompt and your ques-tion to a large language model (LLM) that then generates the requested SQL state-\nment and, optionally, executes it.\n From our experience, it works quite well, and you can try it out on any of the test\ndatasets. The following example uses the Stac kOverflow dataset, attached with the fol-\nlowing statements:\nATTACH\n'md:_share/stackoverflow/6c318917-6888-425a-bea1-5860c29947e5'\nAS stackoverflow_analysis;USE stackoverflow_analysis;\nYou can get a description of the database schema by calling the prompt_schema\nprocedure: \n.mode line\nCALL prompt_schema();\nThe results of running this a co uple of times are as follows:\nsummary = The database contains tables for storing data related to votes,\n\u27a5tags, posts, post links, badges, users, and comments.\nRun Time (s): real 3.672 user 0.007355 sys 0.002674\nIt takes a couple seconds to run, and the result isn\u2019t all that impressive\u2014you could\nhave gotten the same output by looking at th e table list. If you call the function a sec-\nond time, you will most likel y get a different response:\nsummary = The data in the database is about votes, tags, posts, post links,\n\u27a5badges, users, and comments.\nRun Time (s): real 3.054 user 0.007354 sys 0.003175\nThis may initially seem surprising until we consider that an LLM is a probabilistic\nmodel that isn\u2019t guaranteed to retu rn the same response each time.\n Instead of using an SQL statement to quer y for the most popular tags, we will use\nplain English in the next example: What are the most popular tags?  This is, of course, not\na valid SQL statement, so we must indi cate this via a special pragma called\nprompt_query . A pragma  is special directive that tells a compiler or query parser how it\nshould process its input.  Here, the input should be process as a prompt:\n.mode duckbox\npragma prompt_query('What are the most popular tags?');159 7.3 Making the best possible use of MotherDuck\nWithout showing us how, we get 10 rows back, and they are ac tually meaningful:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TagName \u2502 Count \u2502\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 javascript \u2502 2479947 \u2502\u2502 python \u2502 2113196 \u2502\n\u2502 java \u2502 1889767 \u2502\n\u2502 c# \u2502 1583879 \u2502\u2502 php \u2502 1456271 \u2502\n\u2502 android \u2502 1400026 \u2502\n\u2502 html \u2502 1167742 \u2502\u2502 jquery \u2502 1033113 \u2502\n\u2502 c++ \u2502 789699 \u2502\n\u2502 css \u2502 787138 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518-- Run Time (s): real 3.763 user 0.124567 sys 0.001716\nWhile this is a correct answer, you might be curious about which SQL query was used\nto compute the answer. We can determine this  (keeping in mind that probabilistically\nit could have been slightly different) using the prompt_sql  procedure:\n.mode line\ncall prompt_sql('What are the most popular tags?');\nquery = SELECT TagName, Count\nFROM tagsORDER BY Count DESC;\nRun Time (s): real 5.425 user 0.010331 sys 0.005074\nLooks good\u2014it\u2019s even smart enough to use the table columns alon g with ordering and\nlimit to get the most popular tags. The run time for these AI prompts is between 2 and\n10 seconds, with most  of this time being spent inside  the large language model (LLM).\n Let\u2019s see how it deals with  a more involved question: What are the titles and com-\nment counts of the five posts with the most comments?\n.mode duckbox\npragma prompt_query(\"Which 5 questions have the most comments, what is the\npost title and comment count\");\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Title \u2502 comments \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 UIImageView Frame Doesnt Reflect Constraints \u2502 108 \u2502\n\u2502 Is it possible to use adb commands to click on a view by find\u2502 102 \u2502\n\u2502 How to create a new web character symbol recognizable by html\u2502 100 \u2502\u2502 Why isnt my CSS3 animation smooth in Google Chrome (but very \u2502 89 \u2502\n\u2502 Heap Gives Page Fault \u2502 89 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Run Time (s): real 19.695 user 2.406446 sys 0.018353160 CHAPTER  7DuckDB in the cloud with MotherDuck\nAnd let\u2019s check which query was used. It is in teresting that it detects or knows that all\nentries in the posts  table with PostTypeId = 1  are questions and not answers. Per-\nhaps it knows that because it was trained on the StackOverflow dataset:\n.mode line\ncall prompt_sql(\"Which 5 questions have the most comments, what is the\npost title and comment count\");\nquery = SELECT p.Title, COUNT(c.Id) AS comment_count\nFROM posts p\nJOIN comments c ON p.Id = c.PostId AND p.PostTypeId = 1GROUP BY p.Title\nORDER BY comment_count DESC\nLIMIT 5;Run Time (s): real 4.795 user 0.002301 sys 0.001346\nFigure 7.5 shows what this l ooks like in the MotherDuck UI.\nFigure 7.5 MotherDuck UI with AI queries\n161 7.3 Making the best possible use of MotherDuck\nSince the comment count is a column in th e posts table, the join with the comments\ntable isn\u2019t needed. Let\u2019s see if we can ge t it to generate a query using just the posts\ntable by tweaking our prompt:\ncall prompt_sql(\"System: No joins! User: Which 5 questions have the most\ncomments, what is the post title and comment count\");\nquery = SELECT Title, CommentCount\nFROM postsWHERE PostTypeId = 1\nORDER BY CommentCount DESC\nLIMIT 5;Run Time (s): real 3.587 user 0.001733 sys 0.000865\nMuch better!\n You can also use call prompt_fixup()  to fix an SQL code for a statement (e.g.,\nthe infamous, \u201cI forgot GROUP BY \u201d):\ncall prompt_fixup(\"select postTypeId, count(*) from posts\");\nquery = SELECT postTypeId, COUNT(*) FROM posts GROUP BY postTypeId\nRun Time (s): real 12.006 user 0.004266 sys 0.002980\nOr you can use it to fix an inco rrect join column name or two:\ncall prompt_fixup(\"select count(*) from posts join users on\n\u27a5posts.userId = users.userId\");\nquery = SELECT COUNT(*) FROM posts JOIN users ON\n\u27a5posts.OwnerUserId = users.Id\nRun Time (s): real 2.378 user 0.001770 sys 0.001067\nWe think there\u2019s a lot of potential in using an LLM to generate queries for a database,\nespecially when the model can be augmente d with the database schema. While it most\nlikely won\u2019t replace queries that are written and tuned by specialists for reports and\napplications, it will make database syste ms much more accessible to a broader audi-\nence. Notably, in early 2024, MotherDuck introduced FixIt\u2014a fast SQL error fixer for\nthe UI, based on the same technology, whic h fixes syntactically incorrect SQL state-\nments or those that are directly inline in the UI ( https:/ /mng.bz/Bdar ). \n7.3.7 Integrations\nMotherDuck supports a wide variety of da ta transfer, business intelligence, and data\nvisualization tools, as shown in  figure 7.6. Going from left to right, with optional trans-\nformations in between, MotherDuck can sit in the middle of your pipeline. Sourcescan either be ingested directly or via addi tional services and then put into storage at\nMotherDuck before they are queried by either business in telligence use cases or with\nspecific data science tools. They can serve as a retrieval augmented generation  (RAG)\ninformation retrieval compon ent for LLM models too. \n Additionally, any existing DuckDB integrat ions and drivers also work with Mother-\nDuck, so everywhere you ca n use DuckDB, you can use MotherDuck. All it takes is162 CHAPTER  7DuckDB in the cloud with MotherDuck\ninserting the md: prefix into the database conne ction string and appending the\n?motherduck_token=<token>  parameter. \nSummary\n\uf0a1MotherDuck is a serv erless data analytics platform that makes it easy to query\nand analyze data in cloud storage from the browser.\n\uf0a1It integrates seamlessly into the DuckDB CLI, the Python, and other languageintegrations using the \nmd: protocol, which automati cally loads the MotherDuck\nextension.\n\uf0a1MotherDuck enables you to store struct ured data, query that data with SQL,\nand share it with others.\n\uf0a1One key principle of the service is ease of use; you don\u2019t need to configure or\nspin up instances, clusters, or warehous es. You simply write and submit SQL, in\nthe same tool or from within the sa me ecosystem, when working locally.\n\uf0a1In many cases, data can be ingested mu ch faster at MotherDuck than locally\ndue to its closer proximity to data sources.\n\uf0a1Local, remote, and shared datasets can ea sily be joined by using the qualified\nnames of schema and relation.\n\uf0a1The MotherDuck platform also offers s upport for querying datasets in natural\nlanguage, allowing people who are not trained in SQL to benefit from the\nanalytical database too. \nFigure 7.6 MotherDuck integrations163Building data pipelines\nwith DuckDB\nHaving explored DuckDB\u2019s seamless integration with prominent data processing\nlanguages, such as Python, an d libraries, such as pandas , Apache Arrow, and Polars,\nin chapter 6, we know that  DuckDB and its ecosystem are capable of tackling vari-\nous tasks that belong to data pipelines an d can, therefore, be used within them.\nThe combination of a powerful SQL engi ne, well-integrated tooling, and the\npotential of a cloud offering makes it mo re than just another database system.\n In this chapter, we\u2019ll delve deeper in to DuckDB\u2019s role within the broader data\necosystem, emphasizing its significance in building robust data pipelines and\nenhancing workflows. For this, we will first take a step back and discuss theThis chapter covers \n\uf0a1The meaning and relevance of data pipelines\n\uf0a1What roles DuckDB can have as part of a pipeline\n\uf0a1How DuckDB integrates with tools like the Python-\nbased data load tool for ingestion and the data build tool from dbt Labs for transformation\n\uf0a1Orchestrating pipelines with Dagster164 CHAPTER  8Building data pipelines with DuckDB\nmeaning and relevance of data pipelines. Th en, we are going to evaluate a couple of\ntools that we think are help ful when building robust pipelines. These tools cover\ningestion, transformation, an d orchestration. Let\u2019s start with the basics and have a\nlook at the problems we want to solve.\nNOTE As we\u2019re loading data from external live sources and repositories that\nhave been updated, the examples in this chapter might show different out-\nputs of records or counts than the ones you see when you run the code.\n8.1 Data pipelines and the role of DuckDB\nA data pipeline  is usually set up to retrieve and ingest data from various sources into a\ndata store, such as a database, a data lake stored as flat files in the cloud, or a datawarehouse. Prior to storing anything, data is usually processed and transformed in\nmany ways. These transformations includ e joining datasets together, filtering,\naggregating, or masking, with the goal of proper integration and standardization.\nWe\u2019ve already spoken a lot about filtering an d aggregations in ch apters 3 and 4, but\nmasking is new. Masking  is about anonymizing or distor ting confidential or regulated\ndata. \n It\u2019s not enough to just store the data; pi pelines are about creating value. Any use of\na data pipeline is eventually about crea ting a product, such  as dashboards, APIs,\nmachine learning (ML) models, and more. Figure 8.1 shows a potential data pipelinethat demonstrates some actions taken on data  and some of the directions it can take.\nFigure 8.1 Flows in a data pipeline from data s ources through transformation to storage and productsIngest Catalog\nStorage\nDatabases, data  warehouses, and data lakesTransformProducts\nAPIs, dashboards, and \nmodels\n2. Write data directly.1. Load data from sources.\n2. Store\nmetadata.\n3. Use/update\nmetadata.\n4. Products read/write from\nstorage.4.Use/update\n   metadata.\n3. Read/write transformed\ndata.Data sources:\nStreams and databases165 8.2 Data ingestion with dlt\nThere are usually two main types of data pipelines to deal with: batch processing and\nstreaming data. We are looking at pipelines th at deal with batch processing of data in\nthis chapter. Batch processing  usually forms a workflow of sequences of commands,\nwhere each output of one command becomes the input to the next. Processing will be\ncomplete after the last tran sformation is completed and the data has been stored in\nthe desired repository. Batch processing is  appropriate when there is no immediate\nneed to analyze each change or  react to immediate changes. \nNOTE The flow of extract, transform, and load  (ETL) is a subcategory of a data\npipeline. Not all pipelines follow that  exact sequence. While extraction, in\nmost cases, is the first step, data can be  loaded into a desi red storage first and\nthen be transformed. This sequence is known as ELT. Whether you build anETL or an ELT pipeline is relevant wh en you use a cloud service like Mother-\nDuck in which you want to make the be st possible use of resources. Some-\ntimes, it\u2019s better to transform your da ta with local resour ces; at other times,\nit\u2019s more effective to tr ansform data that is alre ady stored in the cloud. \nSo what is DuckDB\u2019s role in this? While DuckDB can be used as a storage system in a\npipeline, it usually sits in the transformati on and processing part of a pipeline due to\nits simple yet powerful  execution model: a single binary  that is capable of dealing with\nlarge datasets, using a plethora of sources and store formats as input, providing a com-\nplete SQL engine to transform data in many different ways.\n The broad support for SQL provides the firs t shared language to integrate with rel-\nevant processing tools like dbt, covered late r in this chapter. The second shared lan-\nguage oftentimes\u2014especially when storing to  data lakes\u2014uses Parquet as the output\nformat.\n Let\u2019s have a look at how a possible data pipeline with DuckDB might look. We start\nby ingesting some data. \n8.2 Data ingestion with dlt\nThe data load tool (dlt; see https:/ /dlthub.com/ ) is an open source Python library\nthat lets us load data from various, and of ten messy, data sources into a variety of desti-\nnations. Why would you want to use dlt and not run your own Python scripts to build\ningestion pipelines? The main entry to dlt, the pipeline  function, can infer a schema\nfrom source data and load that data to the destination, creati ng a suitable schema\nthere. We can use this pipeline with JSON data, DataFrames, or other iterable objects,\nsuch as generator functions, without chan ging any processing that comes afterward.\nThe engine takes care of versioning as well so that a data team can focus on using the\ndata and driving value, while ensuring effective governance through timely notifica-\ntions of any changes. \n dlt provides a set of predefined source s and destinations, including SQL databases,\nGitHub, and other interesting APIs. One of the destinatio ns supported out of the box\nis DuckDB. Custom sources and destinations can be defined as well, but this topic is\nnot covered in this book.166 CHAPTER  8Building data pipelines with DuckDB\n An interesting built-in dlt source is http:/ /chess.com . Their API provides informa-\ntion about players and games. In this section,  we\u2019re going to use that source to build a\nlittle chess database with Duck DB, as depicted in figure 8.2.\ndlt is written in Python, and we assume yo u have a working Python environment with a\nworking pip command. pip is Python\u2019s recommended package manager, and we use it\nto install dlt by running the following command:\npip install dlt\nIf you skipped chapter 6, you probably didn\u2019t install the DuckDB Python extension. It\nmust be installed for the pipeline to work:\npip install duckdb\nThe full source code for the pipeline we are going to build in an interactive fashion in\nthe following sections is also available in our examples repository on GitHub at\nhttps:/ /mng.bz/d6pv .\n8.2.1 Installing a supported source\nNext, we\u2019re going to initialize our Chess.com pipeline ( https:/ /mng.bz/rVle ). This is\ndone via dlt init , which takes two arguments: the source and the destination. The\nsource will be the built-in chess.com dlt source and the destination duckdb. As both\nour desired source and destination are offici ally supported by the tool, we can run the\nfollowing command in our shell to create all necessary files an d definitions for our\nfirst pipeline:\ndlt init chess duckdb\nThis command will create some files locally, including executable scripts, so we\u2019ll need\nto confirm that we\u2019re happy to do that:\nLooking up the init scripts in\n\u27a5https://github.com/dlt-hub/verified-sources.git...\nCloning and configuring a verified source chess (A source loading player \u27a5\nprofiles and games from chess.com api)\nDo you want to proceed? [Y/n]:\nOnce it\u2019s completed, we\u2019ll see the following output:\nVerified source chess was added to your project!\n* See the usage examples and code snippets to copy from chess_pipeline.py\n* Add credentials for duckdb and other secrets in ./.dlt/secrets.toml\nChess.comchess.\nsource\ndltHub\u2014chess_pipeline DuckDBduckdbdestination\nmainload \"source\"\nfrom \"player_pro\ufb01les\"\n\ufb01lter player & date-range Figure 8.2 Ingesting data \nfrom chess.com into DuckDB167 8.2 Data ingestion with dlt\n* requirements.txt was created. Install it with:\npip install -r requirements.txt\n* Read https://dlthub.com/docs/walkthroughs/create-a-pipeline\n\u27a5for more information\nWe should now have a directory called che ss that contains the helper functions and a\nfile called chess_pipeline.py that represents a working sample pipeline. You can either\nuse it as is, as inspiration for your own experiments, or build a pipeline from scratchwith us. We renamed the sample pipeline chess_pipeline.py.sample in our example\nrepository. \n8.2.2 Building a pipeline\ndlt pipelines are written in Python too. Yo u can enter the following Python snippets in\na new Python file, the Python REPL, or a Jupyter notebook. We\u2019re going to use the\nPython REPL, and the following experiment s assume you entere d the Python shell\nfrom the directory into which you initialized the pipeline. \n The first step is adding imports for dlt and the Chess.com source, as shown in the\nfollowing listing.\nimport dlt\nfrom chess import source\nOnce we\u2019ve done that, we\u2019re going to init ialize a pipeline. We\u2019re going to call it\nchess_pipeline , and we\u2019ll also specify DuckDB as the destination. The whole defini-\ntion will be stored in a variable named pipeline .\npipeline = dlt.pipeline(\npipeline_name=\"chess_pipeline\",\ndestination=\"duckdb\",dataset_name=\"main\"\n)\nThe DuckDB database will be written to a file named <pipeline- name>.duckdb (e.g.,\nchess_pipeline.duckd b, in our case).\n Next, we\u2019re going to create a source for four of the most popular players and their\nmatches from November 2022.\ndata = source(\nplayers=[\n\"magnuscarlsen\", \"vincentkeymer\",\n\"dommarajugukesh\", \"rpragchess\"Listing 8.1 Importing the required libraries for the new dlt pipeline\nListing 8.2 Defining a dlt pipeline\nListing 8.3 Defining a dlt sourceThe name of the pipeline will be reflected \nin the name of the DuckDB file created.\nThe dataset name will be used as a schema \nname in DuckDB. We use main here, which \nis the name of DuckDB\u2019s default schema.\nThis is a func tion loaded \nfrom the Chess source \nthat dlt provides.This and the following \narguments are specific to the Chess.com API.168 CHAPTER  8Building data pipelines with DuckDB\n],\nstart_month=\"2022/11\",\nend_month=\"2022/11\",\n)\nThis source contains a numb er of resources related to players, including profiles,\ngames, and online statuses. To import only the players\u2019 profiles, we can write the fol-\nlowing code.\nplayers_profiles = data.with_resources(\"players_profiles\")\nUntil now, we\u2019ve only defined what the pipe line should look like. To finally run it,\nwe\u2019ll pass the source defined as players_profile  to the pipelines run method. \ninfo = pipeline.run(players_profiles)\nprint(info)\nWe should see something li ke the following output:\nPipeline chess_pipeline completed in 0.62 seconds\n1 load package(s) were loaded to destination duckdb and into dataset main\nThe duckdb destination used\n\u27a5duckdb:////path/to/code/ch08/dlt_example/chess_pipeline.duckdb\n\u27a5location to store data\nLoad package 1696519035.883884 is LOADED and contains no failed jobs\nThat looks like it worked. Please don\u2019t leav e the Python shell yet; instead, open a sec-\nond terminal, and load th e database into DuckDB:\nduckdb chess_pipeline.duckdb\nThen check which tables have been created with SHOW TABLES :\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name \u2502\n\u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 _dlt_loads \u2502\n\u2502 _dlt_pipeline_state \u2502\u2502 _dlt_version \u2502\n\u2502 players_profiles \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\ndlt has also created a bunch of tables to store its own metadata, and at the bottom, we\ncan see players_profiles , which presumably contains profiles for the players we\nspecified in our script. Let\u2019s get one record from that table:Listing 8.4 Picking an interesting dataset from the dlt source\nListing 8.5 Running a dlt pipeline169 8.2 Data ingestion with dlt\n.mode line\nFROM players_profiles LIMIT 1;\navatar = https://images.chesscomfiles.com/uploads/v1/user/\n\u27a5138850604.80351cd5.200x200o.3129ed9b015d.jpeg\nplayer_id = 138850604\naid = https://api.chess.com/pub/player/dommarajugukeshurl = https://www.chess.com/member/DommarajuGukesh\nname = Gukesh Dommaraju\nusername = dommarajugukesh\nfollowers = 3\ncountry = https://api.chess.com/pub/country/IN\nlocation = Chennai\nlast_online = 2022-07-16 19:18:02+01\njoined = 2021-05-05 10:27:46+01\nstatus = basic\nis_streamer = false\nverified = false\nleague = Wood\n_dlt_load_id = 1696519035.883884\n_dlt_id = kldRaeRA40OGBA\ntitle =\nThat\u2019s the profile for Gukesh  Dommaraju , so the pipeline is working well so far.\n W h a t  i f  w e  n o w  d e c i d e  w e \u2019 d  l i k e  t o  l o a d  t h e  g a m e s  a s  w e l l ?  Q u i t  D u c k D B  a n d\nreturn to the Python shell; rerun the pipelin e with a slightly different source. Observe\nhow we don\u2019t assign the resource like we did before but just pass it to the run method. \ninfo = pipeline.run(data.with_resources(\"players_profiles\", \"players_games\"))\nprint(info)\nThe output should be similar to the following snipped, indicating it did load several\narchives for the players we had been interest ed in. Also, it did use the same storage as\nbefore:\nGetting archive from https://api.chess.com/pub/player/\n\u27a5magnuscarlsen/games/2022/11\nGetting archive from https://api.chess.com/pub/player/\n\u27a5vincentkeymer/games/2022/11\nGetting archive from https://api.chess.com/pub/player/\n\u27a5rpragchess/games/2022/11\nPipeline chess_pipeline completed in 1.89 seconds\n1 load package(s) were loaded to destination duckdb and into dataset main\nThe duckdb destination used duckdb:////path/to/code/ch08/dlt_example/\n\u27a5chess_pipeline.duckdb location to store data\nLoad package 1696519484.186974 is LOADED and contains no failed jobs\nAnd again, let\u2019s see what data\u2019s been ingest ed into DuckDB. In a second terminal, run\nthe following:Listing 8.6 Running the pipeline with another dataset170 CHAPTER  8Building data pipelines with DuckDB\nduckdb chess_pipeline.duckdb 'SELECT count(*) FROM players_games'\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 589 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThat looks good\u2014the games have been inge sted. What happens if we run the pipeline\nagain? dlt will discover that it already ingested everything  available that we asked for\nand won\u2019t import anything new. This is ac tually quite impressive; picking up batched\nimports where they have stopped is not an easy task. The information that dlt needs to\ncontrol this capability is stored inside the three other tables ( _dlt_loads ,_dlt_\npipeline_state , and _dlt_version ) that appeared in our st ore. We can extract that\nmetadata either with DuckDB or SQL,  or we can use the tools dlt offers. \n8.2.3 Exploring pipeline metadata\ndlt offers the info  command, which will inspect both the pipeline definition and the\nmetatables inside your store to retrieve a view on the state of the pipeline itself. The\ncommand has to be invoked as follows:\ndlt pipeline chess_pipeline info\nOur output indicates that we\u2019ve run the pi peline three times, and it describes the\nnames of the tables that have been created. Your output will differ in date and time\u2014and most likely in state as well:\nFound pipeline chess_pipeline in /Home/.dlt/pipelines\nSynchronized state:_state_version: 2\n_state_engine_version: 2\nschema_names: ['chess']pipeline_name: chess_pipeline\ndestination: dlt.destinations.duckdb\ndefault_schema_name: chessstaging: None\ndataset_name: main\nsources:\nAdd -v option to see sources state. Note that it could be large.\nLocal state:\nfirst_run: False\n_last_extracted_at: 2023-11-04T19:16:35.873231+00:00\nResources in schema: chess\nplayers_profiles with 1 table(s) and 0 resource state slot(s)players_games with 1 table(s) and 1 resource state slot(s)\nWorking dir content:\nHas 3 completed load packages with following load ids:171 8.3 Data transformation and modeling with dbt\n1699125395.876516\n1699125399.292224\n1699125402.854308\nPipeline has last run trace. Use 'dlt pipeline chess_pipeline trace'\n\u27a5to inspect\nIf you are interested in exploring the content of the data loaded with SQL, that's fine,\nof course. We will for now ta ckle transformations of datasets and put DuckDB to use\nnot only as a store but as an in tegral part of transformations. \n8.3 Data transformation and modeling with dbt\nData, in its raw form, often requires shaping, cleaning, and modeling to unlock its\ntrue potential. The data build tool (dbt ) is an SQL-centric transformation tool\ndesigned to support the creation and mana gement of data pipelines. It emphasizes\nsoftware engineering principles, allowing da ta teams to ensure modularity, portability,\nand documentation. Integrating CI/CD within  dbt facilitates the consistent and reli-\nable deployment of data transformations. \n So how do we use dbt with DuckDB? Enter the dbt-duckdb  library, which acts as the\nbridge that connects dbt to DuckDB. It enab les users to combine the strengths of both\ntools, making it possible to use DuckDB to  apply transformations in dbt-powered data\npipelines. The common language that both dbt and DuckDB speak is, of course, SQL. \n We\u2019re going to use dbt-duckdb  to build a straightforward data pipeline that takes\nsome CSV files stored on Gi tHub, applies cleanup and da ta transformation, and then\noutputs a Parquet file with the cleaned data . Thus, DuckDB is not used as a store but\nonly as a means of transformation, which is close to what we described in 8.1. The dia-\ngram in figure 8.3 shows what we\u2019re going to build.\nFigure 8.3 Transforming CSVs to Parquet\nLet\u2019s get to it! The first thing we need to do is install dbt-duckdb  and dbt into our\nPython environment by running the following command:\npip install dbt-duckdb dbt\nList CSV URLs.\nRead the CSV.\nFilter the columns.\nRemove null rows.\nReformat the date.\nOutput a Parquet.\nApplying transformations in DuckDBRaw\ndataTransformed\ndata172 CHAPTER  8Building data pipelines with DuckDB\nThe example project we will buil d in the following sections is built in several iterative\nsteps. We provide a subfolder for ea ch step in our example repository: https:/ /mng.bz/\nVxrW .\n8.3.1 Setting up a dbt project\ndbt thinks in projects and provides comma nds to create new on es. If the preceding\ninstallation succeeded, you should be able to execute the following commands in your\nshell, creating a new project and changing the working directory to it. If you run dbt\nfor the first time, it will se t up your profile and ask you for the database you want to\nuse. Pick DuckDB here:\ndbt init dbt_transformations\ncd dbt_transformations\nLet\u2019s now have a look at the director y structure. We\u2019re going to use the tree\ncommand to do this, but you can also na vigate through the directory structure\nmanually:\ntree\n.\n\u251c\u2500\u2500 README.md\u251c\u2500\u2500 analyses\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 macros\u251c\u2500\u2500 models\n\u2502 \u2514\u2500\u2500 example\n\u2502 \u251c\u2500\u2500 my_first_dbt_model.sql\u2502 \u251c\u2500\u2500 my_second_dbt_model.sql\n\u2502 \u2514\u2500\u2500 schema.yml\n\u251c\u2500\u2500 seeds\u251c\u2500\u2500 snapshots\n\u2514\u2500\u2500 tests\nWe have folders for the main dbt concepts : macros, models, seeds, snapshots, and\ntests. For our sample projec t, we\u2019re going to create mo dels and tests\u2014we won\u2019t be\nusing the other directories.\n The project contains a couple of examples  but no usable profile yet. Let\u2019s create\none via a file\u2014profiles.yml\u2014with th e content in the following listing.\ndbt_transformations:\ntarget: dev\noutputs:\ndev:\ntype: duckdb\npath: '/tmp/atp.db'\nschema: 'main'Listing 8.7 Defining a dbt profile that uses DuckDB for all output\nThis is the intermediate \nDuckDB store being used by \ndbt. It can have  any location.173 8.3 Data transformation and modeling with dbt\nNOTE YAML Ain\u2019t Markup Language (YAML) is a data serialization language\ndesigned for human readability and inte raction with scripting languages. Its\nsyntax is relatively straightforward and utilizes indentation to represent hier-\narchy. It exclusively uses spac es, not tabs, for this purpose. \nNext, let\u2019s rename the example directory to  atp and delete the existing model files:\nmv models/example models/atp\nrm models/atp/*.sql\nNow, with the examples out of the way, we need to define new sources and models. \n8.3.2 Defining sources\ndbt sources provide a standardized way to reference and document raw data in exter-\nnal databases, data warehouses, or anywhere  else. By defining sources, users ensure\nconsistency in how raw data is accessed, wh ile also specifying metadata and quality\nchecks associated with the underlying datase ts. We can define sour ces in a sources.yml\nfile inside the models directory. \n The data we\u2019re going to use is from Jeff Sackmann\u2019s tennis dataset ( https:/ /\ngithub.com/JeffSackmann/tennis_atp ), which we first encountered in chapter 5. To\nstart with, we\u2019re going to pr ocess just one of the CSV fi les. Create a file named\nmodels/atp/sources.yml, which creates a so urce that points to the atp_matches_\n2023.csv file on GitHub.\nversion:\nsources:\n- name: github #\nmeta:\nexternal_location: 'https://raw.githubusercontent.com/\n\u27a5JeffSackmann/tennis_atp/master/atp_matches_2023.csv'\ntables:\n- name: matches_file #\nWe\u2019ll be able to reference this source via the combination of the source name\n(github ) and table name ( matches_file ) in the model, which we\u2019ll define next. \n8.3.3 Describing transformations with models\ndbt models are a set of SQL qu eries or Python scripts that  transform raw data into a\ndesired structure. By defining these models , data analysts and engineers can create,\ntest, and document their data transformati on workflows in a co nsistent and version-\ncontrolled manner. \n Now we\u2019re going to create our first tran sformation in a file named models/atp/\nmatches.sql. We\u2019re going to pull all of the matches from the CSV file we defined inListing 8.8 Defining a dbt source that fetches data from a web location\nThe dbt version\u2014this needs to be 2 to work \nwith the version of dbt at the time of writing.\nThe name of our source\u2014this \ncan be whatever we like.\nThe name we\u2019ll use to reference the \nCSV file\u2014this can be we whatever we like.174 CHAPTER  8Building data pipelines with DuckDB\nlisting 8.8, but we\u2019ll exclude al l the columns that start with w_ or l_. These fields con-\ntain fine-grained match data that we won\u2019t need for our use case. We\u2019ll then write the\noutput of the query to a Parquet file. The fu ll code snippet is shown in the following\nlisting.\n{{ config(\nmaterialized='external',\nlocation='output/matches.parquet',\nformat='parquet'\n)} }\nWITH noWinLoss AS (\nSELECT COLUMNS(col ->\nNOT regexp_matches(col, 'w_.*') AND\nNOT regexp_matches(col, 'l_.*')\n)\nFROM {{ source('github', 'matches_file') }}\n)\nSELECT * REPLACE (\ncast(strptime(tourney_date, '%Y%m%d') AS date) as tourney_date\n)FROM noWinLoss\nNext, let\u2019s create the output directory so that dbt will be able to write the Parquet file\nthere:\nmkdir output\nThe pipeline we defined will run the following steps:\n1dbt will grab the CS V data from GitHub.\n2It will pass the content to DuckDB.\n3Run the transformations written in SQL.\n4Store the outcome as a Parquet.\nLet\u2019s run it with\ndbt run\nThe process will print something along the following lines:\n...\n09:48:35 Found 1 model, 1 source, 0 exposures, 0 metrics, 351 macros,\n\u27a50 groups, 0 semantic models\n09:48:3509:48:35 Concurrency: 1 threads (target='dev')\n09:48:35\n09:48:35 1 of 1 START sql external model main.matches_2023\n\u27a5.......................... [RUN]Listing 8.9 Defining a dbt model using several queries for transforming data via DuckDB\nOutput a Parquet file \nin the output directory.Filter columns that start \nwith w_, using DuckDB\u2019s \nregexp_matches function.\nFilter columns that start \nwith l_, using DuckDB\u2019s \nregexp_matches function.Query the\nsource we\ndefined in\nthe previous\nsection.\nCoerce tourney_date\nfield to date type.175 8.3 Data transformation and modeling with dbt\n09:48:37 1 of 1 OK created sql external model main.matches_2023\n\u27a5..................... [OK in 1.16s]\n09:48:3709:48:37 Finished running 1 external model in 0 hours 0 minutes\n\u27a5and 1.20 seconds (1.20s).\n09:48:3709:48:37 Completed successfully\n09:48:37\n09:48:37 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1...\nIt looks like everything worked. We can look  in the output directory for the generated\nParquet file using the du command-line tool:\ndu -h output/*\n120K output/matches.parquet\nWe can then open up a DuckDB CLI session  to inspect the contents of the file:\nSELECT count(*) FROM 'output/matches.parquet';\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2986 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n.mode line\nFROM 'output/matches.parquet' LIMIT 1;\ntourney_id = 2023-9900\ntourney_name = United Cup\nsurface = Hard\ntourney_level = A\ntourney_date = 2023-01-02\nmatch_num = 300winner_id = 126203\nwinner_seed = 3\nwinner_entry =\nwinner_name = Taylor Fritz\nwinner_hand = R\nwinner_ht = 193\nwinner_ioc = USA\nwinner_age = 25.1\nloser_id = 126610\nloser_seed = 5\nloser_entry =\nloser_name = Matteo Berrettiniloser_hand = R\nloser_ht = 196\nloser_ioc = ITAloser_age = 26.7At the time of writing, you\u2019ll see 2,986 records, \nbut more matches are a dded all the time, so \nit\u2019s possible this number may be higher!176 CHAPTER  8Building data pipelines with DuckDB\nscore = 7-6(4) 7-6(6)\nbest_of = 3\nround = F\nminutes = 135\nwinner_rank = 9\nwinner_rank_points = 3355\nloser_rank = 16\nloser_rank_points = 2375\nThat looks pretty good\u2014we\u2019 ve successfully written our first dbt pipeline. We\u2019ve\nextracted the data from the CSV file, remove d some of the fields, and then written the\nresult out to a Parquet file. But how do we  know if the data in the Parquet file is\ncorrect?\n8.3.4 Testing transformations and pipelines\ndbt tests are assertions applie d to data models to ensure data quality and consistency.\nBy defining these tests, we can validate our transformations, catching problems like\nNULL  values, duplicates, or violatio ns of referential integrity. \n One place we can define tests is in the schema.yml file, which lives in models/atp/\nschema.yml, next to our model files. We\u2019re going to create tests for just a few of the\ncolumns, but in a production pipeline, you\u2019d want to create tests for all the fields to\nmake sure the transformati on worked as expected. \n We\u2019ll test the following assertions:\n\uf0a1tourney_id  is not NULL .\n\uf0a1winner_id  is not NULL .\n\uf0a1loser_id  is not NULL .\n\uf0a1surface  is not NULL  and only contains one of the following values: Grass , Hard ,\nor Clay .\nCreate the file models/atp/schema.yml, and enter the contents of the following\nlisting.\nversion: 2\nmodels:\n- name: matches\ndescription: \"ATP tennis matches schema\"columns:\n- name: tourney_id\ndescription: \"The ID of the tournament.\"tests:\n- not_null\n- name: winner_id\ndescription: \"The ID of the winning player.\"tests:Listing 8.10 A dbt schema asserting various qualities of our dataset177 8.3 Data transformation and modeling with dbt\n- not_null\n- name: loser_id\ndescription: \"The ID of the losing player.\"tests:\n- not_null\n- name: surface\ndescription: \"The surface of the court.\"\ntests:\n- not_null- accepted_values:\nvalues: ['Grass', 'Hard', 'Clay']\nYou don\u2019t need to create a test for every field\u2014only the ones that are necessary for\nyour use case. You don\u2019t need to create tests for fields where the data might be dirty or\nwhere it doesn\u2019t matter if it\u2019s dirty.\nNOTE If you\u2019re feeling adventurous, you ca n try to add assertions for some of\nthe other fields.\nTo run the tests, we can run the following command:\ndbt test\n...\n10:57:39 Found 1 model, 5 tests, 1 source, 0 exposures, 0 metrics,\n\u27a5351 macros, 0 groups, 0 semantic models\n10:57:39\n10:57:39 Concurrency: 1 threads (target='dev')\n10:57:3910:57:39 1 of 5 START test accepted_values_matches_surface__Grass__Hard\n\u27a5Clay ....... [RUN]\n10:57:39 1 of 5 PASS accepted_values_matches_surface__Grass__Hard__Clay\n\u27a5............. [PASS in 0.14s]\n10:57:39 2 of 5 START test not_null_matches_loser_id\n\u27a5................................ [RUN]\n10:57:39 2 of 5 PASS not_null_matches_loser_id\n\u27a5...................................... [PASS in 0.12s]\n10:57:39 3 of 5 START test not_null_matches_surface\n\u27a5................................. [RUN]\n10:57:39 3 of 5 PASS not_null_matches_surface\n\u27a5....................................... [PASS in 0.12s]\n10:57:39 4 of 5 START test not_null_matches_tourney_id\n\u27a5.............................. [RUN]\n10:57:39 4 of 5 PASS not_null_matches_tourney_id\n\u27a5.................................... [PASS in 0.12s]\n10:57:39 5 of 5 START test not_null_matches_winner_id\n\u27a5............................... [RUN]\n10:57:39 5 of 5 PASS not_null_matches_winner_id\n\u27a5..................................... [PASS in 0.12s]\n10:57:3910:57:39 Finished running 5 tests in 0 hours 0 minutes and 0.66\n\u27a5seconds (0.66s).\n...\nEverything looks good so far.178 CHAPTER  8Building data pipelines with DuckDB\n While dbt comes with severa l built-in assertions, we will sometimes want to do\nfiner-grained tests than this. For example, we  might want to test the range of the val-\nues in the tourney_date  column. \n To do this, we\u2019ll need to install a pack age, like dbt_expectatio ns, that offers more\nassertions. At the top level of the project,  create the packages.y ml file, and add the\nfollowing:\npackages:\n- package: calogica/dbt_expectations\nversion: 0.10.1\nTo install the package, we can run the following command:\ndbt deps\nWe should see the following output:\n19:30:56 Running with dbt=1.6.719:30:56 Installing calogica/dbt_expectations19:30:56 Installed from version 0.10.119:30:56 Up to date!\n19:30:56 Installing calogica/dbt_date\n19:30:57 Installed from version 0.10.019:30:57 Up to date!\nWe can then update models/atp/schema.yml to  add an assertion that checks that the\nvalues for tourney_date  are between January 1, 2023 and December 31, 2023, as\nshown in the following listing.\nmodels:\n- name: matches\n# Keep the original assertions, too- name: tourney_date\ndescription: \"Verify that the tournament started in 2023\"\ntests:\n- dbt_expectations.expect_column_values_to_be_of_type:\ncolumn_type: date\n- dbt_expectations.expect_column_min_to_be_between:\nmin_value: \"CAST('2023-01-01' AS DATE)\"\nmax_value: \"CAST('2023-12-31' AS DATE)\"\nIf we rerun dbt test , this new assertion will be picked  up, and we will see it in the test\noutput.\n8.3.5 Transforming all CSV files\nSo far, we\u2019ve only worked with the 2023 matc hes, but there is a list of other CSV files\ngoing back to 1968 that we need to process. We started with only one file to keep our\nwaiting times low, getting immediate feedback regarding whether our schema and\nmodels work or not. Listing 8.11 Adding tests to the schema defined in listing 8.10179 8.3 Data transformation and modeling with dbt\n Let\u2019s change the sources for our pipeli ne first, starting with models/atp/\nsources.yml. We\u2019re going to change the external_location  to return a function that\niterates over the years from 1968 to 2023, creates a list of all the CSV file URLs, and\nreads them using the read_csv_auto  function. This is a prime example of how\nDuckDB is used as a processing tool  in this pipeline, not as storage. \nversion: 2\nsources:\n- name: github\nmeta:\nexternal_location: >\n(FROM read_csv_auto(\nlist_transform(\nrange(1968, 2023),y -> 'https://raw.githubusercontent.com/JeffSackmann/\n\u27a5tennis_atp/master/atp_matches_' || y || '.csv'\n),types={'winner_seed': 'VARCHAR', 'loser_seed': 'VARCHAR'}\n))\nformatter: oldstyle\ntables:\n- name: matches_file\nOnce we\u2019ve done that, we can run dbt run  again to generate a new Parquet file. Let\u2019s\nquickly explore the contents of our new Parquet file:\nSELECT count(*) FROM 'output/matches.parquet';\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 188934 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThat\u2019s a lot more rows than we had before, so we can assume that it worked. Let\u2019s run\ndbt test  to see if our assertions still pass:\n12:58:05 Finished running 7 tests in 0 hours 0 minutes and 1.49\n\u27a5seconds (1.49s).\n12:58:05\n12:58:05 Completed with 3 errors and 0 warnings:\n12:58:0512:58:05 Failure in test accepted_values_matches_surface__Grass__Hard\n\u27a5Clay (models/atp/schema.yml)\n12:58:05 Got 1 result, configured to fail if != 012:58:05\n12:58:05 compiled Code at target/compiled/dbt_transformations/models/\n\u27a5atp/schema.yml/accepted_values_matches_surface__Grass__Hard__Clay.sqlListing 8.12 Computing a list of CSV files as sources for the pipeline\nGenerates a list of all the \nyears from 1968 to 2023Transforms each year into a\nURL that ends with\natp_matches_<year>.csv\nWe need to use the oldstyle formatter so that \nwe can use the {} characters in the query used by the externa l_location property.180 CHAPTER  8Building data pipelines with DuckDB\n12:58:05\n12:58:05 Failure in test dbt_expectations_expect_column_min_to_be\n\u27a5between_matches_tourney_date__CAST_2023_12_31_AS_DATE___CAST_2023\n\u27a501_01_AS_DATE_ (models/atp/schema.yml)\n12:58:05 Got 1 result, configured to fail if != 0\n12:58:0512:58:05 compiled Code at target/compiled/dbt_transformations/models/\n\u27a5atp/schema.yml/dbt_expectations_expect_column_3a4294205f95862ee31c\n\u27a5ce05b1e1ebf7.sql\n12:58:05\n12:58:05 Failure in test not_null_matches_surface (models/atp/schema.yml)\n12:58:05 Got 2937 results, configured to fail if != 012:58:05\n12:58:05 compiled Code at target/compiled/dbt_transformations/models/\n\u27a5atp/schema.yml/not_null_matches_surface.sql\n12:58:05\n12:58:05 Done. PASS=4 WARN=0 ERROR=3 SKIP=0 TOTAL=7\nHmm, not this time. We\u2019ve got three broken tests:\n\uf0a1not_null_matches_surface \u2014This means there are NULL  surfaces.\n\uf0a1accepted_values_matches_surface GrassHard__Clay \u2014This means there are\nsurfaces that aren\u2019t Grass , Hard , or Clay .\n\uf0a1dbt_expectations_expect_column_min_to_be_between_matches_tourney_\ndateCAST_2023_12_31_AS_DATECAST_2023_01_01_AS_DATE \u2014This means some\nmatches have a date that isn\u2019t in 2023.\nLet\u2019s debug them, starting with the surface  field, which now has some NULL  values,\nalthough it was only supposed to have the values Grass , Hard , and Clay :\nFROM 'output/matches.parquet' SELECT surface, count(*) GROUP BY ALL;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 surface \u2502 count_star() \u2502\n\u2502 varchar \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Clay \u2502 67537 \u2502\n\u2502 Carpet \u2502 20900 \u2502\u2502 Hard \u2502 74814 \u2502\n\u2502 Grass \u2502 22746 \u2502\n\u2502 \u2502 2937 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nCarpet  looks like a valid value, but there are al so almost 3,000 rows that don\u2019t have a\nsurface. We\u2019re going to upd ate schema.yml to allow Carpet  as a valid value and update\nthe matches.sql model to filter out the matches that have a NULL  surface. \n We also have a perhaps more  predictable problem in the tourney_date  field,\nwhere we now have dates that aren\u2019t necessa rily in 2023. This has happened because\nwe\u2019re now ingesting data from all years rather than just 2023. We\u2019ll updateschema.yml to allow a range of dates from  December 1967 to December 2023. Our\nnew models/atp/schema.yml file looks like the following listing.181 8.3 Data transformation and modeling with dbt\n \nversion: 2\nmodels:\n- name: matches\ndescription: \"ATP tennis matches schema\"columns:\n- name: tourney_id\ndescription: \"The ID of the tournament.\"tests:\n- not_null\n- name: winner_id\ndescription: \"The ID of the winning player.\"tests:\n- not_null\n- name: loser_id\ndescription: \"The ID of the winning player.\"tests:\n- not_null\n- name: surface\ndescription: \"The surface of the court.\"tests:\n- not_null- accepted_values:\nvalues: ['Grass', 'Hard', 'Clay', 'Carpet']\n- name: tourney_date\ndescription: \"The date when the tournament started\"tests:\n- dbt_expectations.expect_column_values_to_be_of_type:\ncolumn_type: date\n- dbt_expectations.expect_column_min_to_be_between:\nmin_value: \"CAST('1967-12-01' AS DATE)\"max_value: \"CAST('2023-12-31' AS DATE)\"\nThe transformation in models/atp/mat ches.sql now gets an additional WHERE  clause\nto exclude all matches with out a surface, as shown in  the following listing. \n{{ config(\nmaterialized='external',location='output/matches.parquet',format='parquet'\n)} }\nWITH noWinLoss AS (\nSELECT COLUMNS(col ->\nNOT regexp_matches(col, 'w_.*')AND NOT regexp_matches(col, 'l_.*')\n)FROM {{ source('github', 'matches_file') }}\n)Listing 8.13 Adding details to our schema\nListing 8.14 Updating our transformations to deal with oddities in some of the new filesCarpet is allowed \nnow too.182 CHAPTER  8Building data pipelines with DuckDB\nSELECT * REPLACE (\ncast(strptime(tourney_date, '%Y%m%d') AS date) as tourney_date\n)FROM noWinLoss\nWHERE surface IS NOT NULL\nDue to the changes in the model, we must first run the pipeline again with dbt run\nbefore running dbt test  to make all tests pass, ensuri ng our pipeline is working end\nto end. We started out with CSV files on GitHub and have successfully transformed\nthem into a single Parquet file and cleaned up the data as part of the process. \nNOTE The next step would be to set up a production pipeline to go along\nwith the dev one. It would be fairly similar, but perhaps, we\u2019d write the Par-quet file to an S3 bucket , rather than the local file system. We\u2019ll leave that as\nan exercise for the reader.\n8.4 Orchestrating data pipelines with Dagster\nSo far in this chapter, we\u2019ve learned about tools that can load data into DuckDB from\nexternal sources or transfor m data between formats usin g DuckDB. These are import-\nant tasks, but we are still mi ssing a piece of the data pipeline puzzle: How do we trig-\nger or orchestrate the transformation or ingestion code?\n In a world with no orchestration tools,  we would need to write our own manual\nscheduling and execution code . We\u2019d need to set up cron jobs to run dbt commands\nand write custom scripts to handle the sequencing and dependencies of dbt tasks.\n Luckily for us, tools like Ai rflow, Luigi, Kestra , Prefect, and Dagster (the tool we\u2019ll\nbe using in this chapter) do exist. These tools control the orchestration of data pipe-\nlines, which is what we\u2019ll be exploring in th is section. As in the previous section, we\nprovide the pipeline we are bu ilding here in our GitHub repository as several steps:\nhttps:/ /mng.bz/x2og .\n Dagster is a cloud-native tool built to manage and organize data flows in modular\npipelines. One of its core concepts is the (software-defined) asset, which is an objectin persistent storage, such as a table, fi le, or machine learning model. A software-\ndefined asset is a description, in code, of an asset that should exist and how to pro-\nduce and update that asset. Assets form pa rts of data processing jobs, which can then\nbe scheduled.\n Like dlt and dbt, Dagster is written in Python. It aims to make it easier for develop-\ners to work with data throughout differen t stages, such as creating, deploying, and\nmonitoring data assets by using Python fu nctions to describe the data assets. These\nfunctions tell Dagster what data assets to create or update as well as which dependen-\ncies an asset has. Describing assets with Python function lets us describe any depen-dencies and interactions as verifiable  code, which is a big advantage over\nconfiguration-based tools, especially for deve lopers. We will show how to create these\nasset functions in the following sections with our concrete pipeline. Dagster offersFiltering out NULL \nvalues for surface183 8.4 Orchestrating data pipelines with Dagster\nespecially good support for data lineage and data provenance, as those are important\naspects of data pipelines for auditing, debu gging, and tracing. It has an integration\nwith DuckDB, called dagster-duckdb , which we\u2019ll be using in this section. \n We\u2019re going to use the same tennis dataset we used in the previous section, but this\ntime, we\u2019re going to load the data into Du ckDB, rather than create a Parquet file. In\naddition to loading tennis matches, we\u2019ll also  import player profiles, and we\u2019ll see how\nto load static data from pandas DataFrames.\n Let\u2019s start by installing the main depe ndencies required to use Dagster and\nDuckDB:\npip install dagster dagster-duckdb\nWe\u2019ll also install dagster-webserver , which is required to run the Dagster UI and can\nbe used to visualize pipelines:\npip install dagster-webserver\n8.4.1 Defining assets\nWith those dependencies installed, we\u2019re re ady to create our pipeline. Let\u2019s create a\ndirectory called atp and add the following files:\n\uf0a1atp/__init__.py_\u2014Orchestration code will go in here.\n\uf0a1atp/assets.py\u2014Asset definiti on code will go in here.\nWe\u2019re going to start by creating a Python function to define the asset in assets.py\ncalled atp_matches_dataset , which loads the atp_matches_*.csv files into the\nmatches  table in DuckDB. \nNOTE You can also copy the code for these two files from the GitHub reposi-\ntory: https:/ /mng.bz/AdMg .Possible error when installing dagster-webserver\nAt the time of writing, you may see the fo llowing error, which re sults in the Dagster\nserver shutting down when trying to install the dagster-webserver package:\nImportError: cannot import name 'appengine' from\n\u27a5'requests.packages.urllib3.contrib'\n....\nraise Exception(\nException: dagster-webserver process shut down unexpectedly\n\u27a5with return code 1\nIf you see this error, you\u2019ll need to pin the dependencies urllib3  and requests-\ntoolbelt  to the following versions:\npip install urllib3==1.26.15 requests-toolbelt==0.10.1184 CHAPTER  8Building data pipelines with DuckDB\nIt constructs the list of CSV URLs based on the year range (1968 to 2024) in Python\ncode and then uses the DuckDB Python API to load the CSV files into DuckDB.\nDuring loading, it also converts the tourney_date  column into a date type.\nfrom dagster_duckdb import DuckDBResource\nfrom dagster import asset\n@asset\ndef atp_matches_dataset(duckdb_resource: DuckDBResource) -> None:\nbase = \"https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master\"\ncsv_files = [\nf\"{base}/atp_matches_{year}.csv\"\nfor year in range(1968,2024)\n]\ncreate_query = \"\"\"\nCREATE OR REPLACE TABLE matches AS\nSELECT * REPLACE(\ncast(strptime(tourney_date, '%Y%m%d') AS date) as tourney_date\n)FROM read_csv_auto($1, types={\n'winner_seed': 'VARCHAR',\n'loser_seed': 'VARCHAR','tourney_date': 'STRING'\n})\n\"\"\"\nwith duckdb_resource.get_connection() as conn:\nconn.execute(create_query, [csv_files])\nNext, we\u2019re going to update __init__.py to configure atp_matches_dataset  as an asset\nand specify the location where the DuckDB database should be created. This file\nessentially controls the available libraries, determines when jobs should be run, and\nuses information from the environment when necessary. We\u2019re also going to create a\njob that contains our assets and a schedul e that will run the job once an hour. \nfrom dagster_duckdb import DuckDBResource\nfrom dagster import (\nAssetSelection,ScheduleDefinition,\nDefinitions,\ndefine_asset_job,load_assets_from_modules,\n)\nfrom . import assetsatp_job = define_asset_job(\"atp_job\", selection=AssetSelection.all())Listing 8.15 Defining the first asset we want to process in our Dagster pipeline\nListing 8.16 Defining the Dagster job in __init__.pyConstructs the \nlist of CSV files\nImports the CSV files\ninto DuckDB\n$1 refers to the first parameter \npassed to the ex ecute function, \nwhich is the list of CSV files.\nConverts tourney_date\nto date type\nPasses in the list of CSV \nfiles as a parameter\nThe job\ndefinition185 8.4 Orchestrating data pipelines with Dagster\natp_schedule = ScheduleDefinition(\njob=atp_job,\ncron_schedule=\" 0**** \" ,\n)\nall_assets = load_assets_from_modules([assets])\ndefs = Definitions(\nassets=all_assets,jobs=[atp_job],\nresources={\"duckdb\": DuckDBResource(\ndatabase=\"atp.duckdb\",\n)},\nschedules=[atp_schedule],\n)\n8.4.2 Running pipelines\nWe can then launch the Dagster UI by running the dagster dev  command with the\n-m flag pointing at the atp dire ctory to find our definitions:\ndagster dev -m atp\nThen, navigate to http:/ /localhost:3000 in your web browser, where you\u2019ll see the dia-\ngram in figure 8.4, which shows the job, schedule, and asset we defined.\nFigure 8.4 The initial Dagster job and asset graph in the Dagster UI\nWe can manually run the pipeline by clicking  on Materialize at the top of the screen.\nIt will take a few seconds to run, but if we  click the Refresh button, we\u2019ll see that our\nasset has now materialized, as seen in figure 8.5.This is a schedule that runs once an hour. \nSee crontab.guru (https://crontab.guru) for a primer on the Cron syntax.\nDagster picks up all the functions \nthat have the @asset annotation.\nThe definitions tie everything \ntogether\u2014assets, jobs, \nresources, and schedules.\nThe location of the DuckDB \ndatabase relative to the \nlocation where Dagster is run \n186 CHAPTER  8Building data pipelines with DuckDB\nWe can check that the matches have been  successfully loaded by querying the\nDuckDB database. As Dagster do es not keep the database open or locked after materi-\nalizing a pipeline, we don\u2019t have to stop  any process prior to accessing the DuckDB\ndatabase:\nduckdb atp.duckdb 'SELECT count(*) FROM matches'\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 191920 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNOTE At the time of writing, there are 191920  matches in the dataset, but this\nnumber will likely have increased by th e time you try out this code sample. As\nlong as it\u2019s more than 0, the import has likely worked!\nWe\u2019ve now successfully loaded  the roughly 200,000 tennis matches into DuckDB using\nDagster. \n8.4.3 Managing dependencies in a pipeline\nSo far, we\u2019ve only imported tennis match data. But applications that use the database\nmight also need other data on the tennis pl ayers that participated in matches, so we\nneed to import that data too. Loading the actual player informat ion will be an addi-\ntional part of our pipeline. We\u2019ll then cr eate a dependency between the players asset\nand the matches  asset we\u2019ve already created. \n To do this, we\u2019ll add two more assets to assets.py, using Python functions. First up is\natp_players_dataset , which ingests the players. The player informatio n is contained\nin a file named atp_players.csv. \n The values in the dob column are in the format yyyymmdd , but there are some rows\nwhere the last four digits are 0000 . Since 00 isn\u2019t a valid month or day, we\u2019re going to\ndefault each to 01 instead so that we can coerce the values to a date type, using the\nstrptime  function. The asset we are about to ad d not only queries the source but also\napplies some transformation. It is defined as  follows and must be added to assets.py, as\nshown in the following listing. \nNOTE As with the first step, you can copy the code from the GitHub reposi-\ntory: https:/ /mng.bz/ZEn5 .\nFigure 8.5 The state of the dataset in Dagster will \nchange from never materialized to materialized.187 8.4 Orchestrating data pipelines with Dagster\n \n@asset\ndef atp_players_dataset(duckdb: DuckDBResource) -> None:\nbase = \"https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master\"csv_file = f\"{base}/atp_players.csv\"\nwith duckdb.get_connection() as conn:\nconn.execute(\"\"\"\nCREATE OR REPLACE TABLE players AS\nSELECT * REPLACE(\nCASE\nWHEN dob IS NULL THEN NULL\nWHEN SUBSTRING(CAST(dob AS VARCHAR), 5, 4) = '0000' THEN\nCAST(strptime(\nCONCAT(SUBSTRING(CAST(dob AS VARCHAR), 1, 4), '0101'),\n'%Y%m%d'\n) AS date)\nELSE\nCAST(strptime(dob, '%Y%m%d') AS date)\nEND AS dob\n)\nFROM read_csv_auto($1, types = {\n'dob': 'STRING'\n});\n\"\"\", [csv_file])\nThe atp_players.csv file  contains columns for name_first  and name_last , but for eas-\nier filtering by a user, we\u2019d like the resulting players  table to have an additional\nname_full  column that concatenates the other two fields. \n To achieve that, we add another asset, atp_players_name_dataset , which has a\ndependency on atp_players_dataset , which we just defined, and adds the name_\nfull  column to the players  table. The name_full  column will be a result of concate-\nnating name_first  and name_last , separated by a space. \n@asset(deps=[atp_players_dataset])\ndef atp_players_name_dataset(duckdb: DuckDBResource) -> None:\nconcatenate_query = \"\"\"ALTER TABLE players ADD COLUMN name_full VARCHAR;UPDATE playersSET name_full = name_first || ' ' || name_last\"\"\"\nwith duckdb.get_connection() as conn:\nconn.execute(concatenate_query, [])\nCreating a dependency on the atp_players_dataset  means that Dagster will give us a\nwarning if we try to materialize atp_players_name_dataset  without first materializing\natp_players_dataset , and when running the pipeline , Dagster orders the executionListing 8.17 A Dagster asset that uses DuckDB to transform data from an external source\nListing 8.18 A Dagster asset that transforms existing dataLeaves the NULL \ndates as they areSome dates have month and\nday set to 00. This code sets\nboth to 01 so that strptime\ncan parse them.\nParses dates in \nthe correct format with strptime\nPasses in the CSV \nfiles as a parameter\nDeclares a dependency on\nthe atp_players_dataset\nConcatenates the \nname_first and \nname_last columns188 CHAPTER  8Building data pipelines with DuckDB\nof the assets accordingly. This ma kes sense, as we can\u2019t update the players  table to\nadd the name_full  column if the players  table doesn\u2019t exist! This kind of dependent\nasset operation can be used for many differ ent use cases, such as data refactoring,\ncleaning, or augmentation, where one or more  assets are used as input to execute the\noperation. \n To see the new assets, we\u2019ll need to stop and start the dagster dev  command.\nAfter we\u2019ve done that, we\u2019ll see the asset graph shown in figure 8.6.\nFigure 8.6 Dagster asset graph with players included\nWe can then materialize all the assets via th e UI, or we can do it from the terminal by\nrunning the following command:\ndagster job execute -m atp --job atp_job\nA truncated version of the output from this command is as follows:\natp_matches_dataset - STEP_START - Started execution of step\n\u27a5\"atp_matches_dataset\".\natp_players_dataset - STEP_START - Started execution of step\n\u27a5\"atp_players_dataset\".\natp_matches_dataset - STEP_SUCCESS - Finished execution of step\n\u27a5\"atp_matches_dataset\" in 413ms.\natp_players_dataset - STEP_SUCCESS - Finished execution of step\n\u27a5\"atp_players_dataset\" in 1.51s.\natp_players_name_dataset - STEP_START - Started execution of step\n\u27a5\"atp_players_name_dataset\".\natp_players_name_dataset - STEP_SUCCESS - Finished execution of step\n\u27a5\"atp_players_name_dataset\" in 49ms.\nWe can see that atp_matches_dataset  and atp_players_dataset  both start\nrunning immediately, but atp_players_name_dataset  only starts running once\natp_players_dataset  has completed, as it has a depe ndency on that asset. We can\n189 8.4 Orchestrating data pipelines with Dagster\ncheck that the players have loaded correc tly by running the following query, which\nonly returns columns with the name_  prefix:\nduckdb atp.duckdb \\\n'SELECT COLUMNS(col -> col LIKE \"name_%\") FROM players LIMIT 5'\nWe can see from the output that the name_full  column has been successfully added\nto each record:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name_first \u2502 name_last \u2502 name_full \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Gardnar \u2502 Mulloy \u2502 Gardnar Mulloy \u2502\n\u2502 Pancho \u2502 Segura \u2502 Pancho Segura \u2502\u2502 Frank \u2502 Sedgman \u2502 Frank Sedgman \u2502\n\u2502 Giuseppe \u2502 Merlo \u2502 Giuseppe Merlo \u2502\n\u2502 Richard \u2502 Gonzalez \u2502 Richard Gonzalez \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nYou might have noticed that the asset named atp_players_name_dataset  did not use\nany external source data but transformed data  that had already been  ingested into the\nstore. Dagster enables many more transforma tions in assets. You can use all function-\nality and libraries that are availa ble in Python, such as pandas. \n8.4.4 Advanced computation in assets\nAs we learned in chapter 6, DuckDB can query pandas DataFrames, and we can use\nthat functionality in Dagster as well. The following asset ingests a DataFrame that pro-\nvides the tennis tournament levels metadata. \nNOTE Tennis tournaments are categorized by the amount of prize money\nand the number of rankings points available. Grand Slam  is the most valuable\ntournament category, followed by Tour  Finals , Masters  1000s , Other  Tour\nLevel , Challengers , and ITFs . \nWhile this example is more of a demonstr ation, imagine an ac tual computation you\ncan do in Python with all the statistical an d numerical features pandas has to offer. By\nbeing able to integrate DuckDB and Dagste r assets through Python and its capabili-\nties, even doing complex computations and operations as part of data pipelines\nbecomes possible and easy for Python developers.\n Let\u2019s add the asset in the fo llowing listing to assets.py.\nimport pandas as pd\n@asset\ndef atp_levels_dataset(duckdb: DuckDBResource) -> None:\nlevels_df = pd.DataFrame({\n\"short_name\": [Listing 8.19 Deriving new data by computing new values in a Dagster asset190 CHAPTER  8Building data pipelines with DuckDB\n\"G\", \"M\", \"A\", \"C\", \"S\", \"F\"\n],\n\"name\": [\n\"Grand Slam\", \"Tour Finals\", \"Masters 1000s\",\n\"Other Tour Level\", \"Challengers\", \"ITFs\"\n],\"rank\": [\n5, 4, 3, 2, 1, 0\n]\n})\nwith duckdb.get_connection() as conn:\nconn.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS levels ASSELECT * FROM levels_df\n\"\"\")\nThe updated asset graphs can be seen in the Dagster UI in figure 8.7.\nWe can then materialize these assets again via the command line:\ndagster job execute -m atp --job atp_job\nTaking a look at  the levels with\nduckdb atp.duckdb 'FROM levels'\nconfirms that our ingestion of the DataFrame worked correctly:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 short_name \u2502 name \u2502 rank \u2502\n\u2502 varchar \u2502 varchar \u2502 int64 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 G \u2502 Grand Slam \u2502 5 \u2502\n\u2502 M \u2502 Tour Finals \u2502 4 \u2502\u2502 A \u2502 Masters 1000s \u2502 3 \u2502\n\u2502 C \u2502 Other Tour Level \u2502 2 \u2502\n\u2502 S \u2502 Challengers \u2502 1 \u2502\u2502 F \u2502 ITFs \u2502 0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nFigure 8.7 Dagster asset graph with metadata included191 8.4 Orchestrating data pipelines with Dagster\nEverything looks good, and our Dagster pipeli ne has successfully ingested all the data\nrequired, but at the moment, the DuckDB da tabase resides only on our machine. A\ncomplete data pipeline would publish your da ta to the cloud so that your application\ncould use it from there. \n8.4.5 Uploading to MotherDuck\nIf we want to make the newl y created tennis database avai lable to an application, we\nmay choose to upload it to  MotherDuck, a service we learned about in chapter 7. If\nyou want to try out this last step of orch estrating a pipeline with Dagster, make sure\nyou created a MotherDuck account and have your MotherDuck token set as an envi-\nronment variable in your shell. As discusse d in chapter 7, we can take the local data-\nbase, publish and share it to MotherDuck, or  transport the data directly into a cloud\ninstance. \n To keep things concise, we\u2019ll focus on the changes needed to publish the data to\nMotherDuck directly instead of writing to  a local DuckDB database. Let\u2019s open atp/\n__init__.py and go down to the defs  variable, which should  read as follows. \ndefs = Definitions(\nassets=all_assets,jobs=[atp_job],\nresources={\"duckdb\": DuckDBResource(\ndatabase=\"atp.duckdb\",\n)},\nschedules=[atp_schedule],\n)\nTo have the pipeline uploaded to MotherDuck , we need to change the database string\nto use the MotherDuck syntax. We\u2019ll also need to provide our MotherDuck token, as\nshown in the following code listing.\nimport dotenv\nimport os\ndotenv.load_dotenv()\nmduck_token = os.getenv(\"motherduck_token\")defs = Definitions(\nassets=all_assets,jobs=[atp_job],\nresources={\"duckdb\": DuckDBResource(\ndatabase=f\"md:md_atp_db?motherduck_token={mduck_token}\",schema=\"main\"\n)},\nschedules=[atp_schedule],\n)Listing 8.20 Adding a new definition to our Dagster job inside atp/__init__.py\nListing 8.21 Changing the location of the DuckDB store to point to MotherDuck\nThis is only necessary  for running from the \nCLI. The Dagster UI au tomatically picks up \nproperties defined in the .env file.\nConnects to the md_atp_db \ndatabase in MotherDuck192 CHAPTER  8Building data pipelines with DuckDB\nIf we now run the atp_job , the data will be uploaded\nto MotherDuck into a database named md_atp_db .\nMake sure you exported the necessary token to your\nshell first:\ndagster job execute -m atp --job atp_job\nOnce the job has finished, we can confirm the data-\nbase has been created from the MotherDuck UI at\nhttps:/ /app.mo therduck.com . We should see some-\nthing similar to figure 8.8.\n Dagster orchestrated data  loading and transforma-\ntion as well as computing additional information. As afinal step, it published the re sult to a cloud data store, ready to build a product upon,\nwhich we\u2019ll do in the next chapter. \nSummary\n\uf0a1Data pipelines let us load and transfor m data in an automated and consistent\nway.\n\uf0a1DuckDB can be easily integr ated into various data pipeline tools, like dlt, dbt,\nand Dagster.\n\uf0a1DuckDB can play many roles in a data pipeline: data loading, transformation,\nand storage.\n\uf0a1Data transformation and filtering can be done both with SQL and Python APIs.\n\uf0a1Data can be loaded remotely from a set of CSV files (and other sources and for-\nmats) and pandas DataFrames.\n\uf0a1Using Python for data pipelines allows for powerful transformations and com-putations.\n\uf0a1Declaring dependencies helps orchestrate the order of operations in a pipeline.\n\uf0a1MotherDuck is a suitable destinatio n for DuckDB data in the cloud. \nFigure 8.8 The ATP dataset on \nMotherDuck193Building and\ndeploying data apps\nIn section 8.4, we learned how to build a pipeline that ingested data into a local\nDuckDB database and into one running on  MotherDuck. While many data analysts\nwill be happy to interact with the data in  DuckDB with SQL quer ies, other users will\nprefer to have an interface that doesn\u2019 t require them to write code. To provide\nvalue for those users, we want to create applications that retrieve the information\nfrom DuckDB\u2014either via SQL queries or by  any of the other means we\u2019ve learned\nso far, such as the relational Python API. Of equal importance, we aim to present\nthis process in an accessible fashion and in a way that readers find meaningful.This chapter covers \n\uf0a1Building an interactive web application with Streamlit\n\uf0a1Deploying Streamlit applications with Streamlit \nCommunity Cloud\n\uf0a1Rendering interactive charts with Plot.ly\n\uf0a1Creating a dashboard for business intelligence with Apache Superset\n\uf0a1Creating charts from a custom SQL query with Apache Superset194 CHAPTER  9Building and deploying data apps\n Some datasets can be represented in a tabular fashion; more often, it is helpful to\nvisualize and summarize that data as charts . This also makes the data more accessible\nand allows users to answer ques tions, offer insights, or suppo rt decisions, which is crit-\nical for most data-informed use cases. Creati ng ideal charts that don\u2019t skew the mean-\ning of a dataset is a topic for another book, though. We are going to focus on the\ntechnical side of things here and explore Du ckDB\u2019s integration with some of the exist-\ning tools for building accessibl e frontends. First, we create  an end-user-facing applica-\ntion with Streamlit and later a business intelligence (BI) da shboard with Apache\nSuperset. While interactive applications usually focus on ready-made reports and\nrestrict interactivity on filtering those reports, BI dashboards  usually aggregate data\nfrom many possible sources in such a way that they allow a holistic overview, usually\nnot based on raw numbers but dedicated charts. \n9.1 Building a custom data app with Streamlit\nWe already dealt with Association of Tennis Professional s (ATP) data in chapters 5 and\n8. In chapter 5, we aggregat ed ATP rankings from several CSV sources into one table\nand, eventually, only one Parquet file. Later, in chapter 8, we consumed player data\nfrom web resources as part of a data pipeli ne. Eventually, we stored everything in a\nDuckDB database, as you can see in figure 9.1. \nFigure 9.1 ATP database schemamatchestourney_id:varchar\ntourney_name:varchar\nsurface:varchar\ndraw_size:bigint\ntourney_level:varchar\ntourney_date:date\nmatch_num:bigintwinner_id:bigint\nloser_id:bigint\nwinner_rank:varchar\nwinner_rank_points:varchar\nloser_rank:varchar\nloser_rank_points:varcharlevelsname:varchar\nshort_name:varchar\nrank:bigint\nplayersplayer_id:bigint\nname_first:varchar\nname_last:varchar\nhand:varchar\ndob:dateioc:varchar\nheight:bigint\nwikidata_id:varchar\nname_full:varchar\nrounds\nname:varchar\norder:biginttourney_level\nloser_id\nwinner_id195 9.1 Building a custom data app with Streamlit\nBy now, you know how to use the database, but that is not necessarily true for your\nusers. They might not know SQL or have di rect access to the database. An application\ncan help them navigate the data and struct ure, while combining interactive filters with\nuseful visualizations. Let\u2019s build  an application for that inst ead. Our goal is to provide\na platform where users can search for and analyze head-to-head player statistics. We\nare going to use a tool called Streamlit for crafting an interactive web interface. And,\nof course, we\u2019ll use DuckDB to handle all database tasks. Throug h this exercise, you\nwill learn how to combine Streamlit and DuckDB to develop a data app with an\nemphasis on simplicity and ease of deployment.\n9.1.1 What is Streamlit?\nStreamlit  is a library that makes it possible to  create interactive web apps using only\nPython\u2014no knowledge of JavaScript-based  frontend libraries or frameworks is\nrequired. For data engineers, data scientists, and backend developers, who are oftennot full-stack engineers, this is a good wa y to prototype and build quick, data-driven\napplications. Unlike other visualization tool s in the data and BI space that are low-\ncode, visual builders, Streamlit is a code-f irst tool. You write your apps using Python\nand the tools and libraries, such as pandas , scikit-learn, matplotlib, and DuckDB, that\nyou\u2019re already familiar with. For all of these reasons, Streamlit is especially useful for\ncreating frontends for data or machine le arning (ML) apps. In  addition, Snowflake\nInc.\u2014the company behind Streamlit\u2014offers  the Streamlit Community Cloud, a plat-\nform where you can deploy and manage your apps for free.\n9.1.2 Building our app\nStreamlit comes as a Python pack age, so you need to install that first in your terminal:\npip install streamlit\nWe\u2019ll also need to make sure we have the DuckDB Python package installed if we\nhaven\u2019t already done that:\npip install duckdb\nStreamlit does not generate a skeleton for your application. Instead, you create an\napplication, represented by a Python script , that will act as your canvas, on which\nyou\u2019ll draw charts, text, widgets, tables, an d more. Let\u2019s start with that. Create a file\nnamed app.py containing the following code , which will define a wide layout and ren-\nder a title when run via Streamlit.\nimport streamlit as st\nst.set_page_config(layout=\"wide\")\nst.title(\"ATP Head to Head\")Listing 9.1 Writing a minimal Streamlit app196 CHAPTER  9Building and deploying data apps\nIf we then go back to the terminal, we can run the following command to launch the\napp. As soon as you run the script as shown,  a local Streamlit server will spin up, and\nyour app will open in a new tab in your default web browser:\nstreamlit run app.py\nIn the terminal, you\u2019ll see something like the following output:\nYou can now view your Streamlit app in your browser.\nLocal URL: http://localhost:8501\nNetwork URL: http://192.168.86.207:8501\nThe initial page is shown in figure 9.2.\nFigure 9.2 Initial Streamlit app\nThis is not all that interesting so far, but it\u2019s a start!\n Next, we\u2019re going to connect the Streamlit app to the atp.duckdb  database we cre-\nated in section 8.4, which you can also fi nd in the book\u2019s GitHub repository under\nhttps:/ /mng.bz/RZDD . \n The following snippet that will be part of our app.py script will bring in the\nDuckDB Python package and connect to the database stored in atp.duckdb , as\ndescribed in chapter 6. We\u2019re opening the da tabase in read-only mode since we don\u2019t\nintend to change the data:\nimport duckdb\natp_duck = duckdb.connect('atp.duckdb', read_only=True)\nRemember that this setup is different from a client\u2013serve r-based database deployment.\nThe database runs embedded in the same pr ocess as the Streamli t application. As a\nresult, the frontend to be rendered will not require an y remote database connection.\n Next, we create a function inside the application named search_players  that will\nsearch for the provided player name in the matches  table. The function takes in a\nsearch term and then uses it as part of a query string to find records from the matches\ntable we populated in chapter 8. The query is about finding all ro ws that contain the\nsearch term either in the winner_name  or loser_name  columns. In both WHERE  clauses,\n197 9.1 Building a custom data app with Streamlit\nwe make use of the fact that we can refer to  aliases given to a column later. As shown\ni n  l i s t i n g  9 . 2 ,  w e  u s e  p r e p a r e d  s t a t ements with parameterized queries ( https:/ /\nmng.bz/2K29 ) to pass the search term as a named parameter to the query, avoiding\nthe possibility of an SQL injection (we spoke about how to avoid SQL injections inmore detail in chapter 6.2.3). \ndef search_players(search_term):\nquery = '''\nSELECT DISTINCT winner_name AS playerFROM matches\nWHERE player ilike '%' || $search_term || '%'\nUNIONSELECT DISTINCT loser_name AS player\nFROM matches\nWHERE player ilike '%' || $search_term || '%''''\nvalues = atp_duck.execute(query, {\"search_term\":search_term}).fetchall()\nreturn [value[0] for value in values]\nStreamlit is all about writing out data in various\nforms; it supports lists, maps, DataFrames, and\nmore. We use the st instance we imported\nfrom the streamlit  package, as shown in the\nfollowing code, to write out the result of the\nsearch_players  function, producing HTML\nthat looks as shown in figure 9.3: \nst.write(search_players(\"Novak\"))\nSo far, we have created a simple, yet fully func-\ntional page that displays a list of matches in\nwhich a specific player appeared. This gives us\nthe groundwork to make things interactive forthe users. \n9.1.3 Using Streamlit components\nThe app would be pretty boring if it only displayed matches for a specific player. Let\u2019s\ncreate an input field for the search term. \n Streamlit thinks in components . Instead of writing individual HTML fragments and\nJavaScript code for client-side interactiv ity, you declare the use of a component,\nparameterize it, and rely on the component to then render the necessary pieces onto\nyour web page. One such component is the streamlit-searchbox , published as a\nlibrary on GitHub under m-wrzr/streamlit-searchbox ( https:/ /github.com/m-wrzr/\nstreamlit-searchbox ) and providing all the usual features users expect these things to\nhave, such as autocomplete. Listing 9.2 A function that uses a prepared statement\nFigure 9.3 The result of calling \nsearch_players  with Novak  as \nparameter198 CHAPTER  9Building and deploying data apps\n We can install streamlit-searchbox  by running the following command from the\nterminal:\npip install streamlit-searchbox\nOnce we\u2019ve done that, we\u2019ll import st_searchbox  in our Python code: \nfrom streamlit_searchbox import st_searchbox\nThen, we\u2019ll use that function to create a co uple of search boxes, with the initial selec-\ntion defaulted to Roger Federer and Rafael Nadal. We don\u2019t have to think about how\nto write HTML to create a two-column-wide layout; we can just ask Streamlit to do so.\nHow the components are positioned in that layout is also taken care of for us usingthe Python keyword \nwith . Having previously defined both the variables left  and\nright , we create two scopes in which the components will be positioned. The search\nboxes created within these scopes will be labeled with Player 1  and Player 2 , respec-\ntively, and they will show  different default values. \n The most important part, however, is the first argument to st_searchbox , which is\nthe actual search_players  function defined in the fo llowing listing. That function\nwill be called when the user modifies the co ntent of the rendered search box. Add the\nfollowing code to app.py. \nleft, right = st.columns(2)\nwith left:\nplayer1 = st_searchbox(search_players,\nlabel=\"Player 1\",\nkey=\"player1_search\",\ndefault=\"Roger Federer\",placeholder=\"Roger Federer\"\n)\nwith right:\nplayer2 = st_searchbox(search_players,\nlabel=\"Player 2\",\nkey=\"player2_search\",default=\"Rafael Nadal\",\nplaceholder=\"Rafael Nadal\"\n)\nReturning to the browser, our UI should now look like figure 9.4.\nFigure 9.4 Streamlit components rendered as search boxes with default valuesListing 9.3 Parameterizing a Streamlit component with Python\nCreates two columns \non the page\nCreates a searchbox \ncomponent, using search_players as a \ndata source\n199 9.1 Building a custom data app with Streamlit\nThe search box controls are already fu lly functional. The moment we type Murr  to\nsearch for Andy Murray , the underlying search_players  will be called, and the results\nwill be passed back into the UI. This will even tually render as shown in figure 9.5. The\nmoment we click on a result, it will be assigned to the player1  variable, which rep-\nresents both the component and the rendered search box. \nFigure 9.5 Searching for Andy Murray\nSo far, we can retrieve player names from the database and assign them to variables in\nour script. While a list of players is nice, it is most likely not what  the user is interested\nin. They probably want to have a report  containing all the matches between the\nselected players, along with  information about the level of tournament and round in\nwhich the match was played.\n Streamlit is a great support for rendering pandas DataFrames as interactive tables,\nwithout any further coding required. In chap ter 6, we learned that DuckDB also has\nnative support for DataFrames. We can writ e a query that takes both player names as\nparameters, queries the matches  table again to retrieve the desired information, and\nreturns the result as a DataFrame via DuckDB\u2019s fetchdf  method, as shown in the\nPython code in the following listing. \nmatches_for_players = atp_duck.execute(\"\"\"\nSELECT\ntourney_date,tourney_name, surface, round,\nrounds.order AS roundOrder,levels.name AS level, levels.rank AS levelRank,\nwinner_name, score\nFROM matchesJOIN levels ON levels.short_name = matches.tourney_level\nJOIN rounds ON rounds.name = matches.round\nWHERE (loser_name = $player1 AND winner_name = $player2) OR\n(loser_name = $player2 AND winner_name = $player1)Listing 9.4 Querying the database and returning a DataFrame\nReturns the match, tournament level, \nand round metadata for each match\nJoins the levels table \nto get the level of \nthe tournaments for \nthe matches\nJoins the rounds table \nto get round metadata \nfor the matches200 CHAPTER  9Building and deploying data apps\nORDER BY tourney_date DESC\n\"\"\", {\"player1\":player1, \"player2\":player2}).fetchdf()\nLet\u2019s first add a heading about the list of matches that shows the players\u2019 names and\nthe number of wins they have. We can co mpute the number of wins by filtering\nmatches_for_player  to find the rows where the winner_name  matches each player\u2019s\nname. In the following code, we instruct St reamlit to define a set of three columns,\nwith the outer ones for displa ying the names of the selected players outside the search\nboxes and the middle one for the result:\nleft, middle, right = st.columns(3)\nwith left:\nst.markdown(\nf\"<h2 style='text-align: left; '>{player1}</h1>\",\nunsafe_allow_html=True\n)\nwith right:\nst.markdown(\nf\"<h2 style='text-align: right; '>{player2}</h1>\",\nunsafe_allow_html=True\n)\np1_wins = matches_for_players[\nmatches_for_players.winner_name == player1].shape[0]\np2_wins = matches_for_players[\nmatches_for_players.winner_name == player2].shape[0]\nwith middle:\nst.markdown(\nf\"<h2 style='text-align: center; '>{p1_wins} vs {p2_wins}</h1>\",\nunsafe_allow_html=True\n)\nLet\u2019s now render this DataFrame on the page. We\u2019re going to drop the roundOrder ,\nlevel , and levelRank  fields from the DataFrame we render to the page because they\ncreate a bit too much clutter; those fields will come in handy later but aren\u2019t requiredjust yet:Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax,\ndesigned to be easy to write and read. This markup is then converted into structurallyvalid HTML. It is usually used to format th e documentation of README files, but it is\nalso supported in Streamlit via the \nmarkdown  function. \nIn this case, we\u2019re using it to render the player names and the number of wins in three\nheadings: left, right, and center aligned, with custom-styled HTML inside the Mark-\ndown. That\u2019s why we also have to set unsafe_allow_html=True .Passes in player1 and player2 as\nparameters and returns a DataFrame\nCreates a container \nwith three columns\nRenders \nthe name \nof the first player\nRenders the name of the second player\nComputes the \nwins for player 1\nComputes the \nwins for player 2\nThis renders the wins per player. We\u2019re using \ncustom Markdown, so it styles nicely!201 9.1 Building a custom data app with Streamlit\nst.markdown(f'### Matches')\nst.dataframe(\nmatches_for_players.drop([\"roundOrder\", \"level\", \"levelRank\"], axis=1)\n)\nThe result of querying the database, aski ng for a DataFrame, and rendering that is\nshown in figure 9.6. When the user selects a different pair of players, the content will\nautomatically be refreshed.\nFigure 9.6 Matches between Andy Murray and Rafael Nadal\nThat all looks good, and now we can brow se through the Murray\u2013Nadal matches to\nour heart\u2019s content. While some people will  be happy with a tabular representation of\nthe match data, others will prefer a mo re visual way to present that data. \n9.1.4 Visualizing data using plot.ly\nGreat charts are important in both data-cen tric applications and dashboards, but cre-\nating great charts is difficult both technicall y and content-wise. You don\u2019t have to rein-\nvent the wheel, though, as there are va rious ready-made solu tions available for\nrendering charts and diagrams, both under commercial and open source licenses. \n One of our favorite libraries for creati ng interactive visualizations is plot.ly\n(https:/ /plotly.com/ ). plot.ly is a data visualization to ol that lets users create visually\n202 CHAPTER  9Building and deploying data apps\nappealing charts and diagrams with an intuit ive API. Streamlit prov ides direct support\nfor using plot.ly charts.\n We can install plot.ly by running  the following in the terminal:\npip install plotly\nNext, back in app.py, import plotly.express , which is a module that makes it easier\nto quickly construct a chart:\nimport plotly.express as px\nThe visualization we have in mind will be a scatterplot that has the tournament names\n(ordered by the month and day held) along the y axis and dates along the x axis. The\npoints on the chart will represent a match,  and we\u2019ll color them differently, depend-\ning on the winner.\n The first thing we need to do is create a new DataFrame that has the tournaments\nordered by the month and day held. We\u2019ll sort the data using the strftime  function\nin DuckDB:\nsorted_matches_for_players = atp_duck.sql(\"\"\"\nFROM matches_for_players\nORDER BY strftime(tourney_date, '%m-%d')\n\"\"\").fetchdf()\nWe can then create a scatterplot based on sorted_matches_for_players , with the size\nof the points based on the importance of  the round in which the players played:\nfig = px.scatter(sorted_matches_for_players,\nx=\"tourney_date\",\ny=\"tourney_name\",\ncolor=\"winner_name\",size=\"roundOrder\",\ncolor_discrete_sequence=px.colors.qualitative.Plotly,\ncategory_orders={\n\"tourney_name\": (\nsorted_matches_for_players['tourney_name']\n.drop_duplicates().tolist()\n)\n},\n)\nWe can then render the chart using Stream lit\u2019s built-in support for rendering plot.ly\ncharts:\nst.plotly_chart(fig, use_container_width=True)\nNOTE Streamlit also has functions for rend ering charts created by other data\nvisualization tools, such as Al tair, Bokeh, PyDeck, and more.\nThe resulting chart is shown in figure 9.7.Orders the matches \nby month and day\nThe color tones used for the points. The\ndefault is two light blues, which we find\ndifficult to distinguish in print. See\nhttps://plotly.com/python/discrete-\ncolor for more options.\nEnsures plot.ly \ndoesn\u2019t reorder the \ntournament names203 9.1 Building a custom data app with Streamlit\nFigure 9.7 The timeline of matches between Murray and Nadal, visualized as a scatterplot by plot.ly\nThis looks pretty cool, but we need a grid that makes it easi er to see the years in which\nthe matches took place. We can do this  by computing the minimum and maximum\nyears and then drawing a vertical line for each year:\nmin_year = sorted_matches_for_players['tourney_date']\n.dt.year.min()\nmax_year = sorted_matches_for_players['tourney_date']\n.dt.year.max()\nunique_years = list(range(min_year, max_year+2))\nfor year in unique_years:\nfig.add_shape(\ntype=\"line\",x0=f\"{year}-01-01\", x1=f\"{year}-01-01\",y0=0, y1=1,yref=\"paper\",layer=\"below\",line=dict(color=\"#efefef\", width=2)\n)\nComputes the minimum year from\nthe matches between the players\nComputes the maximum \nyear from the matches \nbetween the players\nConstructs a list that \ncontains all the years from \nthe minimum ye ar until one \nyear after the maximum yearIterates over \neach of the years Adds a\nvertical\nline for\neach year204 CHAPTER  9Building and deploying data apps\nWe can then return to the Streamlit app to see the new and improved version of the\nchart in figure 9.8.\nFigure 9.8 Murray vs. Nadal timeline with vertical lines\nThat makes it much easier to see when they\u2019v e played each other over the last 15 years\nor so. Most of their matches took place in 2008\u20132009 and 2010\u20132011, with the rest\nscattered out. This was difficult to see wh en we looked at the table of results. We\nalready knew that Nadal dominated the rivalr y, but that fact is emphasized even fur-\nther by this visualization.\n Whether we want to represent a DataFram e as a table or a chart does not change\nthe way that we retrieve said DataFrame from  DuckDB. It\u2019s a matter of choice to either\npass it directly to Streamlit for rendering a tabular representation or to plot.ly first,\ncreating a chart which then is passed to Stre amlit. This is similar to the experience we\nsaw in chapter 6, where we switched from pandas to Apache Arrow to Polars Data-Frame without changing the way we inte racted with the database technology. \n9.1.5 Deploying our app on the Community Cloud\nDeploying an application can be as simple as  copying static files onto a web server or\nas complicated as setting up co ntainers, a service, and, in  general, a lot of movable\n205 9.1 Building a custom data app with Streamlit\nparts. Sometimes, there are good reasons for any of the extremes, but often, there\u2019s\nmost likely a middle ground. Maybe someon e else already set up the whole compli-\ncated infrastructure and provided a \u201cpush-to-run\u201d scenario for your application. The\nStreamlit Community Cloud, which is backed by Snowflake Inc., provides exactly that. \n If you agree to their terms, you can dire ctly push your new application to produc-\ntion, right from within the local Streamlit server, by clicking on the Deploy button in\nthe top-right-hand corner of the UI. You\u2019ll  then see the modal window shown in figure\n9.9.\n You might need to add requirements.txt with the four dependencies to your repos-\nitory if it doesn\u2019t exist yet.\nstreamlit\nduckdb\nstreamlit-searchbox\nplotly\nThe tooling gives you a choice between th e Community Cloud and a custom deploy-\nment (figure 9.9).\nFigure 9.9 Choosing how to deploy your new application to production\nYou\u2019ll need to have your pr oject connected to a remote GitHub repository; otherwise,\nyou\u2019ll see the error in figure 9.10.Listing 9.5 requirements.txt\n206 CHAPTER  9Building and deploying data apps\nOnce we\u2019ve got our app connected to a GitH ub repository, if we click through the\ndeploy flow again, we\u2019ll see th e screen shown in figure 9.11.\nFigure 9.11 Deploying our app\nFigure 9.10 The error \nmessage received when trying to deploy an app not \nconnected to a remote \nGitHub repository\n207 9.2 Building a BI dashboard with Apache Superset\nWe can then choose a URL for the app and adjust the branch or main file path if\nthose need changing. We can also change the version of Python that will be used via\nthe Advanced Settings.\n As you can see from the screenshot, the completed version of this app lives in the\nmneedham/atp-head-to-head repository ( https:/ /github.com /mneedham/atp-head\n-to-head ), and it\u2019s been deployed to atp-head-to-head.streamlit.app ( https:/ /atp\n-head-to-head.streamlit.app/ ). Without diving into the world of frontend frameworks,\nwe have been able to build an interactive web application by usin g a handful of Stream-\nlit components\u2014all in one language we  already know (Pyt hon)\u2014connecting to\nDuckDB in a way we already understand and by passing data as DataFrames, a familiar\ndata format.\n But admittedly, we did still have to write some code to get all this working! In case\nyou\u2019re not that comfortable with writing code, or you just want to get something upand running quickly for yourself  or your users, there are ot her options. If, rather than\nan interactive application, you want to crea te a dashboard, you can achieve this with a\nlow-code or BI tool. We\u2019ll still get interact ivity in terms of zooming, panning, and oth-\nerwise changing the representation of char ts, but we won\u2019t have to write any applica-\ntion code. This is where a tool such as Apache Superset comes in. \n9.2 Building a BI dashboard with Apache Superset\nWhile Streamlit gives us a lot of control over our application without requiring special-ist frontend knowledge, some times we don\u2019t want to spen d so much time writing cus-\ntom code for a frontend. DuckDB integrates with a variety of BI tools, including Hex,\nTableau, and the one that we\u2019ll be usin g in this chapter: Apache Superset. \n9.2.1 What is Apache Superset?\nApache Superset is an open source data exploration and visualiz ation platform devel-\noped by Maxime Beauchemin, who is best known for creating Apache Airflow. Super-set has integrations with a large number  of databases (including DuckDB!), and\neverything can be configured through its UI . It has preinstalled visualization types,\nwhich should cover most use cases, but you can also create custom visualizations in\nJavaScript. \n Superset has support for SQ LAlchemy, a Python library th at provides a high-level,\nobject-oriented interface for interacting wi th databases. It manages database connec-\ntions, defines database schemas, and performs queries against these databases.\n While there are many BI tools, many of them are cloud services that require you to\nregister for an account. Superset, on the other hand, can be tried out on our\nmachine, and if we later decide to deploy to production, we have  the option of using\nthe Preset ( https:/ /preset.io/ ) hosted service.\n There are a variety of ways to install Superset ( https:/ /mng.bz/1G2y ), including\nDocker Compose scripts and a Helm reposi tory for deploying to Kubernetes. These\nwould be good options if we were deployin g Superset to production, but to install208 CHAPTER  9Building and deploying data apps\nSuperset on our machine, we\u2019re going to foll ow the instructions found on the \u201cInstall-\ning from PyPI\u201d docu mentation page ( https:/ /mng.bz/PZEg ).\n We start by installing the following library. All the following commands are exe-\ncuted from your terminal:\npip install apache-superset==3.1.2\nNOTE At the time of writing, Superset  only works with Python 3.10.\nN o w  w e  n e e d  a  w a y  t o  c o n n e c t  S u p e r s e t  t o  D u c k D B .  duckdb_engine  (https:/ /pypi\n.org/project/duckdb-engine/ ) is DuckDB\u2019s SQLAlchemy driver, and we\u2019re going to\nuse it to get these two tools to play nicely together.\n We can install the driver by running the following command:\npip install duckdb-engine\nNext, we need to configure some enviro nment variables, without which Superset\nwon\u2019t run. You need to set these variable s whenever you have a new terminal session\nbefore running any of the following command s. Superset contains  a Flask application,\nso we need to set the FLASK_APP  variable to define the name of the file that Flask\nshould look for. We also n eed to specify a secret key ( https:/ /mng.bz/JZ0P ), with a\nrandom string of characters, fo r a more secure installation:\nexport SUPERSET_SECRET_KEY=\"sYBpNA2+bQHvmXcojOVp53b8xbmN3ZQ\"\nexport FLASK_APP=superset\nNext, we\u2019re going to initialize  Superset\u2019s database. This stores all the metadata that\nSuperset uses, and it won\u2019t work without it:\nsuperset init\nsuperset db upgrade\nThe next step is to create an admin us er. We\u2019re going to create a user called admin\nwith the same password, but you should use more secure credenti als when you do this\nyourself and your machine is exposed to the internet. Flask app builder (FAB) is theframework Superset is built on top of. It provides authentication, user management,\npermissions, and roles:\nsuperset fab create-admin \\\n--username admin \\\n--firstname Superset \\\n--lastname Admin \\--email admin@example.com \\\n--password admin\nWe should see the following output:\nlogging was configured successfully...\nRecognized Database Authentications.Admin User admin created.209 9.2 Building a BI dashboard with Apache Superset\nNow we can launch the Superset web server on port 8088  by running the following\ncommand:\nsuperset run -p 8088 \\\n--with-threads \\\n--reload \\\n--debugger\nWe can then navigate to the Superset UI at http://localhost:8088 and log in with the\nadmin/admin  username and password we just created. \n9.2.2 Superset\u2019s workflow\nBefore we create anything, it\u2019s probably he lpful to understand the terms Superset uses\nand how they relate to each other. The following are important concepts to under-\nstand:\n\uf0a1Database \u2014The underlying data sour ce (DuckDB, in our case)\n\uf0a1SQL Saved Query \u2014A custom SQL query against one or more tables in the data-\nbase\n\uf0a1Dataset \u2014A wrapper around an SQL-saved query or a database table\n\uf0a1Chart \u2014A visualization based on a dataset\n\uf0a1Dashboard \u2014A collection of charts\nThe tables that comprise a database aren\u2019t  one of Superset\u2019s concepts, but they are\nused by datasets.\n So when we\u2019re using Superset, we\u2019ll need to  first create a database. It will then auto-\nmatically detect the schema, and therefore ta bles, that comprise that database. We can\nthen create datasets based on those tables  or write SQL queries and turn those into\ndatasets. And finally, we create charts on top of those datasets, which are used in dash-\nboards. Figure 9.12 contains a visual representation of the Superset workflow. \nFigure 9.12 Terms used by Superset and their relationships\n210 CHAPTER  9Building and deploying data apps\n9.2.3 Creating our first dashboard\nIn this section, we\u2019re going to learn how to create a dash-\nboard. Dashboards are used to help visualize importantbusiness metrics so that we ca n quickly get a picture of the\nstate of things. We\u2019ll also want to be able to interact with\nthe dashboard so that we can dig deeper into the data ifthere are any problems. \nNOTE Any dashboard we create over a tennis dataset\nwon\u2019t be as important as one over a business dataset, but\nhopefully, you\u2019ll see how to apply these techniques toyour own data.\nLet\u2019s create our first dashboard, starting by connecting\nSuperset to DuckDB. Click on  the Settings button in the\ntop right, as shown in figure 9.13.\n Once you\u2019ve done that, click on the \n+ Database  button, which will result in the\nmodal window shown in figure 9.14. \nFigure 9.13 Database \nconnections\nFigure 9.14 Connecting to a \nnew database211 9.2 Building a BI dashboard with Apache Superset\nClick on DuckDB, and then in the SQL Alchemy  URI field, enter the connection string,\nwhich has the format of duckdb:///<your-database> . For us, that will be duckdb:///\natp.duckdb , as shown in figure 9.15. \nIf your Streamlit applic ation from the previous\nsection is still running, you need to close it firstto release the lock on the database file. Click\non the Test Connection button to confirm that\neverything is wired up correctly.\nNOTE You can also connect to a Mother-\nDuck database using duckdb:///md:<my_\ndatabase>?motherduck_token=<my_token> .\nOnce you\u2019ve done that, click on the Data link\nat the top of the screen and then on the CreateDataset button. We\u2019ll then see the screen in fig-\nure 9.16, where we can select the database,\nschema, and table that we want to use.\nFigure 9.16 Choosing a table\nto make up a dataset\nFigure 9.15\nConnecting to the ATP DuckDB database\n212 CHAPTER  9Building and deploying data apps\nWe\u2019re going to select the matches  table. Once we select that, we\u2019ll see a UI that shows\nall fields in that table, as shown in figure 9.17. \nFigure 9.17 A preview of the matches  table\n213 9.2 Building a BI dashboard with Apache Superset\nIf we\u2019re happy with our selection, we can click on Create Dataset and Create Chart,\nafter which we\u2019ll be asked which chart we\u2019d lik e to create from the screen shown in fig-\nure 9.18.\nFigure 9.18 Choosing a chart for a specific dataset\nLet\u2019s select Bar Chart; we\u2019ll then create a chart that shows the number of matches\nplayed per year from 1967 to 2023. It\u2019s often said that there are a lot more tennis\nmatches played nowadays than there used to be, so it\u2019ll be interesting to see what the\ndata shows us.\n To create this chart, we\u2019ll need to configure the following sections:\n\uf0a1X-Axis is tourney_date .\n\uf0a1Time Grain is Year .\n\uf0a1Metrics is COUNT(*) .\n214 CHAPTER  9Building and deploying data apps\nYou can see a screenshot of how to do this in figure 9.19.\nIf we create the chart, we\u2019ll see a preview on the right-hand side . It should look like\nfigure 9.20.\nFigure 9.20 A preview of the bar chart showing matches played per year\nFigure 9.19 Configuring a bar chart \nshowing matches played per year\n215 9.2 Building a BI dashboard with Apache Superset\nThere doesn\u2019t seem to be any evidence that more matches are being played nowadays.\nIf anything, there seems to be a slight downward trend in the number of matches\nbeing played. There\u2019s also an outlier in 2020, caused by the COVID-19 pandemic,\nwhich resulted in the tennis season be ing paused from March to August 2020.\n If we\u2019re happy with the chart, we can save  it via the screen shown in figure 9.21.\nWe\u2019ll be asked if we want to add the chart to  an existing or new dashboard. Let\u2019s add it\nto a new dashboard called ATP Dashboard.\nFigure 9.21 Saving the chart\nNOTE If we don\u2019t type anything into the dashboard field, the chart won\u2019t\nbe assigned to a dashboard, but we  can always add it to a dashboard\nafterward.\nIf we click Save & Go to Dashboard, we should now see a dashboard containing our\nchart, as shown in figure 9.22. Success! We\u2019ve created our first dashboard.\n We can repeat the process described in this section if we want to add more charts\nbased on individual tables, but if we want to  create a chart based on data from multi-\nple tables, we\u2019ll need to do something slightly different. \n  \n216 CHAPTER  9Building and deploying data apps\nFigure 9.22 A dashboard containing a chart of the total matches played\n9.2.4 Creating a dataset from an SQL query\nIn addition to creating datasets from tabl es, we can also create them from SQL que-\nries. To do this, when we add a new datase t, we\u2019ll need to click on the SQL Lab link\nfrom the top menu. \n Next, we need to add a new query tab, select the database and schema, and then\nenter a query. For example, let\u2019s say we wa nt to work out whether grand slam winners\nhave been getting older over time. We need to get data from both the matches  and\nplayers  tables to answer this question. The following query computes the cumulative\ncount of grand slam winne rs aged 30 and over:\nSELECT\nwinner_name,\ntourney_date,\n(tourney_date - dob)/365 AS age,COUNT(*) OVER (ORDER BY tourney_date) AS cumulative_count\nFROM matches\nJOIN players ON players.player_id = matches.winner_idWHERE round='F'\nAND tourney_level = 'G'\nComputes the age \nof the winnerComputes the\ncumulative count\nof winners\nFilters to only return the Final \nmatch in each tournament\nFilters to only include \nGrand Slam events217 9.2 Building a BI dashboard with Apache Superset\nAND age >= 30\nORDER BY tourney_date;\nLet\u2019s paste that query, and then click on  the Run button. We should see the output\nshown in figure 9.23. Once we\u2019re happy with our query, we can select the Save Datasetoption and click on the Save butt on to give the dataset a name.\nFigure 9.23 Grand slam winners aged 30 and overFilters to only return \nwinners aged 30 or older\n218 CHAPTER  9Building and deploying data apps\nLet\u2019s now return to the Charts page; this time, we\u2019ll choose Big Number with Trend-\nline as our chart. We\u2019ll also select ou r new dataset, as shown in figure 9.24.\nFigure 9.24 Selecting a big number with trendline chart\nTo create this chart, we\u2019ll need to configure the following sections:\n\uf0a1X-Axis is tourney_date .\n\uf0a1Time Grain is Day.\n\uf0a1Metrics is SUM(cumulative_count) , which is a custom metric.\nIf we then click Update Chart, we should see the visualization shown in 9.25 on the\nright-hand side.\nFigure 9.25 The number of grand slam winners aged 30 and over\n219 9.2 Building a BI dashboard with Apache Superset\nLet\u2019s save the chart and add it to the ATP Dashboard we created earlier. If we navigate\nto the dashboard, we should see so mething that looks like figure 9.26.\nFigure 9.26 A dashboard with the over 30 Grand Slam winners included\nHopefully, this has given you a good idea of how to construct a dashboard using Sup-\nerset. We\u2019ve added a few more charts to the dashboard following the approachdescribed in this section and the previous one. You can see the final result in figure 9.27.\nFigure 9.27 The ATP Dashboard\n220 CHAPTER  9Building and deploying data apps\nWe\u2019ve used a few different chart types to give  you an idea of what you can do with Sup-\nerset. In the next section, we\u2019ll explain ho w you can import a copy of this dashboard\nso you can explore the various elements. \n9.2.5 Exporting and importing dashboards\nWe generally configure databases, datasets, charts, and dashboards, using Superset\u2019s\nUI. But if we want to deploy those dashboar ds elsewhere (or, say, share them with the\nreaders of a book), we probably don\u2019t want  to have to go through the whole process\nfrom scratch. This is where Superset\u2019s export and import dashboard features come\ninto play. \n We can export dashboards from the Dashboard page. If we hover over the Actions\ncolumn of the Dashboards row, we\u2019ll see an export button, as shown in figure 9.28.\nFigure 9.28 Exporting a dashboard\nIf we click on this button, Superset will ge nerate a ZIP file that contains our databases,\ndatasets, charts, and dashboards. We\u2019ve included the ZIP file in the book\u2019s GitHub\nrepository ( https:/ /github.com/duckdb-in-action/examples ) ,  a n d  i f  w e  n a v i g a t e  t o\nch09, we can unzip that file:\nunzip dashboard_export_20231203T162310.zip\nIn a Linux-based terminal, we can use the tree  command ( tree /f /a  on Windows\nor find dashboard_export_20231203T162310  on MacOS) to list all the included files:\ntree dashboard_export_20231203T162310\ndashboard_export_20231203T162310\n\u251c\u2500\u2500 charts\n\u2502 \u251c\u2500\u2500 Grand_Slam_Winners__30_11.yaml\n\u2502 \u251c\u2500\u2500 Grand_Slam_winners__25_9.yaml\u2502 \u251c\u2500\u2500 Lowest_Ranked_Grand_Slam_Winners_8.yaml\n\u2502 \u251c\u2500\u2500 Matches_Played_7.yaml\n\u2502 \u2514\u2500\u2500 Multi_Title_Winners_6.yaml\u251c\u2500\u2500 dashboards\n221 Summary\n\u2502 \u2514\u2500\u2500 ATP_Dashboard_1.yaml\n\u251c\u2500\u2500 databases\n\u2502 \u2514\u2500\u2500 DuckDB.yaml\u251c\u2500\u2500 datasets\n\u2502 \u2514\u2500\u2500 DuckDB\n\u2502 \u251c\u2500\u2500 Grand_Slam_Winners__30.yaml\u2502 \u251c\u2500\u2500 Multi_Title_Winners.yaml\n\u2502 \u251c\u2500\u2500 Young_Grand_Slam_Winners.yaml\n\u2502 \u2514\u2500\u2500 matches.yaml\u2514\u2500\u2500 metadata.yaml\nThere\u2019s a directory for each of the concepts we learned about at the start of this\nsection:\n\uf0a1The config to connect to the database is in databases/DuckDB.yaml.\n\uf0a1Datasets are defined in datasets/DuckDB.\n\uf0a1Charts are defined in charts.\n\uf0a1The config for the dashboard is in dashboards/ATP_Dashboard_1.yaml.\nYou can import this dashboard from the dashboard page by clicking on the down\narrow button in the top-right corner of th e Dashboard page, as shown in figure 9.29.\nFigure 9.29 Importing a dashboard\nIf we then select the ZIP file, it will import  the setup into Superset, and we should see\nthe dashboard from figure 9.27. \nSummary\n\uf0a1Streamlit is a low-code environment that  provides various ready-made and reus-\nable components solving recurrent tasks when writing web applications.\n\uf0a1Streamlit is written in Python and thus integrates in various ways with DuckDB\u2019s\nPython API via the DB-API 2.0, th e relational API, or DataFrames.\n\uf0a1In contrast to a no-code and purely de clarative environment, you can write cus-\ntom Python in your Streamlit applic ation to enhance its capabilities.\n222 CHAPTER  9Building and deploying data apps\n\uf0a1plot.ly provides a similar low-code approach for creating visually appealing,\ninteractive visualizations that can be used seamlessl y with Streamlit.\n\uf0a1Apache Superset is at the other end of the spectrum: it is basically a no-code,\ndrag-and-drop alternative for buildin g a visually appealing dashboard.\n\uf0a1The only code you usually need to writ e using Apache Superset are custom SQL\nqueries that feed into  the visualizations. 223Performance considerations\nfor large datasets\nSo far in this book, we\u2019ve seen how to us e DuckDB with a variety of datasets, but\nmost of them have been small or medium in size. This isn\u2019t unusual, as those data-\nsets are representative of many of th ose we\u2019ll come across in our daily work.\n However, huge datasets do exist, and we wouldn\u2019t want you to think that you\nneed to use another data processing tool when you encounter these! In this chap-\nter, we\u2019re going to look at two datasets: th e first contains data about Stack Overflow,This chapter covers \n\uf0a1Preparing large volumes of data to be imported \ninto DuckDB\n\uf0a1Querying metadata and r unning exploratory data \nanalysis (EDA) queries on large datasets\n\uf0a1Exporting full databases concurrently to Parquet\n\uf0a1Using aggregations on multiple columns to speed up statistical analysis\n\uf0a1Using EXPLAIN  and EXPLAIN ANALYZE  to \nunderstand query plans224 CHAPTER  10 Performance considerations for large datasets\nthe popular coding question-and-answer webs ite, and the second contains data about\ntaxi trips in New York City. With these tw o datasets, we can teach you tips and tricks\nwhen working with bigger datasets in DuckDB.\n For each one, we\u2019ll show how to prepar e and then import it into DuckDB. Once\nwe\u2019ve done that, we\u2019ll run so me queries on the data before exporting the database\ninto a portable format.\n10.1 Loading and querying the full Stack Overflow database\nStack Overflow  is an online, community-driven question-and-answer website designed\nfor developers and programmers to ask and an swer technical questions. It was created\nin 2008 and uses a reputation system, wher e users earn points and privileges by con-\ntributing useful answers and content.\n If you\u2019re like us, you\u2019ve pr obably spent a lot of time on Stack Overflow looking for\nthe answers to technical questions. And if you\u2019ve been a good citizen, perhaps you\u2019ve\nanswered some questions as well! But have you ever stopped to consider the system\nand data behind this useful site?\n If not, it\u2019s time to change that by an alyzing a dump of Stack Overflow data with\nDuckDB. The dataset size is 11 GB in compressed CSV format and contains 58 million\nposts, 20 million users, and 65 thousand  tags. It\u2019s not quite \u201cBig Data\u201d (see https:/ /\nmotherduck.com/blog/big-data-is-dead/ ), but it\u2019s big enough to put DuckDB\nthrough its paces.\n In this section, we will explore the Stack Overflow dataset, using DuckDB both\nlocally and on MotherDuck. First, we\u2019re going to download and transform the raw\ndata, and then we\u2019ll load it into DuckDB and inspect it with some EDA queries before\nexporting it to Parquet.\n10.1.1 Data dump and extraction\nIf you want to do some basic exploratory an alysis of the Stack Overflow data, the site\nprovides the Stack Exch ange Data Explorer ( https:/ /mng.bz/wx6W ), which is a web-\nsite for executing SQL queries on the Stack Ov erflow data. It is gr eat for getting a feel\nfor the dataset, but it is limited in the nu mber and complexity of queries you can run\nso that usage doesn\u2019t overload the service. \n We want to have more control over th e queries that we run though, so we want\naccess to the raw data. In this section, we\u2019r e going to show how to download and trans-\nform the raw data, but we know this isn\u2019t the most fun part of the process, so don\u2019t feellike you need to follow al l the steps in the section.\nNOTE If you just want to access the final tabular data, you can either down-\nload the Parquet files from S3 (s3:/ / us-prd-motherduck-open-datasets/stack-\noverflow/parquet/2023-05/) or mount the MotherDuck share (md:_share/\nstackoverflow/6c318917-6888-425a-bea1-5860c29947e5) to focus on queryingthe data. Alternatively, you can pick one of the smaller stack exchange com-\nmunities, like math or biotechnology, if the Stack Overflow data is too big.225 10.1 Loading and querying the full Stack Overflow database\nFor the bravehearted among you, let\u2019s get this dataset ready to load into DuckDB.\nStack Exchange publishes all their data publicly on the Internet Archive Stack\nExchange dump ( https:/ /archive.org/download/stackexchange ) under a Creative\nCommons license. We\u2019re going to use the largest set of files for the Stack Overflow siteitself. We can use the \ncurl  command-line utility for that task as shown, which stores\nthe files under the same name that they have on the server:\ncurl -OL \"https://archive.org/download/stackexchange/stackoverflow.com-\\\n{Comments,Posts,Votes,Users,Badges,PostLinks,Tags}.7z\"\nThe Internet Archive\u2019s bandwidth is limited,  so downloading the data can be a frus-\ntrating and slow process, with frequent abor ts of the connection likely. We will end up\nwith seven compressed XML files with a total size of 27 GB:\n19G stackoverflow.com-Posts.7z\n5.2G stackoverflow.com-Comments.7z\n1.3G stackoverflow.com-Votes.7z684M stackoverflow.com-Users.7z\n343M stackoverflow.com-Badges.7z\n117M stackoverflow.com-PostLinks.7z903K stackoverflow.com-Tags.7z\nAfter the download finishes, you will need to extract the files using 7-Zip ( https:/ /7\n-zip.org/ ) or p7zip ( https:/ /p7zip.sourceforge.net/ ), as shown.\n The files are in the SQL Server export format, where each Row element has all col-\numns as attributes. Here is an  example of the file contents:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<users>\n...\n<row Id=\"728812\" Reputation=\"41063\" CreationDate=\"2011-04-28T07:51:27.387\"\n\u27a5DisplayName=\"Michael Hunger\" LastAccessDate=\"2023-03-01T14:44:32.237\"\n\u27a5WebsiteUrl=\"http://www.jexp.de\" Location=\"Dresden, Germany\" AboutMe=\n\u27a5\"&lt;p&gt;&lt;a href=&quot;http://twitter.com/mesirii&quot; rel=&quot;\n\u27a5nofollow&quot;&gt;Michael Hunger&lt;/a&gt; has been passionate about\n\u27a5so?ware development for a long time. If you want him to speak at your\n\u27a5user group or conference, just drop him an email at michael at jexp.de\"\n\u27a5Views=\"7046\" UpVotes=\"4712\" DownVotes=\"24\" AccountId=\"376992\" />\n...\nUnfortunately, DuckDB doesn\u2019t support parsin g XML yet, so we will have to use some\nexternal tools to get this data into a form at that DuckDB supports. While the process\nis time-consuming, it is a reliable way of converting the XML to CSV.\n We are using xidel  (https:/ /www.videlibri.de/xidel.html ), an XML processing\ncommand-line tool that outputs JSON. We\u2019ll then convert the JSON output to CSV,\nusing the jq (https:/ /jqlang.github.io/jq/ ) command-line JSON processor. You can\nfind download and installation instructions for xidel  and jq on their respective web-\nsites. Finally, we\u2019ll co mpress the CSV file with gzip  to save space. \n Let\u2019s have a look at how to do this for the comments. First, we extract the contents\nof the zip file before piping the output to xidel , as you can see in listing 10.1.226 CHAPTER  10 Performance considerations for large datasets\n The process is as follows:\n1Extract relevant fields from the XML, for instance, the post ID, score, text, cre-\nation date, and user ID.\n2Convert the XML to JSON.\n3Convert the JSON to CSV.\n4Output the header.\n5Compress the CSV to a file called Comments.csv.gz.\nWe could have also used the built-in functi onality of DuckDB to load the JSON files\nfrom xidel  directly, but it can be useful to ha ve the CSV around for other tools that\ndon\u2019t support it. Additionally, the files ar e a bit smaller without the repeated key\nnames.\n7z e -so stackoverflow.com-Comments.7z | \\\nxidel -se '//row/[(@Id|@PostId|@Score|@Text|@CreationDate|@UserId)] '-|\\\n(echo \"Id,PostId,Score,Text,CreationDate,UserId\" &&jq -r '. | @csv') |\ngzip -9 > Comments.csv.gz\nEach file will be processed similarly, but for brevity\u2019s sake, we won\u2019t include all the\ncommands inline. If you want to see the code  so that you can try it yourself, you can\nfind it in the book\u2019s GitHub repository. \n Once we\u2019re done, we\u2019ll have the following list of CSV files, with a total size of 11\nGB:\n5.0G Comments.csv.gz\n3.2G Posts.csv.gz\n1.6G Votes.csv.gz\n613M Users.csv.gz452M Badges.csv.gz\n137M PostLinks.csv.gz\n1.1M Tags.csv.gz\n10.1.2 The data model\nBefore we start exploring, let\u2019s look at th e data model of the Stack Overflow dataset.\nTo remind ourselves of the UI, figure 10. 1 shows a screenshot of the Stack Overflow\nsite with most in formation visible. \n In the downloaded and converted files pr eviously listed, we have the following\nentities, which correspond also to the filenames:\n\uf0a1Questions  (Post  with postTypeId=1 ) with a title , body , creationDate ,\nownerUserId , parentId , acceptedAnswerId , answerCount , tags , upvotes ,\ndownvotes , views , and comments . The maximum of six Tags  define the topics of\nthe question.\n\uf0a1User  with displayName , aboutMe , reputation , last login date , and so on.Listing 10.1 Converting the XML file via JSON to CSV227 10.1 Loading and querying the full Stack Overflow database\nFigure 10.1 The Stack Overflow UI showing a user question and an accepted answer\n\uf0a1Answers  (Post  with postTypeId=2 ) with their own ownerUserId , upvotes , down-\nvotes , and comments . One of the answers can be accepted as the correct answer.\n\uf0a1Questions  and Answers  can have comments with their own text , ownerUserId ,\nand score .\n\uf0a1Badges  with class  columns that users can earn for their contributions.\n\uf0a1Posts  can be linked to other Posts  (e.g., duplicates or related questions as\nPostLinks ).\nThe files don\u2019t have any information about in dexes or foreign keys ; we need to recre-\nate those references manually. For that purp ose, we drew the data mode shown in fig-\nure 10.2, which is a simplified version of the Stack Overflow data model with the most\nimportant columns listed above as attr ibutes and foreign keys as arrows.\n228 CHAPTER  10 Performance considerations for large datasets\n \nFigure 10.2 Stack Overflow data model\nYou can see how the Post  (Question  or Answer ) connects to the User  who has written\nit via the ownerId . Comment , Vote , and Answer  refer to the original Post  via postId ,\nand the accepted Answer \u2013Post  is linked from the Question \u2013Post  via accepted-\nAnswerId . The Badge  connects to the User  via userId , and the PostLink  connects two\nPost  entities via postId  and relatedPostId . \n10.1.3 Exploring the CSV file data\nNow that we\u2019ve got the data prepared, we\u2019re back in familiar territory when it comes\nto importing the data into DuckDB. As we  covered in earlier chapters, DuckDB has\nthe read_csv  function, which we can use to load data directly from the compressed\ngzipped CSV files. read_csv  will automatically try to infer column types, which we\u2019ve\nfound works well for the Stack Overflow dataset. \n Let\u2019s look at the Tags  file first and query it for st ructure and content. We\u2019re going\nto start with the following query, wh ich counts the number of tags with\nSELECT count(*) FROM read_csv('Tags.csv.gz')UsersPosts\nQuestion\nPosts\nAnswerCommentsTags\nVotes\n  voteTypeId: int\ncreationDate: datePosts\nAnswer\nUsersPosts\nBadges\n   class: int\n    date: date\ntagBased: boolpostId\npostId\nla\nstE\nditor\nUserId\nparentIdownerIdtags\nparentIdpostId\nuserId\nuserIdwikiPostIdexcerptPostId\naboutMe: string \ndisplayName: string\nreputation: int \ndownVotes: int \nupVotes: int\ncreationDate: date \nlastAccessDate: date\nviews: inttitle: string \nbody: string\nfavoriteCount: int \nupVotes: int \ndownVotes: int\ncreationDate: date\ncommentCount: int\nlastEditDate: date\npostTypeId: 1\ntags: string\nviewCount: int \nanswerCount: intbody: string\ncreationDate: date\nupVotes: int \ndownVotes: int \ncommentCount: int\npostTypeId: 2tagName: string\ncount: int\ntext: string\ncreationDate: date\nscore: int229 10.1 Loading and querying the full Stack Overflow database\nWe\u2019ve got just under 65,000 tags, which seems like a lot, but there are many different\ntechnologies people can get stuck on, so it makes sense! Next, let\u2019s have a look at the\nstructure of the data in the Tags  file, using the DESCRIBE  function, which gives us the\nname and type of each column. \nDESCRIBE(FROM read_csv('Tags.csv.gz'));\nThe DESCRIBE  command returns the metadata of the tags files, and we can see the\nnames and types of the available columns:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502\u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Id \u2502 BIGINT \u2502\u2502 TagName \u2502 VARCHAR \u2502\u2502 Count \u2502 BIGINT \u2502\n\u2502 ExcerptPostId \u2502 BIGINT \u2502\n\u2502 WikiPostId \u2502 BIGINT \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe TagName  and Count  fields are the ones we need to determine the most popular\ntags. Let\u2019s find the top five most popular tags  with a query that just runs perfectly fast,\neven on the compressed, large file. \nSELECT TagName, Count\nFROM read_csv(\n'Tags.csv.gz',column_names=['Id', 'TagName', 'Count'])\nORDER BY Count DESC\nLIMIT 5;\nWe can see that these are the usual suspects  when it comes to programming languages\n(i.e., the most popular ones):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TagName \u2502 Count \u2502\n\u2502 varchar \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 javascript \u2502 2479947 \u2502\n\u2502 python \u2502 2113196 \u2502\n\u2502 java \u2502 1889767 \u2502\u2502 c# \u2502 1583879 \u2502\n\u2502 php \u2502 1456271 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Listing 10.2 Describing the metadata of the Tags  file\nListing 10.3 Selecting the top Tags  from the CSV file230 CHAPTER  10 Performance considerations for large datasets\nTo see how tags distribute across their usage,  we can bucket the counts into powers of\n10 and then count the number of tags in ea ch bucket, as shown in the following listing.\nSELECT cast(pow(10,floor(log(Count)/log(10))) AS INT) AS bucket,\ncount(*)\nFROM read_csv(\n'Tags.csv.gz',column_names=['Id', 'TagName', 'Count'])\nWHERE Count > 0GROUP BY bucketORDER BY bucket ASC;\nTo create our buckets, using the count of 112 as an example, we do the following:\n1We compute the logarithm of each count to the base of 10 ( log(112)/log(10)\n= 2.049 ).\n2This gives us the order of magnitude of the count.\n3We use this to get the base integer value and floor(2.049) = 2.0 .\n4We use it again to recompute the power of 10 pow(10,2.0) = 100.0 .\n5We get, as an end result, the original value in powers of 10.\n6We cast these to an integer for the grouping cast(100.0 as int) = 100 .\nWe can see the result follows a power law di stribution (i.e., the number of tags with\nfew uses is high, and fewer tags have a high  count), tapering off to only 25 tags with\nmore than 1 million uses. The only exception is rare tags with one mention; there arefewer of them, as expected. \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bucket \u2502 count_star() \u2502\n\u2502 int32 \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 1 \u2502 6238 \u2502\n\u2502 10 \u2502 23018 \u2502\n\u2502 100 \u2502 23842 \u2502\u2502 1000 \u2502 9126 \u2502\n\u2502 10000 \u2502 1963 \u2502\n\u2502 100000 \u2502 252 \u2502\u2502 1000000 \u2502 25 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n10.1.4 Loading the data into DuckDB\nTo get the Stack Overflow data into DuckDB, we can either create a table first and\nthen ingest the data, or we can create the ta ble on the fly as we read the data. The for-\nmer approach is more explicit and allows us to define the column names and types,\nbut it requires us to know and spell out the schema of the data beforehand. Then, theListing 10.4 Query for bucketing Tag frequency\nComputes buckets of Tag\ncount in powers of 10Counts the entries\n in each bucket\nReads the Tags.csv.gz \nfile with three columns Filters columns \nwith zero countGroups\nby our\n bucket\nOrders by bucket scale \nascending (smallest first)231 10.1 Loading and querying the full Stack Overflow database\nCREATE TABLE  statements also need to be adjust ed when the file structure or column\ntypes change; otherwise, th e load will fail. We use CREATE  OR REPLACE  TABLE  so that we\ncan run the imports multiple times for testing without having to drop the table in\nbetween. \n The latter approach is used in listing 10.5. We choose the relevant column names,\nwith the data types being inferred while reading the CSV files, and we get \u201cwhat\u2019s\nthere.\u201d\n Let\u2019s have a look at the import statements for Users  and Posts . You can find the\nimport statements for the other tables in the book\u2019s GitHub repository.\nCREATE OR REPLACE TABLE users AS\nSELECT *\nFROM read_csv(\n'Users.csv.gz',auto_detect=true,column_names=[\n'Id', 'Reputation', 'CreationDate', 'DisplayName',\n'LastAccessDate', 'AboutMe', 'Views', 'UpVotes', 'DownVotes'\n]\n);\nNow, in the following listing, we ca n check the number of rows in the users  table with\nSELECT count(*) FROM users;\nWe will see that we have roughly 20 million users.\nCREATE OR REPLACE TABLE posts AS\nFROM read_csv(\n'Posts.csv.gz',auto_detect=true,\ncolumn_names=[\n'Id', 'PostTypeId', 'AcceptedAnswerId', 'ParentId', 'CreationDate','Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastEditorUserId',\n'LastEditorDisplayName', 'LastEditDate', 'LastActivityDate', 'Title',\n'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount','CommunityOwnedDate', 'ContentLicense'\n]\n);\nNOTE The Tags  column is a text column that contains up to six Stack Over-\nflow tags wrapped in angle brackets (e.g., \u2014 <sql><performance><duckdb> ).\nWe can see the structure of the generated table with\n`select column_name, column_type from (show table posts);`Listing 10.5 Creating or replacing the Stack Overflow users  table\nListing 10.6 Creating or replacing the Stack Overflow posts  table232 CHAPTER  10 Performance considerations for large datasets\nMeanwhile, we will skip irrelevant fields, like null , key, default , and extra :\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502\u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Id \u2502 BIGINT \u2502\u2502 PostTypeId \u2502 BIGINT \u2502\n\u2502 AcceptedAnswerId \u2502 BIGINT \u2502\n\u2502 CreationDate \u2502 TIMESTAMP \u2502\u2502 Score \u2502 BIGINT \u2502\n\u2502 ViewCount \u2502 BIGINT \u2502\n\u2502 Body \u2502 VARCHAR \u2502\u2502 OwnerUserId \u2502 BIGINT \u2502\n\u2502 LastEditorUserId \u2502 BIGINT \u2502\n\u2502 LastEditorDisplayName \u2502 VARCHAR \u2502\u2502 LastEditDate \u2502 TIMESTAMP \u2502\n\u2502 LastActivityDate \u2502 TIMESTAMP \u2502\n\u2502 Title \u2502 VARCHAR \u2502\u2502 Tags \u2502 VARCHAR \u2502\n\u2502 AnswerCount \u2502 BIGINT \u2502\n\u2502 CommentCount \u2502 BIGINT \u2502\u2502 FavoriteCount \u2502 BIGINT \u2502\u2502 CommunityOwnedDate \u2502 TIMESTAMP \u2502\n\u2502 ContentLicense \u2502 VARCHAR \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 19 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNow we have inserted 58 million posts, which we can check using\nSELECT count(*) FROM posts;\nFrom previous experience, we\u2019ve found that  a good approach fo r working with large\ndatasets is to first get an overview of the data values in our tables. In chapter 4, we\nlearned about the SUMMARIZE  clause. Running it on all columns of the users  and\nposts  tables will take a few seconds, and the output is huge, as the tables have a lot of\ncolumns, and SUMMARIZE  computes a lot of metrics. Let\u2019s have a look at some of the\ncolumns of the users  table and check for the approximate number of unique users,\ntheir creation date, and how often they inte ract with posts by upvoting and downvot-\ning them. You get these statistics without wr iting a complex query, as shown in the fol-\nlowing listing. \nSUMMARIZE (\nSELECT Id, Reputation, CreationDate, Views, UpVotes, DownVotes\nFROM users\n);\nHere are the results of summarizing th e most interesting user attributes:Listing 10.7 Summarizing a subset of columns233 10.1 Loading and querying the full Stack Overflow database\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 max \u2502 approx_unique \u2502 avg \u2502\n\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Id \u2502 BIGINT \u2502 21334825 \u2502 20113337 \u2502 11027766.241 \u2502\n\u2502 Reputation \u2502 BIGINT \u2502 1389256 \u2502 26919 \u2502 94.752717160 \u2502\u2502 CreationDate \u2502 TIMESTAMP \u2502 2023-03-05 \u2502 19557978 \u2502 \u2502\n\u2502 Views \u2502 BIGINT \u2502 2214048 \u2502 7452 \u2502 11.630429738 \u2502\n\u2502 UpVotes \u2502 BIGINT \u2502 591286 \u2502 6227 \u2502 8.7674283438 \u2502\u2502 DownVotes \u2502 BIGINT \u2502 1486341 \u2502 2930 \u2502 1.1697560125 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nIf you want to follow along without crea ting and ingesting the CSV files, use the\nStack Overflow example data from Mother Duck (see chapter 7) by running the\nfollowing:\nATTACH 'md:_share/stackoverflow/6c318917-6888-425a-bea1-5860c29947e5'\nAS stackoverflow`;\n10.1.5 Fast exploratory queries on large tables\nNow that we have our tables loaded, we can run a few more queries to see what kind of\ndata is available and how quickly we can ge t results. Let\u2019s say we\u2019re a Stack Overflow\nanalyst and want to check to see who are top users and whether they\u2019re still active.\nAnd if they aren\u2019t, perhaps we can come up with a way to persuade them to come backto the platform! We can find the top users by reputation along with their last login\ntime by writing the following query. \n.timer on\nSELECT DisplayName, Reputation, LastAccessDate\nFROM users\nORDER BY Reputation DESCLIMIT 5;\nThe query finishes in only 0.126 seconds for 20 million rows to analyze, which is quite\nfast:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 DisplayName \u2502 Reputation \u2502 LastAccessDate \u2502\n\u2502 varchar \u2502 int64 \u2502 timestamp \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Jon Skeet \u2502 1389256 \u2502 2023-03-04 19:54:19.74 \u2502\n\u2502 Gordon Linoff \u2502 1228338 \u2502 2023-03-04 15:16:02.617 \u2502\u2502 VonC \u2502 1194435 \u2502 2023-03-05 01:48:58.937 \u2502\n\u2502 BalusC \u2502 1069162 \u2502 2023-03-04 12:49:24.637 \u2502\n\u2502 Martijn Pieters \u2502 1016741 \u2502 2023-03-03 19:35:13.76 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 0.126 user 2.969485 sys 1.696962Listing 10.8 Top users  by reputation234 CHAPTER  10 Performance considerations for large datasets\nIf you\u2019re familiar with Stack Overflow, you wo n\u2019t be surprised to find the legend that is\nJon Skeet in top place. However, Jon has be en using Stack Overflow for a long time, so\nhe\u2019s had ages to accrue that reputation.\n It will be difficult for anyone to exc eed his total reputation score, but perhaps he\nhas more competition when it  comes to the reputation rate . We can compute this by\nworking out the number of re putation points per day on the platform, which will help\nus identify people who are gaining reputatio n points faster but don\u2019t yet have a high\ntotal reputation. We can comp ute this score by dividing reputation  by the number of\ndays from today to createdAt  so that our result gives us a reputation rate  per day, as\nshown in the following listing.\n.timer on\nSELECT DisplayName, reputation,\nround(reputation/day(today()-CreationDate)) as rate,day(today()-CreationDate) as days,CreationDate\nFROM users\nWHERE reputation > 1_000_000ORDER BY rate DESC;\nJon drops into second place on  this measure, with Gordon Linoff taking the top place.\nReputation on Stack Overflow also accrue s when people upvote  your past answers,\nand the more useful the answers you have give n are, the more likely it is that people\nwill upvote them over time:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 DisplayName \u2502 reputation \u2502 rate \u2502 days \u2502 CreationDate \u2502\n\u2502 varchar \u2502 int64 \u2502 double \u2502 int64 \u2502 timestamp \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Gordon Linoff \u2502 1228338 \u2502 294.0 \u2502 4181 \u2502 2012-01-11 19:53:57.59 \u2502\n\u2502 Jon Skeet \u2502 1389256 \u2502 258.0 \u2502 5383 \u2502 2008-09-26 12:05:05.15 \u2502\n\u2502 VonC \u2502 1194435 \u2502 221.0 \u2502 5396 \u2502 2008-09-13 22:22:33.173 \u2502\u2502 BalusC \u2502 1069162 \u2502 211.0 \u2502 5058 \u2502 2009-08-17 16:42:02.403 \u2502\n\u2502 T.J. Crowder \u2502 1010006 \u2502 200.0 \u2502 5059 \u2502 2009-08-16 11:00:22.497 \u2502\n\u2502 Martijn Pieters \u2502 1016741 \u2502 197.0 \u2502 5164 \u2502 2009-05-03 14:53:57.543 \u2502\u2502 Darin Dimitrov \u2502 1014014 \u2502 189.0 \u2502 5360 \u2502 2008-10-19 16:07:47.823 \u2502\n\u2502 Marc Gravell \u2502 1009857 \u2502 188.0 \u2502 5380 \u2502 2008-09-29 05:46:02.697 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Run Time (s): real 0.006 user 0.007980 sys 0.001260\nWe can turn that result into a bar chart using the bar function, which takes a value, a\nminimum and maximum value, and the width of  the bar, returning a string with the\nbar rendered as black blocks. To make th e query more readable, we can turn our\nexisting query into a common table expression (CTE) using WITH  and then use the\nbar function in the outer query, as  shown in the following listing. Listing 10.9 Top users  by reputation rate  per day\nComputes reputation rate, dividing the\nreputation by the number of days since\nthe user\u2019s creation date\nComputes the nu mber of days \nsince the user\u2019s creation date\nOnly considers users with a \nreputation of mo re than 1 million Orders by rate,\n in reverse order235 10.1 Loading and querying the full Stack Overflow database\n \nWITH top_users as (\nSELECT\nDisplayName,\nReputation,\nround(reputation/day(today()-CreationDate)) as rate,day(today()-CreationDate) as days,\nCreationDate\nFROM usersWHERE Reputation > 1_000_000\n)\nSELECT DisplayName, Reputation, rate, bar(rate,150,300,35) AS barFROM top_users\nORDER BY rate DESC;\nAs promised, the bar function will create an ASCII art chart for us:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 DisplayName \u2502 Reputation \u2502 rate \u2502 bar \u2502\n\u2502 varchar \u2502 int64 \u2502 double \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Gordon Linoff \u2502 1228338 \u2502 294.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502\n\u2502 Jon Skeet \u2502 1389256 \u2502 258.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2502 VonC \u2502 1194435 \u2502 221.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 BalusC \u2502 1069162 \u2502 211.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 T.J. Crowder \u2502 1010006 \u2502 200.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2502 Martijn Pieters \u2502 1016741 \u2502 197.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 Darin Dimitrov \u2502 1014014 \u2502 189.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 Marc Gravell \u2502 1009857 \u2502 188.0 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nSo those are our top users. \n Next, we want to get an understanding of  how much activity there is on the plat-\nform and whether it\u2019s changing  over time. We are going to do this by querying the\nposts  table, which has 58 million rows. In th e following listing, we will compute the\ntotal number of posts, the average view count, and the maximum number of answers\nfor each of the last 10 years by grouping the results accordingly.\nSELECT\nyear(CreationDate) AS year,\nround(count(*)/1000000,2) as postM,\nround(count_if(postTypeId = 1)/1000000,2) as questionM,round(count_if(postTypeId = 2)/1000000,2) as answerM,round(count_if(postTypeId = 1)/count_if(postTypeId = 2),2) as ratio,\nround(avg(ViewCount)) as avgViewCount,\nmax(AnswerCount) as maxAnswerCount\nFROM posts\nGROUP BY yearListing 10.10 Computing bar charts of the top users  by reputation rate per day\nListing 10.11 A query for yearly activity on Stack OverflowUses a named CTE, top_users, \nto refer back to later Wraps the\nsame query\nas before\nUses the bar() function on rate with a\nminimum of 150, a maximum of 300,\nand a width of 35\nSelects data from \ntop_users CTE\nThe count of posts \n(questions and \nanswers) in millionsThe count of \nquestions \nin millions\nThe count\nof answers\nin millions The ratio\nbetween\nquestions\nand answersThe average view \ncount of posts\nThe maxAnswerCount \nof postsGrouping by \nthe year236 CHAPTER  10 Performance considerations for large datasets\nORDER BY year DESC\nLIMIT 10;\nThe statistics of the last 10 years on Stack Overflow look like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 year \u2502 postM \u2502 questionM \u2502 answerM \u2502 ratio \u2502 avgViewCount \u2502maxAnswers \u2502\u2502 int64 \u2502 double \u2502 double \u2502 double \u2502 double \u2502 double \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023 \u2502 0.53 \u2502 0.27 \u2502 0.26 \u2502 1.03 \u2502 44.0 \u2502 15 \u2502\u2502 2022 \u2502 3.35 \u2502 1.61 \u2502 1.74 \u2502 0.93 \u2502 265.0 \u2502 44 \u2502\n\u2502 2021 \u2502 3.55 \u2502 1.55 \u2502 2.0 \u2502 0.78 \u2502 580.0 \u2502 65 \u2502\n\u2502 2020 \u2502 4.31 \u2502 1.87 \u2502 2.44 \u2502 0.77 \u2502 847.0 \u2502 59 \u2502\u2502 2019 \u2502 4.16 \u2502 1.77 \u2502 2.39 \u2502 0.74 \u2502 1190.0 \u2502 60 \u2502\n\u2502 2018 \u2502 4.44 \u2502 1.89 \u2502 2.55 \u2502 0.74 \u2502 1648.0 \u2502 121 \u2502\n\u2502 2017 \u2502 5.02 \u2502 2.11 \u2502 2.9 \u2502 0.73 \u2502 1994.0 \u2502 65 \u2502\u2502 2016 \u2502 5.28 \u2502 2.2 \u2502 3.07 \u2502 0.72 \u2502 2202.0 \u2502 74 \u2502\n\u2502 2015 \u2502 5.35 \u2502 2.2 \u2502 3.14 \u2502 0.7 \u2502 2349.0 \u2502 82 \u2502\n\u2502 2014 \u2502 5.34 \u2502 2.13 \u2502 3.19 \u2502 0.67 \u2502 2841.0 \u2502 92 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25021 0r o w s 7 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 5.977 user 7.498157 sys 5.480121 (1st run)\nRun Time (s): real 0.039 user 4.609049 sys 0.078694\nEven when computing these statistics acro ss 58 million rows, the query only takes a\nfew seconds to run for the first run and on ly 40 milliseconds for the subsequent run\nafter the data had been  loaded from disk.\n We don\u2019t have complete data for 2023 in this dataset, so we can\u2019t compare the full\ndata for that year. The total number of post s (the sum of questions and answers) asked\nhas been falling steadily since 2014. The ra tio of questions to answers has also gotten\nworse, as there are more questions than answ ers as of the last 2 years. The view count\nof newer posts is also smaller due to thei r relatively short existence. The maximum\nnumber of answers per post is slightly decreasing over the years too, as older questions\nhad more time to collect answers.\n10.1.6 Posting on weekdays\nWe\u2019re curious when people use the platform. Do they only use it to answer questions\nat work, or do they use it on the weekend as well?\n This question was also asked by Eval ina Gabova in her Stack Overflow Analysis\n(https:/ /evelinag.com/exp loring-stackoverflow/ ). Let\u2019s see if we can reproduce Eva-\nlina\u2019s analysis.\n Here is a query that shows us the questions for the sql tag, grouped by day of the\nweek, with the frequency and a bar chart.\n Sorting years in descending \norder (latest first) Showing the \nlast 10 years237 10.1 Loading and querying the full Stack Overflow database\n \nSELECT count(*) as freq,\ndayname(CreationDate) AS day,\nbar(freq, 0, 150000,20) AS plot\nFROM posts WHERE posttypeid = 1\nAND tags LIKE '%<sql>%'\nGROUP BY allORDER BY freq DESC;\nWe see that most questions are posted on weekdays, especially during the middle of\nthe week, and the least on weekends. That co uld indicate that people mostly deal with\nSQL for work and ask questions during their work time:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 freq \u2502 day \u2502 plot \u2502\n\u2502 int64 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 119825 \u2502 Wednesday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 119514 \u2502 Thursday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 115575 \u2502 Tuesday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2502 103937 \u2502 Monday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 103445 \u2502 Friday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 47390 \u2502 Sunday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2502 47139 \u2502 Saturday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 0.303 user 2.780285 sys 0.010856\nProcessing the 23.5 mi llion questions took only 0.3 se conds before a result was com-\nputed.\n But what if we look at a different tag? Rust is a relatively new programming lan-\nguage that isn\u2019t as ingrained in companie s as SQL. When do people ask questions\nabout Rust?\nSELECT count(*) as freq,\ndayname(CreationDate) AS day,\nbar(freq, 0, 10000,20) AS plot\nFROM posts WHERE posttypeid = 1\nAND tags LIKE '%<rust>%'\nGROUP BY allORDER BY freq DESC;\nThe questions are evenly distributed througho ut the week, with a slight drop on the\nweekend:\n Listing 10.12 Query for asking questions about SQL on weekdays\nListing 10.13 Query for asking questions about Rust on weekdaysGets the day of the week \nfrom the CreationDate\nPlots a bar chart for the \nnumber of questions\nOnly looks at questions \nwith the tag .sql\nGroups by the \nday of the week Orders by the frequency\nin descending order238 CHAPTER  10 Performance considerations for large datasets\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 freq \u2502 day \u2502 plot \u2502\u2502 int64 \u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5205 \u2502 Wednesday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2502 5167 \u2502 Tuesday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 5160 \u2502 Thursday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 5054 \u2502 Monday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2502 5009 \u2502 Friday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 4784 \u2502 Sunday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502 4667 \u2502 Saturday \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe should also note that there are betw een 10 and 20 times fewer questions asked\nabout Rust than SQL!\n10.1.7 Using enums for tags\nWhen you\u2019re using DuckDB with larger datasets, you might sometimes choose to do\noptimizations that wouldn\u2019t be necessary on  a smaller dataset. An example of this in\nthe Stack Overflow dataset is the way that tag names assigned to posts  are stored and\nprocessed. \n DuckDB has the concept of enum types , which represent a fixed set of named values\nthat are stored internally as integers. This  is more efficient for storage and processing\nthan large amounts of string values. You ca n read more about it in the enum docu-\nmentation ( https:/ /duckdb.org/docs/ sql/data_types/enum.html ). \n You can create enums as a type ba sed on a list of values like this.\nCREATE TYPE weekday AS enum (\n'monday', 'tuesday', 'wednesday',\n'thursday', 'friday', 'saturday', 'sunday'\n);\nThe values are accessible via their string representation (e.g., as 'saturday'  or in a\ntyped way as saturday::weekday ). The enum type itself can be referred to with the\nNULL  value (e.g., null::weekday ). \n There are several functions for operating on enums:\n\uf0a1enum_code(enum_str) \u2014This returns the numeric code of the enum.\n\uf0a1enum_range(null:enum_type) \u2014This returns all values of the enum type as a list.\n\uf0a1enum_first ,enum_last , and enum_range_boundary \u2014These are for the first, the\nlast, or a range of values.\nDuckDB enums are automatically cast to st ring types whenever necessary. This allows\nenum values to be used in any string function and comparis ons between enum and\nstring values.\n Although we have a tags table, up to six tags can also be stored in a post. Each tag is\nwrapped in angle brackets (e.g., <sql><duckdb><performance> ), which means theListing 10.14 Example of creating an enum type239 10.1 Loading and querying the full Stack Overflow database\npost has the tags sql, duckdb , and performance . This isn\u2019t particularly efficient for\nanalytical querying, and it ma kes it difficult to search posts  for multiple tags. \n That\u2019s why we want to convert that column into a set of values that makes it easier\nto handle. For instance, we can use a list of strings, or we can use an enum type, which\nis more efficient, as previously mentioned,  and also allows us to use the enum func-\ntions built into DuckDB.\n To convert the tags  column in our posts  table into a list of enums, we need to fol-\nlow a series of intermediate steps. We\u2019re going to start by creating an enum type from\nthe values in the tags  table in the following listing.\nCREATE TYPE tag AS enum (SELECT DISTINCT tagname FROM tags);\nWe can get a list of the enums by running the following query.\nselect enum_range(null::tag)[0:5];\nThis shows us a few known and lesser-known tags:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 enum_range(CAST(NULL AS tag))[0:5] \u2502\n\u2502 varchar[] \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 [textblock, idioms, haskell, flush, etl] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNow let\u2019s see how we can use this enum in our posts  table.\n We\u2019re first going to add an intermediate column for a string array, which we can\nalso use to compare the performance of the enum array.\nALTER TABLE posts ADD tagNames VARCHAR[];\nNext, we\u2019ll split the tags string (e.g., '<sql><duckdb><python>' ) into an array of\nstrings by taking the substr ing after the first character < to before the last character >\nto skip the leading and trailing angl e brackets and then splitting on the >< separator\nbetween two entries.\nUPDATE posts\nSET tagNames = split(tags[2:-2],'><')Listing 10.15 Creating an enum type for tags  from the tags  table\nListing 10.16 Selecting a few of the enum values\nListing 10.17 Adding the tagNames  column\nListing 10.18 Populating the tagNames  column from the tags  column\nUpdates the posts table\nSplits the tags text (except for the first and \nlast 1 character) on >< into an array240 CHAPTER  10 Performance considerations for large datasets\nWHERE posttypeid = 1;\n-- Run Time (s): real 51.120 user 61.063576 sys 2.088018\nAs you can see from the run time, this update takes over a minute because we have a\nlot of rows (23.5 million) to process. No w we\u2019re ready to add our new enum type tag,\nwhich we had populated by the values from the tags  table.\n Let\u2019s now add the tag enum array to the posts  table.\nALTER TABLE posts ADD tagEnums tag[];\nSo our posts  table now looks like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502\n\u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 Id \u2502 BIGINT \u2502\u2502 PostTypeId \u2502 BIGINT \u2502\n\u2502 AcceptedAnswerId \u2502 BIGINT \u2502\n\u2502 CreationDate \u2502 TIMESTAMP \u2502\u2502 Score \u2502 BIGINT \u2502\n\u2502 ViewCount \u2502 BIGINT \u2502\n\u2502 Body \u2502 VARCHAR \u2502\u2502 Title \u2502 VARCHAR \u2502\n...\n\u2502 Tags \u2502 VARCHAR \u2502\u2502 tagNames \u2502 VARCHAR[] \u2502\n\u2502 tagEnums \u2502 ENUM(tag)[] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe can assign the string array to the enum  array, and DuckDB wi ll automatically cast\nthe values to the enum type, as shown in the following listing. That\u2019s really helpful and\nuser friendly.\nUPDATE posts SET tagEnums = tagNames\nWHERE posttypeid = 1;\nIf we want additional tran sformations on the values (like capitalization or changing\nhyphens to spaces in the tag names), we coul d use a list transform function to convert\nthe values of the string array to entries in an equivalent enum array. We can do that by\nusing a list_transform  function on each element of the tagNames  list and passing in\na lambda function that casts each el ement to the appropriate enum type:\nSET tagEnums = list_transform(tagNames, x -> upper(x)::tag)Listing 10.19 Adding the tagEnums  column\nListing 10.20 Populating the tagEnums  column from the tagNames  column directlyOnly considers questions \n(i.e., posttypeid is 1)241 10.1 Loading and querying the full Stack Overflow database\nBy keeping the string array around, we can compare the behavior and performance of\na list of strings and a list of enums. For example, the following queries count the num-\nber of posts with the tag named java , which gives us 1.9 million rows.\nSELECT count(*)\nFROM posts\nWHERE postTypeId = 1\nAND tags LIKE '%<java>%';\nRunning this query on 28 million rows takes 0.3 seconds, which is already pretty good,\nbut let\u2019s see if we can do better with the next approaches.\nSELECT count(*)\nFROM postsWHERE postTypeId=1\nAND list_contains(tagNames, 'java');\nThe list_contains  operation has a runtime of 0.24  seconds, which is .06 seconds\n(20%) faster than the string comparison.\nselect count(*)\nfrom posts\nwhere postTypeId=1 and list_contains(tagEnums, 'java');\nWith an execution time of 0.17s, we\u2019v e achieved another 30% (0.07 second) speed\nimprovement by using the enums, which almo st halves the total execution time. How-\never, this is less relevant at these short ru n times. You should see similar small improve-\nments on your machine. So for these si mple count operations, the difference is\nnegligible, but for more complex queries, the enum type can be more efficient and\neasier to work with.\n Let\u2019s have a look at one more query, whic h should be faster after this optimization.\nTo count the top-ranking tags along with th eir score, we can writ e the following query,\nwhich uses the string-based tags  column.\nSELECT tag,\ncount(*), sum(score) AS score\nFROM (\nSELECT unnest(split(p.tags[2:-2],'><')) as tag,Listing 10.21 Counting posts with the java  tag, using string comparison\nListing 10.22 Counting posts with the java  tag, using list_contains  on the string list\nListing 10.23 Counting posts with the java  tag, using list_contains  on the enum list\nListing 10.24 Statistics query for the top 10 tags, using the tags  columnThis time, we use list_contains \non the tagNam es column.\nCounts the number \nof posts and sums \ntheir score by tag242 CHAPTER  10 Performance considerations for large datasets\np.score AS score\nFROM posts p WHERE p.posttypeid = 1\n)GROUP BY ALL\nORDER BY score DESC LIMIT 10;\nThat query takes 6.7 seconds an d returns the following results:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 tag \u2502 count_star() \u2502 score \u2502\n\u2502 varchar \u2502 int64 \u2502 int128 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 javascript \u2502 2479793 \u2502 5214097 \u2502\n\u2502 python \u2502 2112946 \u2502 5154237 \u2502\u2502 java \u2502 1889685 \u2502 4280171 \u2502\n\u2502 c# \u2502 1583813 \u2502 3790940 \u2502\n\u2502 android \u2502 1399966 \u2502 3241732 \u2502\u2502 c++ \u2502 789658 \u2502 2166603 \u2502\n\u2502 html \u2502 1167672 \u2502 1995072 \u2502\n\u2502 php \u2502 1456223 \u2502 1793966 \u2502\u2502 jquery \u2502 1033102 \u2502 1684906 \u2502\u2502 git \u2502 147408 \u2502 1662800 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 10 rows 3 columns \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n-- Run Time (s): real 6.698 user 60.197508 sys 0.197970\nWhat is the performance if we use the enum field instead?\nSELECT tag, count(*), sum(score) AS score\nFROM (\nSELECT unnest(p.tagEnums) as tag,\np.score AS score\nFROM posts p\nWHERE p.posttypeid = 1\n)GROUP BY ALL\nORDER BY score DESC LIMIT 10;\n-- Run Time (s): real 3.546 user 31.661123 sys 0.072986\nThis one is almost twice as fast at 3.5 seconds.\n So we see that for more complex operations that operate on many values of the list\nof values, the change in representation ma tters. With these kinds of optimizations, we\nneed to invest more effort upfront\u2014bot h in terms of prepar ation and storage\u2014to\nachieve better performance later on. This is worthwhile when the analytics query isrun many times (e.g., for dashbo ards or reports) but less so if it\u2019s a one-off operation.\nThe preparatory work al so needs to be integrated into your data processing pipelines\nto ensure the correct shape of data for your queries.Listing 10.25 Statistics for the top 10 tags using the tagEnums  columnSplits the tags text (except \nfor the first and last character) on >< into an \narray and then unnests it \ninto individual rowsUses the score\ncolumn of each post\nOnly considers questions \nwith the posttypeid of 1\nTurns the tagEnums list \ninto individual rows, \nwith one per entry243 10.2 Query planning and execution\n10.2 Query planning and execution\nIn the large datasets we are working with in  this chapter, efficient query execution is\nmore crucial than ever. DuckDB \u2019s query execution engine is designed to be fast and\nefficient, using modern hardware and the latest database research and implementa-\ntion techniques to achieve this goal. \n Let\u2019s look a bit at how DuckDB processe s our queries internally and make use of\nEXPLAIN  and EXPLAIN ANALYZE  to see which operations an d operators our queries are\nturned into and how we can optimize th em. The steps of the execute process are\nshown in figure 10.3.\nFigure 10.3 Query processing pipeline\n10.2.1 Planner and optimizer\nOnce DuckDB has parsed an SQL query usin g its flexible parser derived from Post-\ngres, it transforms the resulting abstract sy ntax tree (AST) through several stages. In\nthe parse phase , the system can detect syntactic errors, like missp elled keywords or miss-\ning parentheses. \n Initially, the binder  resolves elements like tables, views, types, and column names.\nHere, the processing checks to see if the used elements (tables, columns, and types)\nexist in the database and whether they are correctly used. Following that, the plan gen-\nerator  converts this into a basic logical query plan consisting of logical query operators,\nsuch as scans, filters, and projections.\n During the planning process, the database  system uses statistics from stored data\nand indexes, which assist in  various operations, such as  type transformations, join\norder optimizations, and subquery flattening. After these optimizations, an optimized\nlogical query plan  is created. Ultimately, the planner refines this logical plan into physi-\ncal operations best tailored to the enviro nment, considering statistics, caching, and\nother factors. \n10.2.2 Runtime and vectorization\nDuckDB\u2019s runtime operates on a vectorized and parallelized archit ecture based on its\ncolumnar storage nature. DuckDB\u2019s st orage format stores the data in row groups  (i.e.,\nhorizontal partitions of the data). Horizont al partitioning of data is a strategy for\nsharding data, in which each partition has the same schema and holds a specific sub-\nset of the data. A row group in DuckDB\u2019s database format consists of a maximum of\n122,880 rows. Each row group contains the required information about each column. Parser\nSQL StatementUnoptimized\nlogical planOptimized\nlogical planPhysical\nplanPlanner Optimizer Physical planner244 CHAPTER  10 Performance considerations for large datasets\n This column-centric approach offers nume rous advantages, especially when select-\ning columns or filtering, sca nning, and sorting data. It also allows the CPU to keep the\nprocessing of an operator in-memory, opti mize CPU branch prediction, and have all\nrequired data in CPU caches.\n A main difference compared to row-base d engines lies in DuckDB\u2019s fine-tuning for\nefficient data operations, rather than disk  storage or data transfer (I/O) optimiza-\ntions. In the execution runtime, every data type is represented as a vector or compact\narray of values. These typed vector impl ementations are optimized for various data\ntypes and values (numbers, strings, and arra ys), simplifying data selection and process-\ning by employing compression, metadata, and additional indexes.\n As data flows through the system, these vectors seamlessly transition between plan\noperators in a push-based manner. The exec ution model is centered on a pipeline\ndesign, wherein operators can act as sources, sinks, or both. DuckDB\u2019s execution par-\nallelizes batch processing through a \u201cmor sel\u201d approach, which processes chunks of\nvalues (batches of 2,048 values) throug h a number of parallel pipelines, with\nparallelism-aware oper ators at the start and end of ea ch pipeline. Figure 10.4 shows\ndifferent morsels passing through di fferent pipelines of the runtime.\nFigure 10.4 Morsel runtime\nIn addition, DuckDB also uses vectorized computation, which em ploys single instruc-\ntion, multiple data (SIMD) to process multiple values in a single CPU instruction.\nThis is not to be confused with the data vectors. 0 1 2\nThread A Thread B Thread C\nJoin+FilterJoin, Filter TableScan Join, Filter, Projection\nTableScan\nJoin+Filter\n+Projection245 10.2 Query planning and execution\n10.2.3 Visualizing query plans with Explain and Explain Analyze\nThe query plan the optimizer and planner cr eates is also accessible to you. You can\nprefix each SQL statement with EXPLAIN  to see the tree of operators your original\nquery was transformed into:\nEXPLAIN\nSELECT year(CreationDate) AS year, count(*),\nround(avg(ViewCount)), max(AnswerCount)\nFROM postsGROUP BY year\nORDER BY year DESC LIMIT 10;\nA visualization of the resulting query plan is reproduced as follows. We can see that\nour query has been turn ed into the following operators (a s well as some internal oper-\nators that are used for data compression and decompression):\n\uf0a1ORDER BY  and LIMIT  \u2192 TOP_N\n\uf0a1SELECT  \u2192 PROJECTION\n\uf0a1GROUP BY  \u2192 Perfect_Hash_Group_By\n\uf0a1SELECT  + FROM  \u2192 SEQ_SCAN  with estimated cardinality (EC)\nThe plan is executed bottom up ; it starts with the SEQ_SCAN  of the stored data and then\napplies operators on the chunks of results from previous ones. When run, the physical\nplan looks like this:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TOP_N \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 Top 10 \u2502\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 year(posts.CreationDate) \u2502\n\u2502 DESC \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROJECTION \u2502\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u25020\u2502\n\u2502 count_star() \u2502\u2502 round(avg(ViewCount)) \u2502\n\u2502 max(AnswerCount) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROJECTION \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502__internal_decompress_integ\u2502\n\u2502 ral_bigint(#0, 2008) \u2502\n\u2502# 1 \u2502\u2502# 2 \u2502\n\u2502# 3 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518246 CHAPTER  10 Performance considerations for large datasets\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PERFECT_HASH_GROUP_BY \u2502\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502# 0 \u2502\n\u2502 count_star() \u2502\u2502 avg(#1) \u2502\n\u2502 max(#2) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROJECTION \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502__internal_compress_integra\u2502\n\u2502 l_utinyint(year \u2502\n\u2502 (CreationDate), 2008) \u2502\u2502 ViewCount \u2502\n\u2502 AnswerCount \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SEQ_SCAN \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502 posts \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 CreationDate \u2502\n\u2502 ViewCount \u2502\n\u2502 AnswerCount \u2502\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 EC: 81865857 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Run Time (s): real 0.003 user 0.001378 sys 0.001833\nWith EXPLAIN ANALYZE , our query is not just planned bu t executed as well so we can\nsee the actual time, resources, and nu mber of rows that were processed:\nEXPLAIN ANALYZE\nSELECT year(CreationDate) AS year, count(*),\nround(avg(ViewCount)), max(AnswerCount)\nFROM posts\nGROUP BY year\nORDER BY year DESC LIMIT 10;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 EXPLAIN_ANALYZE \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u25020\u2502\n\u2502 (0.00s) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TOP_N \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502 Top 10 \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 year(posts.CreationDate) \u2502\u2502 DESC \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u25021 0 \u2502\u2502 (0.00s) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518247 10.2 Query planning and execution\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROJECTION \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u25020\u2502\n\u2502 count_star() \u2502\n\u2502 round(avg(ViewCount)) \u2502\u2502 max(AnswerCount) \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u25021 6 \u2502\u2502 (0.00s) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 PROJECTION \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 _internal_decompress_integ\u2502\u2502 ral_bigint(#0, 2008) \u2502\n\u2502# 1 \u2502\n\u2502# 2 \u2502\u2502# 3 \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u25021 6 \u2502\u2502 (0.00s) \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PERFECT_HASH_GROUP_BY \u2502\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502# 0 \u2502\n\u2502 count_star() \u2502\u2502 avg(#1) \u2502\n\u2502 max(#2) \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u25021 6 \u2502\n\u2502 (0.55s) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROJECTION \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502 _internal_compress_integra\u2502\n\u2502 l_utinyint(year \u2502\n\u2502 (CreationDate), 2008) \u2502\u2502 ViewCount \u2502\n\u2502 AnswerCount \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502 58329356 \u2502\n\u2502 (0.39s) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SEQ_SCAN \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502 posts \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 CreationDate \u2502\u2502 ViewCount \u2502\n\u2502 AnswerCount \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\u2502 EC: 81865857 \u2502248 CHAPTER  10 Performance considerations for large datasets\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502 58329356 \u2502\n\u2502 (0.98s) \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 0.199 user 1.367440 sys 0.379777\nThis helps us identify (especially for sl ow queries) which operators need the most\ntime as well as which ones return the most  intermediate rows. We can use that infor-\nmation to optimize our queries\u2014for instance , by adding indexes, adding query hints,\nusing subqueries, or changing  the order of operations. \n10.3 Exporting the Stack Ov erflow data to Parquet\nWe can export our tables to Parquet files fo r safekeeping, easier storage, and process-\ning in other ways. As we discussed before , Parquet as a column ar format compresses\nbetter, includes the schema, and supports optimized reading with column selection\nand predicate pushdown. We want to see here  how long these expo rts take, what opti-\nmizations we can apply, and ho w to export whole databases. \n Supported compression formats for Parquet are UNCOMPRESSED , SNAPPY , and ZSTD .\nTo export the users  table, we can run the followin g command, which takes about 10\nseconds for the 28 million rows. \nCOPY (FROM users)\nTO 'users.parquet'\n(FORMAT PARQUET, CODEC 'SNAPPY', ROW_GROUP_SIZE 100000);\n-- Run Time (s): real 10.582 user 62.737265 sys 65.422181\nAnd then for the posts  table, the statement looks like the following code and takes\nroughly 60 seconds fo r 58 million rows.\nCOPY (FROM posts)\nTO 'posts.parquet'(FORMAT PARQUET, CODEC 'SNAPPY', ROW_GROUP_SIZE 100000);\n-- Run Time (s): real 57.314 user 409.517658 sys 334.606894\nNOTE We saw that the serial export take s between 10 and 60 seconds for our\ntables, which is a write output of about 70 MB/s on a single thread. To opti-\nmize write performance, you can also choose to write the Parquet file in amultithreaded fashion; DuckDB will then create one file per thread. Thisimproves the performance on our system with 10 CPUs from 10 to 1.7 sec-onds for the \nuser  table and 57 to 11 seconds for the post  table. We could also\nchoose to sort the data before exportin g it, which could lead to faster query-\ning of sorted fields when the Parquet file is read, as you can see in the follow-ing listing.Listing 10.26 Exporting the users  table to Parquet\nListing 10.27 Exporting the posts  table to Parquet249 10.3 Exporting the Stack Overflow data to Parquet\nCOPY (\nSELECT *\nFROM usersORDER BY LastAccessDate DESC\n) TO 'users.parquet'\n(FORMAT PARQUET, CODEC 'SNAPPY', PER_THREAD_OUTPUT TRUE);\nWe also exported the other tables but won\u2019t include the COPY  commands here for\nbrevity\u2019s sake. You can find those commands  in the book\u2019s GitHub repository. Once all\nthe exports are done, we\u2019ll have the following files on our disk:\n6.9G comments.parquet\n4.0G posts.parquet2.2G votes.parquet\n734M users.parquet\n518M badges.parquet164M post_links.parquet\n1.6M tags.parquet\nNOTE These Parquet files are available on  the S3 bucket (s3://us-prd-moth-\nerduck-open-datasets/stackoverflow/parquet/2023-05/) and can be readfrom there.\nOut of curiosity, we\u2019re going to compare the performance of reading the users\u2019 data\nfrom the CSV and Parquet files to see which is  quicker. Let\u2019s start with the Parquet file,\nas shown in the following listing.\nSELECT count(*) FROM read_parquet('users.parquet');\nThis gives us a result of 19942787  in subsecond (0.008s) ti me. Querying the CSV file,\non the other hand, take s considerably longer.\nSELECT count(*) FROM read_csv_auto('Users.csv.gz');\nThe result is the same, but it takes about 7 seconds to get there. So reading the row\ncount from the Parquet file is almost 1,000 times faster than the CSV file!\n This isn\u2019t a fair fight because the Parquet file can use its metadata to provide the\nanswer, rather than having to scan the whol e file. Having that metadata is one of the\nbig benefits of the Parquet file format and is why we tend to prefer it to CSV files.\n You can also export your whole database as Parquet files into a target folder. This\nsaves you from calling the ex port commands individually and automatically adds files\nfor creating the table schema and load ing the data back in with a single import\ncommand. Listing 10.28 Exporting the users  table to Parquet in multithreaded fashion\nListing 10.29 Reading the row count of users from a Parquet\nListing 10.30 Reading the row count of users from a CSV250 CHAPTER  10 Performance considerations for large datasets\n \nEXPORT DATABASE 'target_directory'\n(FORMAT PARQUET);\nIn addition to the Parquet files, this will create two SQL files, schema.sql  and\nload.sql , which will be used for creating th e database schema and executing the\nload, when you import the data again with\nIMPORT DATABASE 'source_directory';\nIn the schema.sql file tables, views and en ums are created, as shown in the following\nlisting.\nCREATE TABLE posts(Id BIGINT, PostTypeId BIGINT, AcceptedAnswerId BIGINT,\nCreationDate TIMESTAMP, Score BIGINT, ViewCount BIGINT, Body VARCHAR,OwnerUserId BIGINT, LastEditorUserId BIGINT, LastEditorDisplayNameVARCHAR, LastEditDate TIMESTAMP, LastActivityDate TIMESTAMP, TitleVARCHAR, Tags VARCHAR, AnswerCount BIGINT, CommentCount BIGINT,FavoriteCount BIGINT, CommunityOwnedDate TIMESTAMP, ContentLicenseVARCHAR\n);CREATE TABLE \"comments\"(Id BIGINT, PostId BIGINT, Score BIGINT, \"Text\"\nVARCHAR, CreationDate TIMESTAMP, UserId BIGINT, ContentLicense VARCHAR);\nCREATE TABLE badges(Id BIGINT, UserId BIGINT, \"Name\" VARCHAR, Date\nTIMESTAMP, \"Class\" BIGINT, TagBased BOOLEAN);\nCREATE TABLE users(Id BIGINT, Reputation BIGINT, CreationDate TIMESTAMP,\nDisplayName VARCHAR, LastAccessDate TIMESTAMP, AboutMe VARCHAR,\"Views\" BIGINT, UpVotes BIGINT, DownVotes BIGINT);\nCREATE TABLE tags(Id BIGINT, TagName VARCHAR, Count BIGINT,\nExcerptPostId BIGINT, WikiPostId BIGINT);\nCREATE TABLE votes(Id BIGINT, PostId BIGINT, VoteTypeId BIGINT,\nCreationDate TIMESTAMP);\nAnd in the load.sql file, the data is loaded from the Parquet files, as shown in the fol-\nlowing listing.\nCOPY posts FROM 'parquet/posts.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');\nCOPY \"comments\" FROM 'parquet/comments.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');\nCOPY badges FROM 'parquet/badges.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');\nCOPY users FROM 'parquet/users.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');\nCOPY tags FROM 'parquet/tags.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');Listing 10.31 Exporting the whole database to Parquet\nListing 10.32 The contents of schema.sql\nListing 10.33 The contents of load.sql251 10.4 Exploring the New York Taxi dataset from Parquet files\nCOPY votes FROM 'parquet/votes.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');\nCOPY post_links FROM 'parquet/post_links.parquet' (FORMAT 'parquet',\nROW_GROUP_SIZE 100000, CODEC 'SNAPPY');\nAs we can see in this Stack Overflow ex ample, even medium-sized datasets are no\nproblem for DuckDB. We can import, query, pr ocess, and export them in very reason-\nable times on a single machine with the memory and CPU resources of a modern lap-\ntop or desktop (e.g., 4\u201310 cores and 8\u201364 GB RAM). It doesn\u2019t fail or abort with out-\nof-memory errors, and it doesn\u2019t take hours to process the data.\n But can we go bigger, say billions  of records?! The New York City taxi dataset is the\ngo-to dataset for seeing whether new databa se systems can handle big data, and we\u2019re\ngoing to explore that in the next section. \n10.4 Exploring the New York Taxi  dataset from Parquet files\nThe New York City taxi dataset we use in this example ( https:/ /mng.bz/qODE ) is pub-\nlished and maintained by the NYC Taxi & Limousine Commission. This dataset con-\ntains taxi trip records and includes pickup  and drop-off dates, times, and locations;\ntrip distances; itemized fares; rate types;  payment types; and driver-reported passenger\ncounts. The data is published in Parque t format, with one Parquet file for each\nmonth, starting in January 2009 , and is continuously updated. \n Over the years, many articles have been  written explaining how to load and query\nthe dataset in R, Python, Spark, Redshift, SQLite, and other databases. Just search SQL\nAnalysis New York Taxis  on your favorite search engine to find some of them.\n At the time of writing, the dataset contai ns over 1.7 billion ro ws, consisting of 175\nParquet files of 28 GB in size. We have co llected all the Parquet files and put them in\nan S3 bucket (s3://us-prd-md-duckdb-in -action/nyc-taxis/), kindly hosted by\nMotherDuck.\n This section of the large data explor ation will focus on using Parquet files as\nsources for querying and show that DuckDB can use predicate and projection push-\ndown to optimize the queries on these file s without actually populating a database.\nYou could use this approach when doing a on e-off analysis of data stored on a cloud\nstorage bucket, such as S3 or Google Cloud Storage. For instance, you can use it toanalyze access or download logs of your website or application.\n For these amounts of data, it is beneficial to move the query computation to where\nthe data lives and only transfer the results ov er the network. If you have the files down-\nloaded onto your local machine, you can exec ute DuckDB there. Otherwise, it is sensi-\nble to run DuckDB on a cloud in stance that is as close to the data as possible. In our\ncase, with the Parquet files on S3, that can be an EC2 instance in the same region as\nthe S3 bucket or a hosted service, like MotherDuck. If not, you would have to pay boththe egress costs and network transfer time and latency for reading the files to your\nlocal machine when executing queries that cannot be satisfied by the Parquet meta-\ndata alone.252 CHAPTER  10 Performance considerations for large datasets\n10.4.1 Configuring credentials for S3 access\nThe objects on the aforementioned bucket  are publicly readable , so you can access\nthem directly from DuckDB wi thout any extra setup. But if you want to try this out\nwith your own S3 bucket, which you probabl y don\u2019t want to make accessible to the\nworld, you can configure your S3 credenti als by creating a temporary or persistent\nsecret (available from duckdb version 0.1.0 ) of TYPE S3 . \n The REGION  is the region of your bucket, while the KEY_ID  and SECRET  are the cre-\ndentials for accessing the bucket, shown in the following listing.\nCREATE [PERSISTENT] SECRET (\nTYPE S3,KEY_ID 'AKIA...',\nSECRET ''Sr8VSfK...',\nREGION 'us-east-1'\n);\nThe httpfs  extension is used to access files on S3, which is auto-loaded by DuckDB\nwhen necessary. In case it is not automatica lly loaded for you, you can do so manually,\nas shown in the following listing. \nINSTALL httpfs;\nLOAD httpfs;\nNow we can access the Parquet files directly without any extra set up just by specifying\nthe filename or URL in the FROM  clause of a statement. \n10.4.2 Auto-inferring file types\nTo count the number of records in a Parquet file, you can write a query like the follow-ing to compute the row count of a Parquet file:\nSELECT count(*)\nFROM's3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_2022-06.parquet';\n-- 3,558,124 rows in 600 ms\nDespite reading a huge file, it finishes in 600 milliseconds because the query uses the\nmetadata of the file to compute the answer . Under the hood, this  query is converted\nto the following read_parquet  function call:\nSELECT count(*)\nFROM read_parquet(\n's3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_2022-06.parquet'\n);-- 3,558,124 rowsListing 10.34 Creating the S3 secret\nListing 10.35 Installing and loading the httpfs  extension253 10.4 Exploring the New York Taxi dataset from Parquet files\nInference from filename extensions works au tomatically when we want to process data\nfrom a single file or URL or from a glob-wi ldcard pattern that ma tches multiple files,\nlike this:\nSELECT count(*)\nFROM 's3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_*.parquet';\n-- 1,721,158,822 rows in 11s\nBut if we want to load a more specific set of files, for example, to find trips in June\n2021 and June 2022, we need to call the underlying read_parquet  function ourselves:\nSELECT count(*)\nFROM read_parquet([\n's3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_2021-06.parquet','s3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_2022-06.parquet'\n]);\n-- 6,392,388\nThe queries we\u2019ve seen so far have all be en counting numbers of records, and they\nreturn a result quickly because they\u2019re able  to use Parquet metadata to compute the\nanswer and don\u2019t need to  read the actual data.\n The metadata of a Parquet file, as alread y mentioned in chapte r 5, contains infor-\nmation about the number of rows, the sc hema (column names and types), and the\nmininum values, maximum values, and nullabilit y of each column. Let\u2019s have a look at\nthat metadata. \n10.4.3 Exploring Parquet schema\nBefore we start querying the Parquet files, let\u2019s have a quick look at the structure of\nthe data they contain. Throughout this book, we\u2019ve learned about various functions\nwe can use to do this. We\u2019re going to use the parquet_schema  function here since this\ngives us control over the fi elds rendered, and since we\u2019r e mostly interested in the\nname and type, this is exactly what we need:\nFROM parquet_schema(\n's3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_2022-06.parquet')SELECT name, type;\nThe output of this query is as follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 name \u2502 type \u2502\n\u2502 varchar \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 schema \u2502 \u2502\n\u2502 VendorID \u2502 INT64 \u2502\n\u2502 tpep_pickup_datetime \u2502 INT64 \u2502\u2502 tpep_dropoff_datetime \u2502 INT64 \u2502\n\u2502 passenger_count \u2502 DOUBLE \u2502\n\u2502 trip_distance \u2502 DOUBLE \u2502254 CHAPTER  10 Performance considerations for large datasets\n\u2502 RatecodeID \u2502 DOUBLE \u2502\n\u2502 store_and_fwd_flag \u2502 BYTE_ARRAY \u2502\n\u2502 PULocationID \u2502 INT64 \u2502\u2502 DOLocationID \u2502 INT64 \u2502\n\u2502 payment_type \u2502 INT64 \u2502\n\u2502 fare_amount \u2502 DOUBLE \u2502\u2502 extra \u2502 DOUBLE \u2502\n\u2502 mta_tax \u2502 DOUBLE \u2502\n\u2502 tip_amount \u2502 DOUBLE \u2502\u2502 tolls_amount \u2502 DOUBLE \u2502\n\u2502 improvement_surcharge \u2502 DOUBLE \u2502\n\u2502 total_amount \u2502 DOUBLE \u2502\u2502 congestion_surcharge \u2502 DOUBLE \u2502\n\u2502 airport_fee \u2502 DOUBLE \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 20 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWe can see all the types of information relevant  to a taxi trip and their data types, like\nthe passenger count, pickup and drop off lo cations, and, of course, the amount that\nthe trip costs!\nNOTE If the Parquet files you\u2019re reading from have different schemas, you\ncan use the union_by_name = true  option of read_parquet  to combine them\ninto a single result. Nonexistent or di fferently named colu mns will be filled\nwith NULL  values. \n10.4.4 Creating views\nWe probably don\u2019t want to type (or more likely copy\u2013paste) that wildcard string of all\nthe Parquet files. If we\u2019re planning to do a lot of analysis on the data, it probably\nmakes sense to create a DuckDB database that materializes the contents of the Par-\nquet files into tables. If we want to do ad hoc analysis instead, we can create a view over\nthe Parquet files. \n A view in DuckDB is not physically materialized but instead runs the underlying\nquery each time queries on the view are exec uted. It is integrated into the query plan-\nning and optimization of the outer query as a whole, much like with common table\nexpressions (CTE). The benefit of defining a view in this case is that it provides us a\nshorthand for querying the da ta rather than having to write out the full location of\nthe files on S3 each time. Views also allow for stable interfaces to the data so that if the\nunderlying data changes, the view can be updated to reflect that change without hav-\ning to change the queries that use the view. \n We can create a view by running th e query shown in the following listing.\nCREATE OR REPLACE VIEW allRidesView AS\nFROM 's3://us-prd-md-duckdb-in-action/nyc-taxis/\nyellow_tripdata_202*.parquet';Listing 10.36 Creating a view across multiple files255 10.4 Exploring the New York Taxi dataset from Parquet files\nTo keep the amount of data read from S3 reasonable, we only include the files from\n2020 onward, which is still 118 million rows. But if you want to query all the data, feel\nfree to expand the wildcard to include more of the files.\n This should return immediately because it\u2019s not reading any data\u2014it\u2019s only defin-\ning the view. The view covers roughly 118 mill ion rows, so it\u2019s not a small dataset, but\nit\u2019s also not huge. \n10.4.5 Analyzing the data\nTo get an overview of the data values and thei r distribution as well as test the read per-\nf o r m a n c e  o f  D u c k D B  w h i l e  r e t r i e v i n g  d a t a  from S3, we\u2019ll use the following query,\nusing SUMMARIZE  again. \n.timer on\nSUMMARIZE allRidesView;\nThis query needs to read the actual data to get all the statistics information, so it will\ntake a bit for it to return a result. Ev en when running the query on an AWS EC2\ninstance close to the data, it took more than 30 seconds to produce the result from\nthe 118 million rows of data:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_name \u2502 column_type \u2502 min \u2502 max \u2502approx_uniq \u2502\u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502 varchar \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 VendorID \u2502 BIGINT \u2502 1 \u2502 6 \u2502 4 \u2502\u2502 tpep_pickup_datetime \u2502 TIMESTAMP \u2502 2001-01-01 \u2502 2098-09-11 \u2502 59777057 \u2502\u2502 tpep_dropoff_datetime\u2502 TIMESTAMP \u2502 2001-01-01 \u2502 2098-09-11 \u2502 60115892 \u2502\u2502 passenger_count \u2502 DOUBLE \u2502 0.0 \u2502 112.0 \u2502 12 \u2502\u2502 trip_distance \u2502 DOUBLE \u2502 -30.62 \u2502 389678.46 \u2502 14254 \u2502\u2502 RatecodeID \u2502 DOUBLE \u2502 1.0 \u2502 99.0 \u2502 7 \u2502\u2502 store_and_fwd_flag \u2502 VARCHAR \u2502 N \u2502 Y \u2502 2 \u2502\u2502 PULocationID \u2502 BIGINT \u2502 1 \u2502 265 \u2502 264 \u2502\u2502 DOLocationID \u2502 BIGINT \u2502 1 \u2502 265 \u2502 265 \u2502\u2502 payment_type \u2502 BIGINT \u2502 0 \u2502 5 \u2502 6 \u2502\u2502 fare_amount \u2502 DOUBLE \u2502 -133391414 \u2502 998310.03 \u2502 17618 \u2502\u2502 extra \u2502 DOUBLE \u2502 -27.0 \u2502 500000.8 \u2502 671 \u2502\u2502 mta_tax \u2502 DOUBLE \u2502 -0.55 \u2502 500000.5 \u2502 82 \u2502\u2502 tip_amount \u2502 DOUBLE \u2502 -493.22 \u2502 133391363 \u2502 9064 \u2502\u2502 tolls_amount \u2502 DOUBLE \u2502 -99.99 \u2502 956.55 \u2502 3897 \u2502\u2502 improvement_surcharge\u2502 DOUBLE \u2502 -1.0 \u2502 1.0 \u2502 5 \u2502\u2502 total_amount \u2502 DOUBLE \u2502 -2567.8 \u2502 1000003.8 \u2502 36092 \u2502\u2502 congestion_surcharge \u2502 DOUBLE \u2502 -2.5 \u2502 3.0 \u2502 16 \u2502\u2502 airport_fee \u2502 INTEGER \u2502 -2 \u2502 2 \u2502 5 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25021 9r o w s \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518Run Time (s): real 33.907 user 586.582679 sys 8.706259Listing 10.37 The SUMMARIZE  command256 CHAPTER  10 Performance considerations for large datasets\nAs we can see, there are trips with negative distances, so it might make sense to filter\nthose out before doing any further analysis.\n Let\u2019s redefine our base view to exclude tr ips with negative distances so our follow-\ning analysis and queries will no t be skewed by those outliers.\nCREATE OR REPLACE VIEW allRidesView AS\nFROM 's3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_202*.parquet'\nWHERE trip_distance > 0;\nIf we read only one column of the data, trip_distance , it will be faster, only taking\nabout 3 to 4 seconds, as shown in the following listing. \n.timer on\n.mode line\nSUMMARIZE (SELECT trip_distance FROM allRidesView);\nThe result in line mode now looks like this and gives us detailed information about\nthe column trip_distance , on which we are able to es timate whether we are dealing\nwith sane data or not as well as wh ether we should fi lter out outliers:\ncolumn_name = trip_distance\ncolumn_type = DOUBLE\nmin = 0.01max = 389678.46\napprox_unique = 13114\navg = 5.430654112761637std = 536.0306563795983\nq25 = 1.093394143132663\nq50 = 1.8271836758492157q75 = 3.389607008136687\ncount = 115976028\nnull_percentage = 0.0%Run Time (s): real 3.033 user 49.924611 sys 1.228218\nAside from the more sensible aggregation re sults, we can see that  about 2 million rows\nhave been filtered out because they had negative distances.Listing 10.38 Creating a filtering view\nListing 10.39 Summarizing on a single column\nUsing SUMMARIZE\nSince DuckDB version 0.10.0, SUMMARIZE  has been capable of being used as a\nsource for a SELECT  statement, so you can write\nSELECT column_name, column_type, count, max FROM SUMMARIZE allRidesView;\nif you\u2019re only in terested in a few of the columns. At  the time of writing, it still com-\nputes the rest of the column statistics , though, so there is no time saved.257 10.4 Exploring the New York Taxi dataset from Parquet files\nIf we only execute certain aggregation operations that are available as metadata in\nParquet, we can get results even faster, taking less than a second.\nSELECT min(trip_distance),\nmax(trip_distance),\navg(trip_distance),stddev(trip_distance),\ncount(trip_distance) AS nonNull,\ncount(*) as total,1-(nonNull/total) AS nullPercentage\nFROM allRidesView;\nWe see the same results as before in the ou tput but at a fraction of the time, as we\ndon\u2019t need to read the actual data:\nmin(trip_distance) = 0.01\nmax(trip_distance) = 389678.46avg(trip_distance) = 5.430654112761703\nstddev(trip_distance) = 536.030656379596\nnonNull = 115976028\ntotal = 115976028\nnullPercentage = 0.0\nRun Time (s): real 0.747 user 7.789698 sys 0.682236\nWith the advanced SQL features in DuckDB  that allow you to apply computation on\nmultiple columns, we can also  compute the same aggregatio ns on all columns at once,\nas shown in listing 10.41.\n For the average and standard deviation,  we only apply the aggregation to the\nnumeric columns that contain the words distance , amount , tax, surcharge , or fee,\nonce as a regular expression filter and once  as a lambda function (for demonstration\npurposes). We can even post-process the resu lts, including rounding them as needed.\nYou can learn more about this feature at Star  Expressions ( https:/ /duckdb.org/docs/\nsql/expre ssions/star ) in the documentation. \n.mode line\nSELECT count(*),\nmin(columns(*)), max(columns(*)),round(avg(columns('_(distance|amount|tax|surcharge|fee)')),2),\nround(stddev(\ncolumns(c ->\nc SIMILAR TO '.+(distance|amount|tax|surcharge|fee)')),2)\nFROM allRidesView;Listing 10.40 Aggregations on single column\nListing 10.41 Aggregations on multiple columnsUses various aggregate \nfunctions on the same column\nComputes the percentage of NULL \nvalues by dividing  the count of non-\nnull values by the total count of rows\nCounts \nall rows Computes the minimum and \nmaximum values of all columns\nComputes the Average value of all\ncolumns that contain the words distance,\namount, tax, surcharge, or feeUses a lambda function for each column name with the \nregular expression operator  SIMILAR TO and computes \nthe standard deviation fo r all matching columns258 CHAPTER  10 Performance considerations for large datasets\nYou can see in the output that the aggregat ions are applied to all columns at once,\nwhich is much faster than applying them in dividually; it only takes roughly 7 seconds:\ncount_star() = 115976028\nmin(allRidesView.VendorID) = 1\nmin(allRidesView.tpep_pickup_datetime) = 2001-01-01 00:03:14\nmin(allRidesView.tpep_dropoff_datetime) = 2001-01-01 00:16:31\nmin(allRidesView.passenger_count) = 0.0\nmin(allRidesView.trip_distance) = 0.01\nmin(allRidesView.RatecodeID) = 1.0\nmin(allRidesView.store_and_fwd_flag) = N\nmin(allRidesView.PULocationID) = 1min(allRidesView.DOLocationID) = 1min(allRidesView.payment_type) = 0\nmin(allRidesView.fare_amount) = -2564.0\nmin(allRidesView.extra) = -27.0\nmin(allRidesView.mta_tax) = -0.5\nmin(allRidesView.tip_amount) = -493.22\nmin(allRidesView.tolls_amount) = -91.3\nmin(allRidesView.improvement_surcharge) = -1.0\nmin(allRidesView.total_amount) = -2567.8\nmin(allRidesView.congestion_surcharge) = -2.5\nmin(allRidesView.airport_fee) = -2\nmax(allRidesView.VendorID) = 6\nmax(allRidesView.tpep_pickup_datetime) = 2098-09-11 02:23:31\nmax(allRidesView.tpep_dropoff_datetime) = 2098-09-11 02:52:04\nmax(allRidesView.passenger_count) = 112.0\nmax(allRidesView.trip_distance) = 389678.46\nmax(allRidesView.RatecodeID) = 99.0\nmax(allRidesView.store_and_fwd_flag) = Y\nmax(allRidesView.PULocationID) = 265max(allRidesView.DOLocationID) = 265max(allRidesView.payment_type) = 5\nmax(allRidesView.fare_amount) = 998310.03\nmax(allRidesView.extra) = 113.01\nmax(allRidesView.mta_tax) = 53.16\nmax(allRidesView.tip_amount) = 1400.16\nmax(allRidesView.tolls_amount) = 956.55\nmax(allRidesView.improvement_surcharge) = 1.0\nmax(allRidesView.total_amount) = 998325.61\nmax(allRidesView.congestion_surcharge) = 2.75\nmax(allRidesView.airport_fee) = 2\nround(avg(allRidesView.trip_distance), 2) = 5.43\nround(avg(allRidesView.fare_amount), 2) = 14.74\nround(avg(allRidesView.mta_tax), 2) = 0.49\nround(avg(allRidesView.tip_amount), 2) = 2.65\nround(avg(allRidesView.tolls_amount), 2) = 0.45\nround(avg(allRidesView.improvement_surcharge), 2) = 0.43\nround(avg(allRidesView.total_amount), 2) = 21.58\nround(avg(allRidesView.congestion_surcharge), 2) = 2.3\nround(avg(allRidesView.airport_fee), 2) = 0.09\nround(stddev(allRidesView.trip_distance), 2) = 536.03\nround(stddev(allRidesView.fare_amount), 2) = 157.03\nround(stddev(allRidesView.mta_tax), 2) = 0.08259 10.4 Exploring the New York Taxi dataset from Parquet files\nround(stddev(allRidesView.tip_amount), 2) = 3.18\nround(stddev(allRidesView.tolls_amount), 2) = 1.9\nround(stddev(allRidesView.improvement_surcharge), 2) = 0.29\nround(stddev(allRidesView.total_amount), 2) = 157.38\nround(stddev(allRidesView.congestion_surcharge), 2) = 0.72\nround(stddev(allRidesView.airport_fee), 2) = 0.32\nRun Time (s): real 6.813 user 82.841430 sys 7.124035\n10.4.6 Making use of the taxi dataset\nNow let\u2019s imagine we\u2019re a city planner in New York who wants to understand the way\nthat people are navigating the city. From ou r previous queries on trip distances, we\u2019ve\nlearned that there\u2019s quite a big difference in the types of journeys being taken. Some\no f  t h e m  l o o k  l i k e  r o a d  t r i p s ,  w h i l e  o t h e r s  a r e  v e r y  s h o r t  s h u t t l e  r u n s .  W e  c a n  d r i l ldown into the data to see how it varies ac ross the years by running the following query.\nThe query computes the average distance, th e fare amount, and the fare amount per\ndistance per year between 2020 and 2024. The year is taken from the pickup time. \nSELECT year(tpep_pickup_datetime) AS year,\nround(avg(trip_distance)) AS dist,\nround(avg(fare_amount),2) AS fare,\nround(AVG(fare_amount/trip_distance),2) AS rate,count(*) AS trips\nFROM allRidesView\nGROUP BY yearHAVING year BETWEEN 2020 AND 2024\nORDER BY year;\nWe\u2019ve restricted the years used in the qu ery because some year\u2019s entries had wrong\ndate values, notably 2098, 2028, 2001, and 2008. The results of running the query are\nas follows:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 year \u2502 dist \u2502 fare \u2502 rate \u2502 trips \u2502\n\u2502 int64 \u2502 double \u2502 double \u2502 double \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 2020 \u2502 4.0 \u2502 12.49 \u2502 7.57 \u2502 24316408 \u2502\n\u2502 2021 \u2502 7.0 \u2502 13.42 \u2502 7.09 \u2502 30496201 \u2502\n\u2502 2022 \u2502 6.0 \u2502 14.69 \u2502 8.47 \u2502 39081642 \u2502\u2502 2023 \u2502 4.0 \u2502 19.15 \u2502 9.82 \u2502 22080786 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 1.831 user 22.398789 sys 3.331710\nWe can see that the average fare is going up over time because the value of money\ndecreases due to inflat ion. We also notice a further di p in 2020, which most likely was\ncaused by the COVID-19 pandemic. In our full  analysis of the dataset, a similar dip\ncould be observed in the mid-2010s when ri de services like Uber and Lyft came onto\nthe scene. You can explore this pe riod with the shared database.Listing 10.42 Trip data yearly aggregation\nRestricts the years used in the query to between \n2020 and 2024, which must be done in the \nHAVING clause, as the ye ar is the grouping key260 CHAPTER  10 Performance considerations for large datasets\n We can also look into the average passeng er count over time. In the following list-\ning, we\u2019ll focus on journeys longer than  10 miles with fewer than 10 passengers.\nSELECT passenger_count, count(*)\nFROM allRidesView\nWHERE passenger_count <10GROUP BY passenger_count\nORDER BY count(*) DESC;\nThe output of this query show s a power law distribution, with  most trips having only a\nsingle passenger:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 passenger_count \u2502 count_star() \u2502\u2502 double \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1.0 \u2502 5366688 \u2502\u2502 2.0 \u2502 1580495 \u2502\u2502 3.0 \u2502 370175 \u2502\n\u2502 4.0 \u2502 193333 \u2502\n\u2502 5.0 \u2502 149485 \u2502\u2502 0.0 \u2502 118751 \u2502\n\u2502 6.0 \u2502 102836 \u2502\n\u2502 8.0 \u2502 65 \u2502\u2502 7.0 \u2502 60 \u2502\n\u2502 9.0 \u2502 40 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 10 rows 2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRun Time (s): real 0.897 user 7.388453 sys 0.862978\nThis only scratches the surfac e of what kinds of queries can be run with these datasets,\nbut we wanted to focus more on examinin g the performance of queries on these data\nvolumes than on digging deeper into use ca ses. As you learned in previous chapters,\nyou can go ahead and build apps, APIs, or dashboards on top of those kinds of data-sets and sources, or you can just analyze them in a notebo ok and output the results for\nfurther processing. \nSummary\n\uf0a1Using the SUMMARIZE  clause to get an overview of a dataset is a good way to start\nexploring a dataset.\n\uf0a1Converting strings to enums is a usef ul technique for speeding up queries.\n\uf0a1DuckDB\u2019s modern analytical  architecture utilizes ve ctor representations and\nintra-query parallelism.\n\uf0a1During execution, SQL quer ies are turned into execution plans, which can be\ninspected with EXPLAIN .Listing 10.43 Taxi trips per passenger count261 Summary\n\uf0a1Datasets located in cloud buckets shou ld be analyzed from machines close to\nwhere they are stored to avoid ne twork transfer costs and latency.\n\uf0a1DuckDB can use predicate and projecti on pushdown to optimize queries on\nParquet files without actually populating a database.\n\uf0a1DuckDB\u2019s ability to use Parquet metadata  when reading files is extremely useful\nwhen working with large datasets, as it avoids reading data over the network\nthat is not needed for the query.\n\uf0a1Column expressions within the SELECT cl ause allow us to ap ply multiple aggre-\ngations to many columns at once.\n\uf0a1DuckDB can comfortably query datasets that contain hundreds of millions, or\neven billions, of records. 262Conclusion\nThank you for accompanying us on our journey through DuckDB in Action . We\nhope you have learned a lo t about DuckDB and how it can be used to make your\nday-to-day data engineering life more productive and enjoyable. We\u2019re glad we\ncould share our excitement and passion for the power and usefulness of DuckDB\nand empower you to use it to solve your data engineering problems and to build\namazing data products. In this chapter, we will summarize what we have learned,\nmention the areas we did not cover, and di scuss the future of data engineering with\nDuckDB. \n11.1 What we have learned in this book\nWe looked at how to get started with Duck DB, how to install it, and how to use the\nCLI and the Python API. Next, we illustrate d how easy it is to load data from CSV,\nJSON, and Parquet files and then analyze it with DuckDB using SQL\u2014even without\ncreating tables for the data. We also explored how to use DuckDB via the Python\nAPIs, both for SQL and fluent queries, and how tightly and efficiently it integrates\nwith pandas DataFrames. \n We learned how to make the most out of DuckDB with SQL, from the basics to\nmore advanced features like window func tions and CTEs. As part of our SQL explo-\nrations, we highlighted all the goodies that DuckDB adds to the standard SQL lan-\nguage, like support for JSON, nested data structures, ad vanced joins, and flexible\nselection, grouping , and aggregation.\n To demonstrate how to inte grate DuckDB into your da ta architectures, we built\ndata pipelines with dbt, dltHub, and Dagster. On the application side, we used\nStreamlit and Apache Supe rset to visualize data from DuckDB directly.\n In the last chapter, we looked at the sp ecifics of handling large datasets, includ-\ning performance considerations. 263 11.3 Aspects we did not cover\n11.2 Upcoming stable versions of DuckDB\nWith the release of the book, you should s oon have the first stable version in your\nhands: DuckDB 1.0. Ou r work is based on 0.10.0, the la test prerelease version at the\ntime of writing, which is meant to stabilize all features, APIs, and formats as well as to\nbe compatible with version 1.0. These vers ions are meant to handle database version\nformat changes automatically and be backwa rd compatible (and, in parts, even for-\nward compatible), which will also extend to the MotherDuck service. \n11.3 Aspects we did not cover\nEven with the breadth of the book, we could not cover everything in the vast ecosys-\ntem of DuckDB. Since the focus of the book is on being an introductory and practical\nguide, we did not cover the internals of DuckDB, how it is implemented, and how itworks under the hood. There is much more to learn about the ar chitecture, the query\nexecution engine, the indexing  capabilities, the storage la yer, and the vectorized exe-\ncution model. \n We touched a little bit on that in chapte r 10, but there is much more to explore in\nthe DuckDB source code, bl og posts, videos, and docu mentation. Hannes and Mark\nhave also been interviewed in several podcasts (see https:/ /www.youtube.com/\nwatch?v=pZV9FvdKmLc  and https:/ /www.youtube.com/watch?v=f9QlkXW4H9A )\nwhere they dive into the details of DuckDB , which you can listen to for more insight.\n We mostly used the CLI and the Python API throughout the book and cover access\nfrom Java in the appendix. However, DuckDB  also has APIs for C,  R, Rust, Go, Java-\nScript, and many other languages, so you can use DuckDB in your favorite environ-\nment. Please consult the docu mentation for more details: https:/ /duckdb.org/docs/\napi/overview .\n Another area we didn\u2019t dive deeper into  is performance optimizations and consid-\nerations. Fortunately, DuckDB performs really well out of the box, so you rarely have to\nworry about performance, but there are still many things you can do to optimize your\nqueries and your data to make  the most out of DuckDB. Feel free to consult the docu-\nmentation ( https:/ /mng.bz/7d2g ) and additional bl og posts, such as \u201cPerf is not\nEnough\u201d ( https:/ /motherduck.com/b log/perf-is-not-enough/ ) by Jordan Tigani.\n The documentation also provides great content about DuckDB\u2019s extension frame-\nwork. Extensions can be written in both C+ + and Rust. Speaking of extensions, there\nare already many useful extensions, including the spatial index ( https:/ /duckdb.org/\ndocs/extensions/spatial.html ), which has a lot of capabilities, similar to PostGIS, and\ncan be used to build advanc ed geospatial applications.\n DuckDB provides an information schema, al igning with the classic SQL catalog, but\nalso provides various table functions for accessing curren t configurations and ways to\nchange it. These are covered on the \u201cDuc kDB_% Metadata Functions\u201d documentation\npage ( https:/ /mng.bz/maWM ). DuckDB Labs and MotherDuck list over 50 partners264 CHAPTER  11 Conclusion\nthat integrate DuckDB into their products and services, so there is a lot for you to\nexplore, according to your needs. \n11.4 Where can you learn more?\nThe DuckDB docs ( https:/ /duckdb.org/docs ) are a great place to start. They are very\ncomprehensive and cover all aspects of DuckDB  in great detail. The same is true for\nthe MotherDuck documentation ( https:/ /motherduck.com/docs ). \n There are a few YouTube channels run by  either MotherDuck and DuckDB people\nor individuals, which have a number of tu torials, talks, and pr esentations on DuckDB\nand MotherDuck. For quick answers from an active community, check out the\nDuckDB Discord ( https:/ /discord.duckdb.org/ ) and the MotherDuck Community\nSlack ( https:/ /slack.motherduck.com/ ). \n If you want to contribute to DuckDB, you can find the source code on GitHub\n(https:/ /github.com/duckdb ). There you can also raise bugs and feature requests. \n11.5 What is the future of da ta engineering with DuckDB?\nWe think DuckDB is a very promising technology that will play a prominent role in the\nfuture of data engineering. DuckDB is a ve ry versatile tool that can be used in many\ndifferent scenarios, from local analysis of  medium-sized datasets to large-scale data\nprocessing in the cloud close to where your data lives. With the growth of local data\nfrom health monitoring, home automation , and other personal information systems,\nefficient processing of private data on yo ur own devices will be come more relevant. \n Given its similarity to SQLite, we think DuckDB will be used in many places SQLite\nis used today. That includes applicatio ns, games, browser, phones, IoT and edge\ndevices for analytics, data ag gregation, prefiltering, and serving data to other systems\nand end users.\n An interesting business use case is the long tail of cloud data warehouses, like\nBigQuery, Redshift, Snowflake, and so on. Mo st users of these systems don\u2019t have peta-\nbytes of data to analyze and process but, rather, gigabyte s or terabytes, which can be\nhandled by DuckDB at a fraction of the co st and resource usage\u2014and with much less\ncomplexity.\n Some potential areas of innovation that will be interesting to watch include the\nintegration in generative AI use cases, like adding efficient embedding vector storage\nand indexing and supporting streaming da ta processing. Given the DuckDB team\u2019s\nfocus on usability, we\u2019re also looking forw ard to their ideas on making SQL and data\nprocessing even more flexible , accessible, and user friendly . As part of that work, we\nwould expect more composability so that every aspect of the system can be queried\nand combined flexibly.\n And now it\u2019s your turn to shape the fu ture of data engineering with DuckDB!\nHappy Quacking!\n 265appendix\nClient APIs for DuckDB\nSo far, we\u2019ve focused on either the DuckDB  CLI or the Python integration. The for-\nmer is not only the easiest and fastest way to bring up a DuckDB database and use it\nbut also the smoothest way of teaching Du ckDB without having to install a lot of\nthings. The latter just follows naturally,  as many of the analytics use cases of\nDuckDB are rooted in the broader Python world.\n There is only so much you can put into  one book, however, so the look we are\ntaking at integrations in this appendix wi ll be more concise. If you\u2019re interested in\nother integrations, we assume  you are familiar with conc epts like the classpath for\nJava and are aware of dependency management for both Java and R.This appendix covers \n\uf0a1Alternative client APIs for DuckDB\n\uf0a1When and where those APIs are useful\n\uf0a1A word on concurrency\n\uf0a1How to ingest large amounts of data through client APIs\n\uf0a1A showcase of the Java Database Connectivity \nintegration 266  APPENDIX Client APIs for DuckDB\nA.1 Officially supported languages\nDuckDB provides support for the following languages:\n\uf0a1C\u2014DuckDB implements a custom C API fo llowing the SQLite C API to a large\nextent. The API is contained in the duckdb.h  header. In addition, a full wrap-\nper for the SQLite API is provided, whic h can be used to relink existing SQLite\nprograms against DuckDB. The C API is distributed as libduckdb .\n\uf0a1C++\u2014DuckDB is written in C++, so an inte gration follows quite naturally. It is\ndistributed as libduckdb  too. It\u2019s worth mentioning  that you want to use this\nAPI for providing any user-d efined functions (UDFs) th at should be vectorized.\n\uf0a1Java\u2014As explained in this appendix, DuckDB provides a JDBC driver\n(org.duckdb:duckdb_jdbc ) through Maven central, which bundles the\nDuckDB binaries for each major operating system and can run as part of yourJava programs.\n\uf0a1Julia \u2014This integration will run in the same process as the Julia client and fully\nsupports the DBInterface  interface. DuckDB provides native Julia DataFrames,\nwhich let you seamlessly continue your an alytic work in a scientific language.\nThe integration is delivered as a DuckDB  package.\n\uf0a1Python \u2014As shown throughout the book, DuckDB integrates well with the\nPython ecosystem, allowing you to run analytics directly in your Python pro-\ngrams and notebooks and even seamlessl y interact with Pandas DataFrames.\nThe integration is delivered as the duckdb  package.\n\uf0a1Node.js \u2014An API modelled along the SQLite AP I is available. It\u2019s noteworthy\nthat the API exposes Apache Arrow integr ation for zero-copy ingestion of data.\n\uf0a1R\u2014The official duckdb  package provides an implementation of R\u2019s DBI Inter-\nface and all of its methods. As with Juli a, the package is optimized for efficient\ndata transfer. Any DuckDB table can be mapped directly onto an R DataFrame\nand vice versa. The DuckDB R in tegration also works well with dbplyr  and\ndplyr , two packages that are somewhat sim ilar to the relational API offered in\nPython for the safe, progra mmatic query construction.\n\uf0a1Rust\u2014The Rust API is an idiomatic an d ergonomic wrapper for Rust around\nthe C-API and can be installed via crates.io.\n\uf0a1Swift \u2014The Swift API enables developers on  Swift platforms to harness the full\npower of DuckDB using a native Swift inte rface. The API is not only available on\nApple platforms but on Linux too, opening up new opportunities for the grow-ing Swift on server ecosystem.\n\uf0a1ODBC \u2014The Open Database Connectivity  (ODBC) is a C-style API that provides\naccess to different flavors of databa se management systems (DBMSs). The\nODBC API consists of the driver manager (DM) and the ODBC drivers.\nDuckDB supports ODBC version 3.0.\nBoth the WebAssembly (WASM) and Arrow Database Connectivity (ADBC) integra-\ntions are worth a special mention:267 A.2 A word on concurrency\n\uf0a1WASM is a binary instruction format for a stack-based virtual machine and ships with\nall four major browsers: Firefox, Chrome, Safari, and Edge.  DuckDB can be compiled\nto WASM and will run natively in your browser. There, it can be used as an inter-active Web shell or programmatically through JavaScript, which is also how Mot-herDuck provides a web-based inte rface to their DuckDB backend.\n\uf0a1Arrow Database Connectivity (ADBC), similarly to ODBC and JDBC, is a C-style API\nthat enables code portability between different database systems.  The main difference\nbetween ADBC and ODBC/JDBC is that AD BC uses Apache Arrow as an effi-\ncient columnar format to transfer data  between the database system and the\napplication. DuckDB has an ADBC driver  that takes advantage of the zero-copy\nintegration between DuckDB and Arrow to  efficiently transfer data. The ADBC\ndriver is available for C++ and Python.\nA.2 A word on concurrency\nIn this book, we focused on data processi ng and only briefly touched on application\ndevelopment in chapter 9 with Streamlit. Most interactive appl ications will provide\nconcurrent access to themselves. This might be throug h multiple threads in a multi-\nthreaded environment, such as Java, or within event loops on a single thread. \n DuckDB offers two configurable options for concurrency:\n\uf0a1A single process can both read and write to the database.\n\uf0a1Multiple processes can read from the da tabase, but none of them can write, as\nthe database is in read-only mode for all of them.\nThis does not mean you can\u2019t use DuckDB in  concurrent, interactive applications; you\njust need to be aware of the limitations. When using the first option, DuckDB supports\nmultiple writer threads using a combination of multiversion concurrency control(MVCC) and optimistic concurre ncy control, but all within that single writer process. \n If your application does not span mult iple processes, it can safely access one\nshared in-memory or file-based da tabase under the following rules:\n\uf0a1As long as there are no write conflicts, multiple concurrent writes will succeed.\n\uf0a1Appends will never conflict, even on the same table.\n\uf0a1Multiple threads can also simultaneously update separate tables or separate sub-\nsets of the same table.\n\uf0a1Optimistic concurrency co ntrol comes into play wh en two threads attempt to\nedit (update or delete) the same row at th e same time. In that  situation, the one\nthread attempting the update wi ll fail with a conflict error.\nWhat will not work without restriction is a typical microservices architecture, in which\nyour orchestrator will scale your application up and down by starting new processes orstopping existing ones, while accessing the same underlying  files. Such a scenario is\nlimited to using read-only access to the same  DuckDB files or read \u2013write access to dif-\nferent separate DuckDB files. 268  APPENDIX Client APIs for DuckDB\nA.3 Use cases\nMany applications we all use on a daily ba sis on our phones embed a relational data-\nbase: SQLite. \n It can be found in\n\uf0a1Every Android device\n\uf0a1Every iPhone and iOS device\n\uf0a1Every Mac\n\uf0a1Many television sets and set-top cable boxes\n\uf0a1Many automotive mu ltimedia systems\nSQLite itself claims to be the \u201cmost widely deployed and used database engine,\u201d with\nan estimated 1 trillion SQLite databases in active use (see https:/ /www.sqlite.org/\nmostdeployed.html ). DB-Engines, a website that re gularly ranks databases for their\npopularity, usually finds it in the top 10.\n With an API mostly compat ible with SQLite, you can swap out SQLite for DuckDB\nfor your example in your next iOS applic ation. By incorporating DuckDB into your\napplication, even in read-only mode, you ca n benefit from its data-ingestion capabili-\nties. Instead of using some arbitrary API to process CSV files, you can use DuckDB\nconsistently to query all supp orted formats and use the result s directly in your applica-\ntion through SQL, as explained in chapte r 5. The same holds true for read-only\nDuckDB databases, against which you can run hardcoded or dynamically generated\nqueries.\n Whether your typical online transactio n processing (OLTP)  application should\nmake use of DuckDB as a primary storage is  debatable. While DuckDB supports trans-\nactional semantics and several thousand writes per second, it \u2019s not designed for typical\ntransactional workloads with many small operations. In guaran teed single-process\ndeployments, this will work similarly to other applicatio ns with this style, using SQLite\nor H2, a Java-based, embedded, in-process data base. DuckDB is explicitly designed for\nonline analytical pr ocessing (OLAP) and will defini tely shine in those use cases.\n Maybe a hybrid approach better suits yo ur needs. The following is a personal\nexample.\n I run a page tracking my sports activiti es. All tracking data is maintained and\ninserted via a handful of scripts, and after updating, the website is  statically regener-\nated using a Python Flask application. Whil e this is a hobby project, this approach\nscales for bigger setups. The source code can be found here: https:/ /github.com/\nmichael-simons/biking3 . \nA.4 Importing large amounts of data\nUsing prepared SQL statements, where data is bound to named or indexed parameters,\nis the preferred method of inserting data in to relational databases. This is not only\nbecause using parameters prevents most case s of SQL injections but also for perfor-\nmance reasons. These statements have th e same shape and don\u2019t require repeated269 A.5 Using DuckDB from Java via the JDBC Driver\nparsing and planning by the database, and their execution plans can be cached. Pre-\npared statements exist in all standardized client APIs, such as JDBC and the database\nconnectivity in C++, but they are not the best  option for bulk loading data into DuckDB. \n Your go-to approach should actually be ingesting data from inside DuckDB via the\nmethods we learned in chapters 3, 4, and 5: read_csv , read_json , and read_parquet\nwith direct inserts into tables as necessary. If this is not possible, both the JDBC driver\nand the C++ client offer an appender, which directly writ es into the corresponding\ntables. The JDBC driver additionally offe rs import and export via Apache Arrow.\nA.5 Using DuckDB from Ja va via the JDBC Driver\nThe Java Database Connectivity (JDBC) API is  one of the oldest specifications on the\nJava platform, and a client\u2014or a driver , in JDBC lingo\u2014exists for almost every data-\nbase. JDBC itself is closely tied to the SQ L standardization efforts and relational data-\nbases. So of course, DuckDB offers a JDBC driver too. \n This section is not an introduction to  Java, JDBC, or dependency management on\nthe JVM. We will have a look at the st ructure and the peculiarities of the DuckDB\nJDBC driver, what it is capable of, and what it is not. If you want to use the JDBC driver\nto ingest large amounts of data, we will cover that too.\n The Maven coordinates for the DuckDB JDBC driver are org.duckdb:duckdb_\njdbc:0.10.0 . In a project based on the Maven build system, you would declare a\ndependency on the driver as follows.\n<dependency>\n<groupId>org.duckdb</groupId>\n<artifactId>duckdb_jdbc</artifactId><version>0.10.0</version>\n</dependency>\nNOTE We suggest including the driver in the compile  scope, in contrast to\nprovided , so that you are able to ex plicitly unwrap a generic JDBC\nConnection  into a dedicated DuckDBConnection  for accessing some of its spe-\ncific methods. \nFor Gradle, another build system for Java, the declaratio n also uses the compile  scope:\ndependencies {\ncompile 'org.duckdb:duckdb_jdbc:0.10.0'\n}\nT o  k e e p  t h i n g s  a s  s i m p l e  a s  p o s s i b l e ,  w e  a r e  n o t  u s i n g  a n y  b u i l d -  o r  d e p e n d e n c y -\nmanagement systems in the following exampl es. Our examples are all single-class pro-\ngrams that don\u2019t require any dependencies apart from the JDBC driver.\n You can download duckdb_jdbc-0.10.0.jar directly from Maven central, using the\nlink in the listing. The listing utilizes cURL , a command-line tool for accessing webListing A.1 Maven dependency declaration for DuckDB JDBC270  APPENDIX Client APIs for DuckDB\nresources, to download  the JAR file. This needs to be done only once, and you can, of\ncourse, use any other tool (or your browser) for that purpose.\n The filename of our first example is simp le.java. The example re quires Java 17 and\ncan be run with the following incantation directly from the source file, without\nexplicit compilation:\ncurl -OL https://repo1.maven.org/maven2/org/duckdb/duckdb_jdbc/\\\n0.10.0/duckdb_jdbc-0.10.0.jar\njava -cp duckdb_jdbc-0.10.0.jar simple.java\nAll example source code is available in the book repository ( https:/ /github.com/\nduckdb-in-action/exa mples/tree/main/a1 ). There is no need to manually enter the\ncode.\n On the outside, the DuckDB JDBC driver looks like any other JDBC driver. It pro-\nvides an implementation of the java.sql.Driver , the java.sql.Statement , and\nthe java.sql.PreparedStatement . It does not support callable statements, the\nretrieval of generated keys, or some other details of the JDBC spec. When in doubt,\nyou will need to read its sources to check if the API you want to use is supported, or\njust try it out. \n The JAR file is quite large\u2014for version 0. 10.0, it\u2019s about 65 megabytes. The reason\nfor this is actually quite simple. Looking into the JAR file with unzip -l duckdb_jdbc-\n0.10.0.jar reveals that the driver ships the DuckDB bi naries for all major operating sys-\ntems.\n The files beginning with libduckdb_java are the actual native DuckDB binaries:\nArchive: duckdb_jdbc-0.10.0.jar\nLength Date Time Name\n--------- ---------- ----- ----\n0 02-13-2024 13:20 META-INF/\n64 02-13-2024 13:20 META-INF/MANIFEST.MF24 02-13-2024 13:20 META-INF/services/java.sql.Driver\n3297 02-13-2024 13:20 org/duckdb/DuckDBAppender.class\n2581 02-13-2024 13:20 org/duckdb/DuckDBArray.class\n20549 02-13-2024 13:20 org/duckdb/DuckDBArrayResultSet.class\n2699 02-13-2024 13:20 org/duckdb/DuckDBColumnType.class\n812 02-13-2024 13:20 org/duckdb/DuckDBColumnTypeMetaData.class\n11017 02-13-2024 13:20 org/duckdb/DuckDBConnection.class\n24447 02-13-2024 13:20 org/duckdb/DuckDBDatabaseMetaData.class\n676 02-13-2024 13:20 org/duckdb/DuckDBDate.class\n2289 02-13-2024 13:20 org/duckdb/DuckDBDriver.class\n...\n48822768 02-13-2024 13:26 libduckdb_java.so_linux_amd6487819276 02-13-2024 14:54 libduckdb_java.so_osx_universal\n24764928 02-13-2024 14:54 libduckdb_java.so_windows_amd64\n44872680 02-13-2024 14:55 libduckdb_java.so_linux_arm64\n--------- -------\n206440213 33 files\nThe JDBC driver will load the native DuckDB library for your operating system into\nthe Java process, thus staying true to the fact that DuckDB is an embedded, in-process271 A.5 Using DuckDB from Java via the JDBC Driver\ndatabase. So what we said about concur rency in the beginning of this appendix\na p p l i e s  h e r e  a s  w e l l .  A n  i n s t a n c e  o f  t h e  D u c k D B  d r i v e r  a n d  d a t a b a s e  c a n  s a f e l y  b e\naccessed from several threads inside a Ja va program and several instances of a\njava.sql.Connection  at once. It is not possible for several Java processes to use the\nsame DuckDB database file in write mode at  once. In that case, they would be limited\nto read-only mode. \nA.5.1 Understanding the general usage pattern\nThe general usage pattern for the DuckDB JD BC driver is no different from other\nJDBC drivers. The JDBC URL for DuckDB is jdbc:duckdb:  for a pure in-memory\ndatabase that will not be saved to disk wh en the Java process ends. The following code\nacquires a connection, creates a JDBC st atement, executes a query returning all\nDuckDB settings, prints them, and then exit s. Just using an available database func-\ntion here allows us to focus on the rele vant parts for understanding how to use the\nJDBC driver. \n Asking the DriverManager  to get a connection for a JDBC URL is the standard way\nto acquire a JDBC conne ction. Of course, we are using a DuckDB URL.\nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nclass simple {\npublic static void main(String... a) throws SQLException {\nvar query = \"SELECT * FROM duckdb_settings() ORDER BY name\";\ntry (\nvar con = DriverManager\n.getConnection(\"jdbc:duckdb:\");\nvar stmt = con.createStatement();var resultSet = stmt.executeQuery(query)\n){\nwhile (resultSet.next()) {\nSystem.out.printf(\"%s %s%n\",\nresultSet.getString(\"name\"),\nresultSet.getString(\"value\"));\n}\n}\n}\n}\nThe first few lines when running this Java  p r o g r a m  l o o k  l i k e  t h i s  o n  t h e  a u t h o r \u2019 s\nmachine. Remember, you need to  run the program like this: java  -cp duckdb _jdbc\n-0.10.0.jar  simple.java , from our example repository using at least Java 17:\nCalendar gregorian\nTimeZone Europe/BerlinListing A.2 simple.java\nThe connection is wrapped in a \u201ctry-\nwith-resources\u201d block so that it will\nbe automatically closed.\nYou need a Statement\nto execute queries,\nwhich we acquire in\nthe same block to close\nit after usage too.Our query will be\nexecuted here; a\nResultSet is a\nresource and needs\nto be closed too.\nWe iterate the\nresult set and\nprint its contents.272  APPENDIX Client APIs for DuckDB\naccess_mode automatic\nallocator_flush_threshold 134.2MB\nallow_unsigned_extensions falsearrow_large_buffer_size false\nautoinstall_extension_repository\nautoinstall_known_extensions trueautoload_known_extensions true\nbinary_as_string\nA.5.2 Using multiple connections from several threads\nHere we answer the following question: Could you safely use DuckDB inside one\ninstance of your Java  application that might use multip le threads to respond to multi-\nple concurrent http requests? In  short, yes, you totally can.  We are, however, not build-\ning a backend for a web application here but demonstrating this by using a few\nthreads. In a proper applicat ion, you would most likely not create a new connection in\neach thread but use a JDBC connection pool instead. Frameworks like Spring Boot or\nQuarkus do this for you automatically. Wh ile not included in this book, we confirmed\nsuccessfully that DuckDB ind eed works fine in any of those frameworks with the\nrespective connection po ols. What we recreate in the following text is similar to how\nmultiple concurrent requests against a web application would eventually end up as\nconcurrent queries running in one DuckDB instance. \n The following program will save the database  to a file named readings.db; hence, it\nuses the following JDBC URL to retrieve a connection: jdbc:duckdb:readings.db. It\nwill create a readings  table on the main thread and then spawn 20 new threads that\ninsert readings with random values.\n This example shows that while only one process can open a DuckDB file, it is no\nproblem to use multiple threads in that pr ocess to access it for reading and writing. \nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nimport java.sql.Timestamp;import java.time.LocalDateTime;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ThreadLocalRandom;import java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport org.duckdb.DuckDBConnection;\nclass using_multiple_connections {\nprivate static final AtomicInteger ID_GENERATOR = new AtomicInteger(0);\nprivate static final String DUCKDB_URL\n= \"jdbc:duckdb:readings.db\";\npublic static void main(String... a) throws Exception {Listing A.3 using_multiple_connections.java\nThe DuckDB URL that will \ncreate a readings.db273 A.5 Using DuckDB from Java via the JDBC Driver\nvar createTableStatement = \"\"\"\nCREATE TABLE IF NOT EXISTS readings (\nid INTEGER NOT NULL PRIMARY KEY,created_on TIMESTAMP NOT NULL,power DECIMAL(10,3) NOT NULL\n)\"\"\";\nvar executor = Executors.newWorkStealingPool();\ntry (\nvar con = DriverManager\n.getConnection(DUCKDB_URL);\nvar stmt = con.createStatement()\n){\nstmt.execute(createTableStatement);var result = stmt\n.executeQuery(\"SELECT max(id ) + 1 FROM readings\");\nresult.next();ID_GENERATOR.compareAndSet(0, result.getInt(1));result.close();\nf o r( i n ti=0 ;i<2 0 ; ++i) {\nexecutor.submit(() -> insertNewReading(con));\n}executor.shutdown();executor\n.awaitTermination(5, TimeUnit.MINUTES);\n}\n}\n}\nThis program creates 20 tasks that are subm itted to an executor, which uses a thread\npool with as many threads as available proce ssors. Which task do they run? It turns out\nthey run the static method using_multiple_connections::insertNewReading , which\nis shown in listing A.4. The method is calle d with the original connection. Inside the\nmethod, we turn this connection into a DuckDBConnection , using the typesafe JDBC\napproach by unwrapping the class. The DuckDBConnection  now can be duplicated,\nwhich is equivalent to calling DriverManager.getConnection(\"jdbc:duckdb:\nreadings.db\")  again, but much faster. The method  then inserts a single row with a\nrandom value and handles the resources correctly. Checked SQL exceptions are\ncaught and rethrown as runtime exceptions  to be handled outside, which is a com-\nmon pattern in Java. \nstatic void insertNewReading(Connection connection) {\nvar sql = \"INSERT INTO readings VALUES (?, ?, ?)\";\nvar readOn = Timestamp.valueOf(LocalDateTime.now());\nvar value = ThreadLocalRandom.current().nextDouble() * 100;Listing A.4 using_multiple_connections::insertNewReadingThis statement\nensures the target\ntable exists.\nThe connection is created on \nthe main thread and closed \nwhen the try block ends.\nA java.sql.Statement opened in \nthe try block as well to get it cleaned up after usage.\nRetrieves the\nlargest value of the\nid column so far\nCreates 20 tasks, each executing \nthe given method in parallel\nMakes sure the program does not \nterminate until all tasks are finished274  APPENDIX Client APIs for DuckDB\ntry (\nvar con = connection\n.unwrap(DuckDBConnection.class).duplicate();\nvar stmt = con.prepareStatement(sql)\n){\nstmt.setInt(1, ID_GENERATOR.getAndIncrement());\nstmt.setTimestamp(2, readOn);\nstmt.setDouble(3, value);stmt.execute();\n} catch (SQLException e) {\nthrow new RuntimeException(e);\n}\n}\nRunning this program with java -cp duckdb_jdbc-0.10.0.jar using_multiple_\nconnections.java  will create readings.db  in the same directory, populated with a\nreadings  table and at least 20 rows. If you run a SELECT  on the table, ordered by the\nmillisecond timestamp at whic h the records have been created, you can see that the\nID values do not have the same order. This indicates that the insert statement did\nactually run asynchronously:\njava -cp duckdb_jdbc-0.10.0.jar \\\nusing_multiple_connections.java\nduckdb readings.db -s \".maxrows 6\" -s \"FROM readings ORDER BY created_on\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 id \u2502 created_on \u2502 power \u2502\n\u2502 int32 \u2502 timestamp \u2502 decimal(10,3) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 0 \u2502 2024-02-10 18:15:58.457194 \u2502 73.185 \u2502\n\u2502 4 \u2502 2024-02-10 18:15:58.457272 \u2502 24.159 \u2502\n\u2502 2 \u2502 2024-02-10 18:15:58.457432 \u2502 46.807 \u2502\u2502\u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502\n\u2502\u00b7 \u2502 \u00b7 \u2502 \u00b7 \u2502\u2502 16 \u2502 2024-02-10 18:15:58.462457 \u2502 55.934 \u2502\n\u2502 18 \u2502 2024-02-10 18:15:58.462785 \u2502 1.298 \u2502\n\u2502 19 \u2502 2024-02-10 18:15:58.46287 \u2502 55.559 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 20 rows (6 shown) 3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nYou can use all SQL constructs supported by DuckDB over JDBC. The main APIs for\nyour SQL statements are java.sql.Statement  and java.sql.PreparedStatement .\nUse the latter if you want to parameterize yo ur statements. If you find yourself looking\nfor a fluent query builder similar to the re lational API in DuckDB\u2019s Python package or\nto what R offers, you can have a look at jOOQ ( https:/ /www.jooq.org ), which is an\nopen source project that bridges SQL and the Java world. I\u2019ve used it with great suc-\ncess (and fun!) in many projects. Unwraps into a \nspecialized connection\nDuplicates\nGets a \njava.sql.PreparedStatement \nthat can be parameterized275 A.5 Using DuckDB from Java via the JDBC Driver\nA.5.3 Using DuckDB as a tool for data processing from Java\nIn chapter 5, we used  DuckDB to explore data without using DuckDB\u2019s persistent stor-\nage. This is, of course, possible not only  from the DuckDB CLI or the Python client\nbut also from Java. For some formats, such as  Parquet, it\u2019s a great solution: Java cannot\ndeal with Parquet files without using extern al libraries. The libraries that exist for\ndealing with Parquet in Java oftentimes depend on Apache Hadoop, Apache Spark, orApache Avro\u2014great products, but ones that come with a huge number of dependen-\ncies. If you prefer a project without that many dependencies, you can simply use\nDuckDB. \n We have a list of Parquet files in the a1/weather folder of our example repository\nthat contain weather data scraped from Wikipedia ( https:/ /en.wikipedia.org/wiki/\nList_of_cities_by_average_temperature ). We want to list these weather stations in our\nJava program by name and the yearly temp erature value. Instead of writing a lot of\nJava code to load and inspect the files one by one, we can just ask an embedded\nDuckDB instance to do this for us. The method presented in the following listingopens an in-memory connection, selects the da ta of interest, and creates our list. The\nmethod itself is part of the using_the_appender.java file. The whole program will be\nshown later.\n The query we are using here does a lot of things that normally would require a\nconsiderable amount of code and at least on e additional library. It reads all Parquet\nfiles in the weather folder by using a glob in the \nFROM  clause (weather/*.parquet) and\nthen extracts one value from the Year  column per City  (the source does contain the\ntemperature in Celsius and Fahrenheit). As with previous data sources, data quality\nvaries, and the query also makes sure the te mperature value can actually be read as a\nnumeric value. The rest of the code then bo ils down to the ceremony Java requires for\nJDBC, as shown in the following listing. \nprivate record WeatherStation(String id, double avgTemperature) {\n}\nstatic List<WeatherStation> weatherStations() throws SQLException {\nvar query = \"\"\"\nSELECT City AS id,\ncast(replace(\ntrim(\nregexp_extract(Year,'(.*)\\\\n.*', 1)\n), '\u2212', '-') AS double)\nAS avgTemperature\nFROM 'weather/*.parquet'\n\"\"\";\nvar weatherStations = new ArrayList<WeatherStation>();\ntry (\nvar con = DriverManagerListing A.5 using_the_appender::weatherStations\nKeeps all the resources \nin the try block276  APPENDIX Client APIs for DuckDB\n.getConnection(\"jdbc:duckdb:\");\nvar stmt = con.createStatement();\nvar resultSet = stmt.executeQuery(query)\n){\nwhile (resultSet.next()) {\nvar id = resultSet.getString(\"id\");var avgTemperature = resultSet.getDouble(\"avgTemperature\");\nweatherStations.add(new WeatherStation(id, avgTemperature));\n}\n}\nreturn weatherStations;\n}\nA.5.4 Inserting large amounts of data\nYou already know by now how to ingest da ta with DuckDB \u201cfrom the inside,\u201d essen-\ntially treating CSV, Parquet, or JSON file s as tables and relying on DuckDB\u2019s machin-\nery to insert batches of data efficiently. Bu t what about data that was created as part of\nanother process and never written to a file? A Java-based service might run all kinds of\ncomputations, maybe calling other services while doing so. It could be wasteful to\nwrite large results first to a file and then ingest them. \n Normally, you would use an instance of java.sql.PreparedStatement  and its\nbatch processing capabilities to do so. We already used a prepared statement in listing\nA.4 for a single insert, and the batch usage would look similar. While nothing prevents\nyou from using it as a batch insert, it woul d be slower than nece ssary with the DuckDB\nJDBC driver. \n For our example, we will use the list of we ather stations created in the previous sec-\ntion. In early 2024, a challenge had take n the hearts\u2014but especially the minds\u2014of\nthe Java community by storm: the One Billi on Row Challenge, also known as 1BRC,\nrun by Gunnar Morling (see https:/ /github.com/gunnarmorling/1brc ). The task of\nthis challenge was to write a Java program that read a CSV file, calculated the mini-\nmum, average, and maximum temperature values per weat her station, and emitted\nthe results in a specific format.\n The original challenge was based on a CSV file with one billion rows, which can, of\ncourse, read via DuckDB directly, but we wa nt to have the data available in DuckDB\u2019s\nnative table format for processing. So we tasked ourselves to create a DuckDB data-base with a configurable number of rows in one table derived from the weather sta-\ntions from the previous section.\n We will use the \norg.duckdb.DuckDBAppender  to directly write data into the table.\nTo get a hold of an instance of that class,  we must again unwrap the generic JDBC con-\nnection into a DuckDBConnection , which will let you create an appender that writes\ndirectly into a table. The following method is also part of the bigger program\nusing_the_appender.java . It creates one connection, persisting data into\nweather.db , and unwraps the generic JDBC connection into DuckDBConnection  to\naccess its specialized methods. The DuckDBConnection  is used to create an appender\nfor the weather  table, to which many rows are appended. Iterates the date and creates \nobjects as desired from it277 A.5 Using DuckDB from Java via the JDBC Driver\n \nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nimport java.util.concurrent.ThreadLocalRandom;import org.duckdb.DuckDBConnection;\nstatic void generateData(int size) throws SQLException {\nvar stations = weatherStations();\nvar numStations = stations.size();\ntry (var con = DriverManager.getConnection(\"jdbc:duckdb:weather.db\")\n.unwrap(DuckDBConnection.class)\n){\nvar rand = ThreadLocalRandom.current();\nlong start = System.currentTimeMillis();\ntry (var appender = con.createAppender(\nDuckDBConnection.DEFAULT_SCHEMA,\"weather\")\n){\nf o r( i n ti=0 ;i<s ize; ++i) {\ni f( i>0& &i% 50_000_000 == 0) {\nappender.flush();\n}\nvar station = stations.get(rand.nextInt(numStations));appender.beginRow();\nappender\n.append(station.id());\nappender\n.append(station.measurement());\nappender.endRow();\n}\n}\n}\n}\nThe program, executed with java  -cp duckdb_jdbc-0.10.0.jar  using_the_\nappender.java 1000000000 , takes about 10 minutes on my machine to create a 2.4\nGB database file containing a billion randomized rows.\n DuckDB takes roughly 3 seconds to comp ute the answer to the One Billion Row\nChallenge, as shown in the following listing.\n.mode line\nWITH src AS (\nSELECT id AS station_name,\nMIN(measurement) AS min,CAST(AVG(measurement) AS DECIMAL(8,1)) AS mean,\nMAX(measurement) AS max\nFROM weatherListing A.6 using_the_appender::generateData\nListing A.7 Solving the 1BRC with SQLThe generic connection must be \nunwrapped into a DuckDBConnection \nto access vendor-specific methods.\nAppenders must be created \nspecifically for one table\u2014here, for the weather table.\nThe beginning of a new\nrow must be indicated.\nColumns must be appended \nin the order in which the table columns are defined.\nThe end of the row\n must be indicated.278  APPENDIX Client APIs for DuckDB\nGROUP BY station_name\n)\nSELECT '{' || ARRAY_TO_STRING(\nLIST(station_name || '=' || CONCAT_WS('/', min, mean, max)\nORDER BY station_name), ', ')\n|| '}' AS \"1BRC\"\nFROM src;\nThe abbreviated output looks like this:\n1BRC = {Abha=-33.5/18.0/71.7, Abidjan=-22.1/26.0/75.0, .. }\nRun Time (s): real 3.051 user 29.247171 sys 0.033100\nA.6 Additional connection options\nThe DuckDB JDBC driver doesn\u2019t support any URL parameters, but instead, it uses\nconnection options. In read_only.java , we demonstrate the available options. Here,\nwe\u2019re using a read-only conne ction so that multiple pr ocesses can access the same\ndatabase and streaming JDBC result rows. The program uses a similar query to the\npreceding one to solve the One Billion Row Challenge and formats the output in Java\ncode, as shown in the following listing. \nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nimport java.util.Locale;import java.util.Properties;\nimport org.duckdb.DuckDBDriver;class read_only {\npublic static void main(String... args) throws SQLException {\nvar properties = new Properties();\nproperties.setProperty(\nDuckDBDriver.DUCKDB_READONLY_PROPERTY, \"true\");\nproperties.setProperty(\nDuckDBDriver.JDBC_STREAM_RESULTS, \"true\");\nvar query = \"\"\"\nSELECT id AS station_name,\nMIN(measurement) AS min,CAST(AVG(measurement) AS DECIMAL(8,1)) AS mean,\nMAX(measurement) AS max\nFROM weatherGROUP BY station_name\nORDER BY station_name\n\"\"\";\nvar url = \"jdbc:duckdb:weather.db\";\ntry (\nvar con = DriverManagerListing A.8 read_only.java\nOptions for JDBC are passed\nvia a Properties object.\nUses the available \nconstants on the \nDuckDBDriver class and doesn\u2019t hardcode the \nnames as strings279 Summary\n.getConnection(url, properties);\nvar stmt = con.createStatement();var result = stmt.executeQuery(query)\n){\nboolean first = true;System.out.print(\"{\");while (result.next()) {\nif(!first) {\nSystem.out.print(\", \");\n}var station = result.getString(\"station_name\");var min = result.getDouble(\"min\");var mean = result.getDouble(\"mean\");var max = result.getDouble(\"max\");System.out.printf(\nLocale.ENGLISH, \"%s=%3.2f/%3.2f/%3.2f\",station, min, mean, max);\nfirst = false;\n}\n}System.out.println(\"}\");\n}\n}\nWhile the pure DuckDB solution runs in about 3 seconds, the Java program takes\nabout 26 seconds, which is astonishin gly good, as it includes the following:\n\uf0a1Compiling the Java program into byte code\n\uf0a1Loading the JDBC Driver and the native DuckDB code\n\uf0a1Loading the 2.4 GB database file\n\uf0a1Doing the aggregation\n\uf0a1Streaming and formatting the results\nThe baseline timing for the original One Billion Row Challenge setup was around 5\nminutes. In 2024, the winni ng program solved the chal lenge in about 300 millisec-\nonds\u2014however, with beautifu l, hand-optimized code.\n Nevertheless, the One Billion Row Ch allenge and our own experiments here\ndemonstrate one thing: Java is not slow, and neither is database interaction with Java.If you are familiar with Java or have an existing codebase in which you need some ana-lytics of medium to relatively large data vo lumes, using DuckDB fr om Java is a feasible\napproach. \nSummary\n\uf0a1DuckDB can be used from a pl ethora of different languages.\n\uf0a1Most languages are supported by Duck DB or DuckDB Labs with official\nextensions.\n\uf0a1There\u2019s usually a mechanism to directly map a table into the host languages on\nplatforms that support DataFrames.The properties object \nneeds to be passed to \nthe driver manager.280  APPENDIX Client APIs for DuckDB\n\uf0a1Prepared statements should only be us ed for handling parameterized queries,\nnot for batch loading.\n\uf0a1If possible, use an SQL-based import from the inside. Otherwise, look for a ded-icated mechanism, such as an appender or the Arrow import, and export in the\nlanguage of your choice.\n\uf0a1A single DuckDB file can only be acce ssed from one process in write mode or\nfrom many in read-only mode.\n\uf0a1A process can access a single  DuckDB resource from many threads in both write\nand read-only mode at once. 281index\nSymbols\n* operator 106\n+ Database button 210\n~ operator 132\nA\nADBC (Arrow Databa se Connectivity) 8, 266\naggregate function 130\naggregating data\nASOF JOIN 86\u201390\nconditions and filtering outside WHERE \nclause 78\u201382\nFILTER clause 81\u201382\nQUALIFY clause 80\u201381\ngrouping sets 66\u201369\nLATERAL joins 93\u201396\nPIVOT statement 82\u201386\npre-aggregating data while ingesting 59\u201361\nusing table functions 90\u201393\naliases, accessing everywhere 54\nALL operator 66\nALTER TABLE statement 26, 28, 140\namount column 257\nanalyzing\nCSV files with DuckDB CLI 17\u201321\ndata 59\u201361\nANY operator 65\nApache Arrow 141\u2013143\nApache Superset, building BI dashboards \nwith 207\u2013221\ncreating dashboards 210\u2013215creating datasets from SQL queries 216\u2013220\nexporting and importing dashboards 220\u2013221\noverview 207\u2013209\nworkflow 209\nArrow, interoperability with Python 141\u2013143\nASOF JOIN 86\u201388, 90\u201396\nATTACH command 120, 154\nauto-inferring\nfile types 99\nschema 99\navg function 73, 93\nB\nbar function 10\nbatch processing 165\nbernoulli method 55\nBI (business intelligence) dashboards 194\nwith Apache Superset 207\u2013221\ncreating dashboards 210\u2013215\ncreating datasets from SQL queries 216\u2013220\nexporting and importing dashboards\n220\u2013221\noverview 207\u2013209\nworkflow 209\nBIGINT type 101, 115\nbinary transfer mode 99\nBOOLEAN type 101\nBY NAME clause 53\nC\n-c COMMAND argument 15\nCASE statement 60282\nCASE WHEN THEN ELSE END construct 60\ncast function 105\ncatalog, defined 145\nCLI (command-line interface) 12\u201315\narguments 15\ndot commands 14\ninstalling on Linux, macOS and Windows 13\nSQL statements 14\nclient APIs 265\u2013280\nconcurrency 267\nimporting large amounts of data 269\u2013278\nJava, additional connection options 278\u2013279\nofficially supported languages 266\nuse cases 268\ncloud, MotherDuck in 147\u2013150\nconnecting to with DuckDB via token-based \nauthentication 148\u2013150\nusing through UI 148\ncoalesce function 77, 82\ncold storage 157\nCOLUMNS expression 52\u201353\ncolumns, grouping and ordering by all \nrelevant 54\u201355\ncommon table expressions (CTEs) 9, 234\nCommunity Cloud, deploying app on 205\u2013207\ncompile scope 269\nconcatenated natural key 26\nconcurrency 267\nconditions and filtering, outside WHERE \nclause 78\u201382\nFILTER clause 81\u201382\nQUALIFY clause 80\u201381\nconnection options, additional Java 278\u2013279\nconvert_locale function 139\nCOPY..TO clause 112\nCount field 229\ncount query 67\nCREATE DATABASE statement 151\u2013152\nCREATE OR REPLACE SECRET statement 156\nCREATE OR REPLACE TABLE statement 27, \n231\nCREATE SEQUENCE statement 28\nCREATE SHARE statement 154\nCREATE STATEMENT 27\nCREATE TABLE statement 26, 231\nCREATE VIEW statement 29\ncreating databases in MotherDuck 152\u2013153\nCROSS JOIN 95\nCSV (comma-separated values) files\nanalyzing with DuckDB CLI 17\u201321\nresult modes 18\u201321\nexploring data 228\u2013230exploring without persistence 116\u2013120\nquerying 116\u2013120\ntransforming all 178\u2013182\nCSV (comma-separated values), translating to \nParquet 108\u2013113\nCSV parsing 101\nCTAS (Create table as select) 29\nCTEs (common table expressions) 9, 234, 254\ncurl command-line utility 225\ncurrent_database function 153\nD\ndagster dev command 185, 188\ndagster-duckdb 183\ndagster-webserver 183\nDagster, orchestrating data pipelines with\n182\u2013192\ndata\nanalyzing and querying Parquet files\n113\u2013116\nexploring without persistence 116\u2013120\nquerying databases 116\u2013120\nsummarizing 61\u201366\nsubqueries 62\u201366\ndata analysis, window functions 69\u201378\naccessing preceding or following rows in \npartition 77\u201378\nframing 74\nnamed windows 75\u201377\npartitioning 72\u201374\ndata apps 193\u2013222\nbuilding custom data apps with Streamlit\n194\u2013207\nbuilding app 195\u2013197\ndeploying app on Community Cloud\n205\u2013207\nusing components 197\u2013201\nvisualizing data using plot.ly 201\u2013204\ndata exploration 98\u2013122\nreasons for using database without persisting \ndata 99\ndata ingestion, dlt (data load tool) 165\u2013171\nbuilding pipeline 167\u2013170\nexploring pipe line metadata 170\u2013171\ninstalling supported source 166\u2013167\ndata model 226\u2013228\ndata pipelines 163\u2013192\ndlt (data load tool), data ingestion with\n165\u2013171\norchestrating with Dagster 182\u2013192\ndefining assets 183\u2013185283\nmanaging dependencies in pipeline\n186\u2013191\nrunning pipelines 185\u2013186\nuploading to MotherDuck 191\u2013192\noverview of 164\u2013165\ndata processing flow 7\u201311\ndata formats and sources 7\u20138\ndata structures 8\ndeveloping SQL 9\nusing or processing results 9\u201311\ndata transformation and modeling, with \ndbt 171\u2013182\ndefining sources 173\ndescribing transformations with models\n173\u2013176\nsetting up dbt project 172\u2013173\ntesting transformations and pipelines\n176\u2013182\ndatabase connections, opening 124\u2013125\n.databases CLI command 152\nDATE type 101, 113, 115\ndateformat parameter 56, 101\ndbt (data build tool), da ta transformation and \nmodeling with 171\u2013182\ndefining sources 173\ndescribing transformations with models\n173\u2013176\nsetting up dbt project 172\u2013173\ntesting transformations and pipelines 176\u2013182\ndbt-duckdb library 171\nDDL (data definition language) queries 26\u201331\nALTER TABLE statement 28\u201329\nCREATE TABLE statement 26\u201328\nCREATE VIEW statement 29\u201330\nDESCRIBE statement 30\u201331\ndecimal_separator argument 101\nDEFAULT declaration 28\ndense_rank() function 70\nDESCRIBE clause 113\nDESCRIBE SHARE command 154\nDETACH statement 155\ndlt (data load tool), data ingestion with 165\u2013171\nbuilding pipeline 167\u2013170\nexploring pipe line metadata 170\u2013171\ninstalling supported source 166\u2013167\ndlt init command 166\ndot commands 14\nDOUBLE type 101, 104\u2013105\nDriverManager 271\nDROP DATABASE command 153\nduckbox mode 18\nDuckDB CLI, analyzing CSV files with 17\u201321result modes 18\u201321\nduckdb tag 239\nduckdb_engine 124, 135\nduckdb_extensions function 15\u201316\nduckdb_functions SQL function 137\nduckdb.__version__ 124\nduckdb.connect(datab ase=\u2019:default:\u2019) 125\nduckdb.default_connection 125\nDuckDBPyRelation 127\u2013128\nDucklings 145\nE\nEDA (exploratory data analysis) 7\nELT (extract, load, and transform) 165\nenum types 238\nenum_code(enum_str) function 238\nenum_first function 238\nenum_last function 238\nenum_range_boundary function 238\nenum_range(null:enum_type) function 238\nenums, using for tags 238\u2013242\nETL (extract, transform, and load) 165\nExcel files 120\u2013122\nexcept_ function 131\nEXCLUDE clause 51\nexecute function 133\nexecute.fetchall 135\nexecute.fetchone 135\nEXISTS expression 65\n.exit command 112\nEXPLAIN 245\u2013248\nEXPLAIN ANALYZE 245\u2013248\nexporting, Stack Overflow dataset to \nParquet 248\u2013251\nextensions 15\u201317\nextracting data 224\u2013226\nF\nfast exploratory queries 233\u2013235\nfetchall method 133\nfetchdf method 199\nfetchone method 133\nfifa database 117\nfilename argument 101\nfilename=true flag 109\nFILTER clause 79, 81\u201382, 93\nfilter function 129\nFLASK_APP variable 208\nframing, window functions 74284\nFROM clause 50, 60, 76, 252, 275\nfunctions with optional parameters 56\nG\nGROUP BY ALL clause 54\nGROUP BY clause 23, 29, 54, 79\nGROUP BY CUBE 68\nGROUP BY ROLLUP 73\ngrouping by all relevant columns 54\u201355\ngrouping sets 66\u201369\nGZIP codec 112\nH\nHAVING clause 54, 78\u201380\n.help command 15\nhot storage 157\nhttpfs extension 17, 24, 252\nHUGEINT 115\nI\nIF NOT EXISTS clause 27\nif/else construct 60\nimport command 249\nimporting data, using DuckDB from Java via \nJDBC driver 269\u2013278\ngeneral usage pattern 271\ninserting large amounts of data 276\u2013278\nusing DuckDB as tool fo r data processing from \nJava 275\nusing multiple connections from several \nthreads 272\nIN operator 65\ninferring file type and schema 99\u2013101\nCSV parsing 101\ninfo command 170\ningesting data, with Python API 126\u2013128\nINSERT statement 53\ninserting by name 53\u201354\nINSTALL statement 16, 121\nINTEGER 115\nintegrations 161\u2013162\ninteroperability, with Apache Arrow and \nPolars 141\u2013143\nintersect function 131\nINTO clause 53\nIPC (interprocess communication) 143J\nJava, additional connection options 278\u2013279\njava.sql.Connection 271\njava.sql.Driver 270\njava.sql.PreparedStatement 270, 274, 276\njava.sql.Statement 270, 274\nJDBC (Java Database Connectivity) 265\ndriver 269\u2013278\ngeneral usag e pattern 271\ninserting large amounts of data 276\u2013278\nusing DuckDB as tool for data processing from \nJava 275\nusing multiple connections from several \nthreads 272\nJOIN clause 87\njq command-line JSON processor 225\nJSON (JavaScript Object Notation), shredding \nnested JSON 102\u2013107\n-json argument 15\nL\nlag() function 77\nlambda function 53\nlarge datasets 223\u2013261\nexploring New York Taxi dataset from Parquet \nfiles 251\u2013260\nanalyzing data 255\u2013258\nauto-inferring file types 252\u2013253\nconfiguring credentials for S3 access 252\ncreating views 254\u2013255\nexploring Parquet schema 253\u2013254\nmaking use of taxi dataset 259\u2013260\nexporting Stack Overflow dataset to \nParquet 248\u2013251\nloading and querying full Stack Overflow data-\nbase\ndata dump and extraction 224\u2013226\ndata model 226\u2013228\nexploring CSV file data 228\u2013230\nextracting data 224\u2013226\nfast exploratory queries on large tables\n233\u2013235\nloading data into DuckDB 231\nposting on weekdays 236\u2013238\nusing enums for tags 238\u2013242\nquery planning and execution 243\u2013248\nplanner and optimizer 243\nruntime and vectorization 243\u2013244\nvisualizing query plans with EXPLAIN and \nEXPLAIN ANALYZE 245\u2013248285\nLATERAL joins 93\u201396\nlead() function 77\nleft variable 198\nlength function 137\nLIKE operator 53\nLIMIT 0 clause 29\nLIMIT clause 103, 112\n-line argument 15\nline based modes 18\nLinux, installing DuckDB CLI 13\nlist_transform function 240\nLOAD statement 17, 121\nloading data, large datasets 231\nlogical types 110\nlogical_type field 115\nM\n-m flag 185\nmacOS, installing DuckDB CLI 13\nmarkdown function 200\nmatches table 183, 197, 199, 212, 216\nmax function 73, 76\nMD_RUN parameter 156\u2013157\nmemory_limit setting 112\nmin function 73, 76\nMotherDuck 6, 144\u2013162, 233\nconnecting to with DuckDB via token-based \nauthentication 148\u2013150\nfeatures of 150\u2013162\ncreating databases in MotherDuck 152\u2013153\nintegrations 161\u2013162\nmanaging S3 secrets and loading data from \nS3 buckets 156\noptimizing data ingestion and MotherDuck \nusage 156\u2013157\nquerying data with AI 157\u2013161\nsharing databases 153\u2013156\nuploading databases to MotherDuck\n151\u2013152\noverview of 145\u2013150\nreasons for using 146\u2013147\nuploading to 191\u2013192\nusing through UI 148\nMotherDuck Community Slack 264\nMVCC (multiversion concurrency control) 267\nN\nnames argument 101\nnextval() function 28normalized tables 109\nNULL values 105, 238\nO\nOLAP (online analytical processing) 23\nOLTP (online transaction processing) data-\nbases, querying 116\u2013120\nON clause 83, 95\noptimizing data ingestion and MotherDuck \nusage 156\u2013157\nORDER BY clause 23, 71\nORDER clause 71\norder function 130\norg.duckdb.DuckDBAppender 276\nOUTER JOIN 95\nOVER() clause 69, 71\nP\npackages.yml file 178\npandas DataFrames, querying 134\u2013135\nparameterized query 133\nParquet 108\nexporting Stack Overflow dataset to 248\u2013251\ntranslating CSV to 108\u2013113\nParquet files\nanalyzing and querying 113\u2013116\nexploring New York Taxi dataset from\n251\u2013260\nanalyzing data 255\u2013258\nauto-inferring file types 252\u2013253\nconfiguring credentials for S3 access 252\ncreating views 254\u2013255\nexploring Parquet schema 253\u2013254\nmaking use of taxi dataset 259\u2013260\nparquet_metadata function 115\nparquet_schema function 114, 253\nparse phase 243\nPARTITION BY clause 72\npartitions, window functions 72\u201374\naccessing preceding or following rows in \npartition 77\u201378\nperformance tag 239\nphysical data types 110\npip command 166\npipeline function 165\nPIVOT clause 83, 86\npivoting 83\npl function 141\nplanner and optimizer 243286\nplot.ly, visualizing data using 201\u2013204\nPolars 141\u2013143\npopulation object 128\npostgres extension 120\nproject function 129\nprompt_query pragma 158\nprompt_schema procedure 158\nprompt_sql procedure 159\npushdown predicates 29\nPython\ninteroperability with  Apache Arrow and \nPolars 141\u2013143\nrelational API 126\u2013143\ncomposing queries 128\u2013133\ningesting CSV data with Python API\n126\u2013128\nSQL querying 133\u2013134\nUDFs (user-defined functions) 136\u2013141\nPython ecosystem 123\u2013135\ninstalling Python package 124\nopening database connection 124\u2013125\nquerying pandas DataFrames 134\u2013135\nQ\nQUALIFY clause 71, 78, 80\u201381\nquantile function 77\nquantile_cont function 77\nquantile_disc function 77\nquery planning and execution 243\u2013248\nplanner and optimizer 243\nruntime and vectorization 243\u2013244\nvisualizing query plans with EXPLAIN and \nEXPLAIN ANALYZE 245\u2013248\nquerying\nother databases 116\u2013120\npandas DataFrames 134\u2013135\nSQLite 116\u2013120\nquerying data with AI 157\u2013161\nR\nRAG (retrieval augmented generation) 161\nrange() function 91\nRDBMS (relational database management \nsystem) 25\n.read command 21\nread_csv function 91, 126\nread_csv_auto function 18, 109, 179\nread_json_auto function 56\nread_parquet function 91, 252\u2013253read_xxx functions 100\nreadings table 27\u201328, 30\n-readonly argument 15\nrecursive:= true parameter 102\nrelation 25\nrelational API 126\u2013134\ncomposing queries 128\u2013133\ningesting CSV data with Python API 126\u2013128\nSQL querying 133\u2013134\nrelational database management system \n(RDBMS) 25\nremove_spaces function 137\nREPEATABLE clause 55\nREPLACE clause 51\nreservoir method 56\nresult modes 18\u201321\nright variable 198\nROLLUP clause 68\nround function 93\nrow groups 243\nrun method 168\u2013169\nruntime 243\u2013244\nRust, programming language 237\nS\ns table 65\nS3 (Simple Storage Service), configuring creden-\ntials for access 252\nsampling data 55\u201356\nscalar, correlated subquery 63\nscalar, uncorrelated subquery 63\nschema.yml file 176\nsearch_players function 197\u2013199\nSELECT statement 50, 52\u201353, 103\nself-join 73\nSEQUEL (Structured English QUEry \nLanguage) 158\nservice layer 145\nsharing databases 153\u2013156\nSHOW DATABASES command 152\nshow function 128\u2013129\nshredding nested JSON 102\u2013107\nSIMD (single instruction, multiple data) 244\nSMALLINT 115\nSNAPPY\ncodec 112\nformat 248\nSQL (Structured Query Language)\nanalyzing energy production 23\u201326\ndownloading dataset 24\u201325\ntarget schema 25\u201326287\ndeveloping 9\nDuckDB-specific SQL extensions 50\naccessing aliases everywhere 54\nfunctions with optional parameters 56\ngrouping and ordering by all relevant \ncolumns 54\u201355\ninserting by name 53\u201354\nsampling data 55\u201356\nSELECT * 51\u201353\nexecuting queries 22\u201323\nquerying 133\u2013134\nstatements 14\nSQL Alchemy URI field 211\nSQL injection attacks 133\nSQL queries\ncreating datasets from 216\u2013220\nDDL queries 26\u201331\nALTER TABLE statement 28\u201329\nCREATE TABLE statement 26\u201328\nCREATE VIEW statement 29\u201330\nDESCRIBE statement 30\u201331\nsql tag 239\nsql() method 124\u2013125\nsqlite extension 117\nsqlite_scan command 118\nSQLite, querying 116\u2013120\nst instance 197\nst_read function 121\nst_searchbox function 198\nStack Overflow dataset, exporting to \nParquet 248\u2013251\nstart parameter 91\nstop parameter 91\nstorage 145\nstreamlit package 197\nstreamlit-searchbox component 197\nStreamlit, building custom data apps with\n194\u2013207\nbuilding app 195\u2013197\ndeploying app on Community Cloud 205\u2013207\nusing components 197\u2013201\nvisualizing data using plot.ly 201\u2013204\nstrftime function 202\nstrongly typed database system 118\nstrptime function 110, 186\nSTRUCT arrays 103\nsubqueries 61\u201366\nas expressions 65\u201366\nALL 66\nANY 65\nEXISTS 65\noverview of 62\u201366sum aggregate 80\nsum function 29, 73\nSUMMARIZE clause 9, 232\nsurface assertion 176\nsurrogate key 25\nT\ntable based modes 18\ntable functions 90\u201393, 128\n.tables command 117\ntags, using enums for 238\u2013242\nTIME type 101\ntime_bucket() function 59\nTIMESTAMP type 101\ntimestampformat argument 101\nTINYINT 115\nto_arrow_table function 142\nto_table function 128\ntourney_date column 178\ntourney_id assertion 176\nTPC-H benchmark 99\ntree command 172, 220\ntrim function 137\ntrip_distance column 256\ntry_cast function 105\ntuples 30\ntype function 127\nU\nUDFs (user-defined functions) 136\u2013141\nUNCOMPRESSED format 248\nuncorrelated subquery 62\nunderstatapi library 102\nUNION ALL clause 103\nunion function 131\nunion_by_name = true option 254\nunion_by_name option 100\nunnest function 103\n-unsigned argument 15\nunzip command 117\nUPDATE SHARE statement 154\nuploading database s to MotherDuck 151\u2013152\nuse cases, where DuckDB fits in 6\u20137\nUSE statement 151\nUSING clause 85\u201386\nusing_multiple_connections, insertNewReading \nmethod 273\nusing_the_appender.java program 276\nUUID (universally unique identifier) 25288\nV\nVARCHAR type 101, 118, 139\nvectorization 243\u2013244\nvectorized query engine 3\nviews, creating 254\u2013255\nW\nWASM (WebAssembly) 266\nweakly typed database system 118\nweekdays, posting on 236\u2013238\nWHERE clause 23, 53\u201354, 71, 78\u201381, 181\nwindow functions 69\u201378\naccessing preceding or following rows in \npartition 77\u201378framing 74\nnamed windows 75\u201377\npartitioning 72\u201374\nWindows, installing DuckDB CLI 13\nX\nxidel 225\nY\nYAML (YAML Ain't Markup Language) 173\nZ\nZSTD format 2483\n \nA list of useful extensions to the SQL standard\nWhat Why How\nFROM  first in \nSELECT  \nstatementsWhen building a query, the first thing you need to \nknow is where your data is coming FROM . DuckDB \nlets you put FROM  before SELECT  or even com-\npletely omit the SELECT  clause in case of a \nSELECT * .FROM my_table;\nFine-grained star-\nselectsSELECT *  selects all columns that are projected \nin the FROM  clause. These might be (too) many \ncolumns. You can reshape the star using the EXCLUDE  and REPLACE  clauses.Selecting all columns but full_text : \nSELECT * EXCLUDE (full_text) \nFROM article;Selecting all columns and replacing \npopulation  with population  divided by \n1M: SELECT * REPLACE (population \n/ 10^6 AS population) FROM \ncountries;\nFine-grained col-\numn selectionYou can use regular expressions or lambda func-\ntions to select sets of columns. And also apply \nfunctions to those.Using regular expressions: SELECT \nCOLUMNS('_(regional|sales?)') \nFROM sales;Using lambda functions: SELECT \nCOLUMNS(c -> c LIKE \n'%_regional_%') FROM sales;Applying aggregation function on column \nset: SELECT \nAVG(COLUMNS('_sales')), MAX(COLUMNS('_sales')) FROM \nsales;\nSimplified GROUP \nBY clauseUse GROUP BY ALL  to GROUP BY  all columns in \nthe SELECT  statement that are not wrapped in \naggregate functions. This allows the columns list to be maintained in a single place, saves typing, \nand prevents bugs by keeping the SELECT  granu-\nlarity aligned to the GROUP BY  granularity.SELECT town, street_name,\n    avg(income)\nFROM addressesGROUP BY ALL;\nColumn aliases in \nWHERE , GROUP  BY, \nand HAVING  \nclausesIn many SQL dialects, it is not possible to use an \nalias defined in a SELECT  clause anywhere but in \nthe ORDER BY  clause This commonly leads to \nqueries that are much more verbose than neces-\nsary, as you have to repeat the same expression multiple times. In DuckDB, a nonaggregate alias \nin the SELECT  clause can be immediately used in \nthe WHERE  and GROUP BY  clauses. Likewise, an \naliased defined for an aggregate function can be \nused in a HAVING  clause filtering the aggregate.SELECT town AS city,\nstreet_name AS street,\n avg(income)\n    AS income_per_street\nFROM addressesWHERE city = 'Springfield'\nGROUP BY city, street\nHAVING income_per_street > 5000;Mark Needham \u25cf Michael Hunger \u25cf Michael Simons\nForeword by  Mark Raasveldt and  Hannes M\u00fchleisen \nDuckDB makes data analytics fast and fun! You don\u2019t need \nto set up a Spark or run a cloud data warehouse just to process a few hundred gigabytes of data. \nDuckDB is \neasily embeddable in any data analytics application, runs on a laptop, and processes data from almost any source, including \nJSON , CSV, Parquet, SQLite and Postgres.\nDuckDB in Action  guides you example-by-example from setup, \nthrough your fi  rst SQL query, to advanced topics like building \ndata pipelines and embedding DuckDB as a local data store \nfor a Streamlit web app.  You\u2019ll explore DuckDB\u2019s handy SQL \nextensions, get to grips with aggregation, analysis, and data without persistence, and use Python to customize \nDuckDB. \nA hands-on project accompanies each new topic, so you can see \nDuckDB in action. \nWhat\u2019s Inside\n\u25cf Prepare, ingest and query large datasets\n\u25cf Build cloud data pipelines\n\u25cf Extend DuckDB with custom functionality\n\u25cf Fast-paced SQL recap: From simple queries to \n   advanced analytics\nFor data pros comfortable with Python and CLI tools.\nMark Needham  is a blogger and video creator at @LearnData-\nWithMark. Michael Hunger  leads product innovation for the \nNeo4j graph database. Michael Simons  is a Java Champion, \nauthor, and Engineer at Neo4j.\nFor print book owners, all ebook formats are free:\nhttps:/ /www.manning.com/freebook\nDuckDB  IN ACTIONDATA\nMANNING\u201cI use DuckDB every day, \nand I still learned a lot about \nhow DuckDB makes things \nthat are hard in most \n  databases easy!\u201d \n\u2014Jordan Tigani\nFounder, MotherDuck\n\u201cAn excellent resource! \nUnlocks possibilities for \nstoring, processing, analyzing, \nand summarizing data at the\n edge using DuckDB.\u201d\u2014Pramod Sadalage\nDirector, Th  oughtworks \n\u201cClear and accessible. \nA comprehensive resource for \nharnessing the power of \nDuckDB for both novices and\n experienced professionals.\u201d \n\u2014Qiusheng Wu, Associate Professor\nUniversity of Tennessee\n\u201cExcellent! Th  e book all \nwe ducklings have been \n  waiting for!\u201d \n\u2014Gunnar Morling, Decodable\nISBN-13: 978-1-63343-725-8\nSee first page"}